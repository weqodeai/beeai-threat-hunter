# Contribute
Source: https://framework.beeai.dev/community/contribute





# Requirement Agent
Source: https://framework.beeai.dev/experimental/requirement-agent



<Note>
  This is an **experimental** feature and will evolve based on community feedback.
</Note>

The `RequirementAgent` is a declarative AI agent that combines language models, tools, and execution requirements to create predictable, controlled behavior across different LLMs.

<Tip>
  Curious to see it in the action?
  Explore our [interactive exercises](https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/agents/experimental/requirement/exercises) to discover how the agent solves real problems step by step!
</Tip>

## Why Use Requirement Agent?

Building agents that work reliably across multiple LLMs is difficult. Most agents are tightly tuned to specific models, with rigid prompts that cause models to misinterpret instructions, skip tools, or hallucinate facts.

`RequirementAgent` provides a declarative framework for designing agents that strikes a balance between flexibility and control. It allows for agent behavior that is both predictable and adaptable, without the complexity and limitations of more rigid systems.

## Core Concepts

**üí° Everything is a Tool**

* Data retrieval, web search, reasoning, and final answers are all implemented as tools
* This structure ensures valid responses with structured outputs and eliminates parsing errors

**üí° Requirements Control Tool Usage**

> You can define rules that control when and how tools are used.

* "Only use tool A after tool B has been called"
* "Tool D must be used exactly twice, but not two times in a row"
* "Tool E can only be used after both tool A and tool B have been used"
* "Tool F must be called immediately after tool D"
* "You must call tool C at least once before giving a final answer"

## Quickstart

This example demonstrates how to create an agent with enforced tool execution order:

```py
from beeai_framework.agents.experimental import RequirementAgent
from beeai_framework.agents.experimental.requirements.conditional import ConditionalRequirement

# Create an agent that plans activities based on weather and events
agent = RequirementAgent(
    llm=ChatModel.from_name("ollama:granite3.3:8b"),
    tools=[
        ThinkTool(),             # to reason
        OpenMeteoTool(),         # retrieve weather data
        DuckDuckGoSearchTool()   # search web
    ],
    instructions="Plan activities for a given destination based on current weather and events.",
    requirements=[
        # Force thinking first
        ConditionalRequirement(ThinkTool, force_at_step=1),
        # Search only after getting weather, at least once
        ConditionalRequirement(DuckDuckGoSearchTool, only_after=[OpenMeteoTool], min_invocations=1),
    ],
)

# Run with execution logging
response = await agent.run("What to do in Boston?").middleware(GlobalTrajectoryMiddleware())
print(response.answer.text)
```

This agent will:

1. First use `ThinkTool` to reason about the request
2. Check weather using `OpenMeteoTool`
3. Search for events using `DuckDuckGoSearchTool` (at least once)
4. Provide recommendations based on the gathered information

**‚û°Ô∏è Check out the following examples**

* [Multi-agent](https://github.com/i-am-bee/beeai-framework/blob/main/python/examples/agents/experimental/requirement/multi_agent.py) system via handoffs.
* [ReAct](https://github.com/i-am-bee/beeai-framework/blob/main/python/examples/agents/experimental/requirement/multi_agent.py) loop in a second.
* Generating [text](https://github.com/i-am-bee/beeai-framework/blob/main/python/examples/agents/experimental/requirement/structured_output.py) and [structured](https://github.com/i-am-bee/beeai-framework/blob/main/python/examples/agents/experimental/requirement/text_output.py) output.
* [Advanced](https://github.com/i-am-bee/beeai-framework/blob/main/python/examples/agents/experimental/requirement/complex.py) (detailed configuration).

## Requirements and Rules

**Requirements** are functions that evaluate the current agent state and produce a list of rules. The system evaluates requirements before each LLM call to determine which tools are available.

**Rules** define specific constraints on tool usage. Each rule contains the following attributes:

| Attribute      | Description                                           |
| :------------- | :---------------------------------------------------- |
| `target`       | The tool the rule applies to                          |
| `allowed`      | Whether the tool can be used                          |
| `hidden`       | Whether the tool‚Äôs definition is visible to the agent |
| `prevent_stop` | Whether the rule blocks termination                   |
| `forced`       | Whether the tool must be invoked                      |

When requirements generate conflicting rules, the system applies this precedence:

* **Forbidden takes precedence:** If any rule forbids a tool, it cannot be used
* **Highest priority forced rule wins:** Among forced rules, the highest priority requirement determines the forced tool
* **Multiple prevention rules combine:** All `prevent_stop` rules are respected

<Warning>
  Requirements are evaluated on every iteration before calling the LLM.
</Warning>

<Tip>
  Start with a single requirement and add more as needed.
</Tip>

## Conditional Requirement

The conditional requirement controls when tools can be used based on specific conditions.

### Force Execution Order

This example forces the agent to use `ThinkTool` for reasoning followed by `DuckDuckGoSearchTool` to retrieve data. This trajectory ensures that even a small model can arrive at the correct answer by preventing it from skipping tool calls entirely.

```py
RequirementAgent(
  llm=ChatModel.from_name("ollama:granite3.3:8b"),
  tools=[ThinkTool(), DuckDuckGoSearchTool()],
  requirements=[
      ConditionalRequirement(ThinkTool, force_at_step=1), # Force ThinkTool at the first step
      ConditionalRequirement(DuckDuckGoSearchTool, force_at_step=2), # Force DuckDuckGo at the second step
  ],
)
```

### Creating a ReAct Agent

A ReAct Agent (Reason and Act) follows this trajectory:

```text
Think -> Use a tool -> Think -> Use a tool -> Think -> ... -> End
```

You can achieve this by forcing the execution of the `Think` tool after every other tool:

```py
RequirementAgent(
  llm=ChatModel.from_name("ollama:granite3.3:8b"),
  tools=[ThinkTool(), WikipediaTool(), OpenMeteoTool()],
  requirements=[ConditionalRequirement(ThinkTool, force_at_step=1, force_after=[OpenMeteoTool, WikipediaTool])],
)
```

<Tip>
  For a more general approach, use `ConditionalRequirement(ThinkTool, force_at_step=1, force_after=Tool, consecutive_allowed=False)`, where the option `consecutive_allowed=False` prevents `ThinkTool` from being used multiple times in a row.
</Tip>

### ReAct Agent + Custom Conditions

You may want an agent that works like ReAct but skips the "reasoning" step under certain conditions. This example uses the priority option to tell the agent to send an email after creating an order, while calling `ThinkTool` after every other action.

```py
RequirementAgent(
  llm=ChatModel.from_name("ollama:granite3.3:8b"),
  tools=[ThinkTool(), retrieve_basket(), create_order(), send_email()],
  requirements=[
    ConditionalRequirement(ThinkTool, force_at_step=1, force_after=Tool, priority=10),
    ConditionalRequirement(send_email, only_after=create_order, force_after=create_order, priority=20, max_invocations=1),
  ],
)
```

### Prevent Early Termination

The following requirement prevents the agent from providing a final answer before it calls the `my_tool`.

```py
ConditionalRequirement(my_tool, min_invocations=1)
```

### Complete Parameter Reference

```py
ConditionalRequirement(
  target_tool, # Tool class, instance, or name (can also be specified as `target=...`)
  name="", # (optional) Name, useful for logging
  only_before=[...], # (optional) Disable target_tool after any of these tools are called
  only_after=[...], # (optional) Disable target_tool before all these tools are called
  force_after=[...], # (optional) Force target_tool execution immediately after any of these tools are called
  min_invocations=0, # (optional) Minimum times the tool must be called before agent can stop
  max_invocations=10, # (optional) Maximum times the tool can be called before being disabled
  force_at_step=1, # (optional) Step number at which the tool must be invoked
  only_success_invocations=True, # (optional) Whether 'force_at_step' counts only successful invocations
  priority=10, # (optional) Higher number means higher priority for requirement enforcement
  consecutive_allowed=True, # (optional) Whether the tool can be invoked twice in a row
  force_prevent_stop=False,  # (optional) If True, prevents the agent from giving a final answer when a forced target_tool call occurs.
  enabled=True, # (optional) Whether to skip this requirement‚Äôs execution
  custom_checks=[
     # (optional) Custom callbacks; all must pass for the tool to be used
    lambda state: any('weather' in msg.text for msg in state.memory.message if isinstance(msg, UserMessage)),
    lambda state: state.iteration > 0,
  ],
)
```

‚û°Ô∏è See the [full example](https://github.com/i-am-bee/beeai-framework/blob/main/python/examples/agents/experimental/requirement/complex.py#L56-L82).

<Tip>
  Pass a class instance (e.g., `weather_tool = ...`) or a class (`OpenMeteoTool`) rather than a tool's name. Some tools may have dynamically generated names.
</Tip>

<Note>
  The reasoner throws an error if it detects contradictory rules or a rule without an existing target.
</Note>

## Ask Permission Requirement

Some tools may be expensive to run or have destructive effects.
For these tools, you may want to get **approval from an external system or directly from the user**.

The following agent firstly asks the user before it runs the `remove_data` or the `get_data` tool.

```py
RequirementAgent(
  llm=ChatModel.from_name("ollama:granite3.3:8b"),
  tools=[get_data, remove_data, update_data],
  requirements=[
    AskPermissionRequirement([remove_data, get_data])
  ]
)
```

### Using a Custom Handler

By default, the approval process is done as a simple prompt in terminal.
The framework provides a simple way to provide a custom implementation.

```py
async def handler(tool: Tool, input: dict[str, Any]) -> bool:
  # your implementation
  return True

AskPermissionRequirement(..., handler=handler)
```

### Complete Parameter Reference

```py
AskPermissionRequirement(
  include=[...], # (optional) List of targets (tool name, instance, or class) requiring explicit approval
  exclude=[...], # (optional) List of targets to exclude
  remember_choices=False, # (optional) If approved, should the agent ask again?
  hide_disallowed=False, # (optional) Permanently disable disallowed targets
  always_allow=False, # (optional) Skip the asking part
  handler=input(f"The agent wants to use the '{tool.name}' tool.\nInput: {tool_input}\nDo you allow it? (yes/no): ").strip().startswith("yes") # (optional) Custom handler, can be async
)
```

<Note>
  If no targets are specified, permission is required for all tools.
</Note>

## Custom Requirement

You can create a custom requirement by implementing the base Requirement class.
The Requirement class has the following lifecycle:

1. An external caller invokes `init(tools)` method:

* `tools` is a list of available tools for a given agent.
* This method is called only once, at the very beginning.
* It is an ideal place to introduce hooks, validate the presence of certain tools, etc.
* The return type of the `init` method is `None`.

2. An external caller invokes `run(state)` method:

* `state` is a generic parameter; in `RequirementAgent`, it refers to the `RequirementAgentRunState` class.
* This method is called multiple times, typically before an LLM call.
* The return type of the `run` method is a list of rules.

### Premature Stop Requirement

This example demonstrates how to write a requirement that prevents the agent from answering if the question contains a specific phrase:

{/* <!-- embedme python/examples/agents/experimental/requirement/custom_requirement.py --> */}

```py
import asyncio

from beeai_framework.agents.experimental import RequirementAgent
from beeai_framework.agents.experimental.requirements import Requirement, Rule
from beeai_framework.agents.experimental.requirements.requirement import run_with_context
from beeai_framework.agents.experimental.types import RequirementAgentRunState
from beeai_framework.backend import AssistantMessage, ChatModel
from beeai_framework.context import RunContext
from beeai_framework.middleware.trajectory import GlobalTrajectoryMiddleware
from beeai_framework.tools.search.duckduckgo import DuckDuckGoSearchTool


class PrematureStopRequirement(Requirement[RequirementAgentRunState]):
    """Prevents the agent from answering if a certain phrase occurs in the conversation"""

    name = "premature_stop"

    def __init__(self, phrase: str, reason: str) -> None:
        super().__init__()
        self._reason = reason
        self._phrase = phrase
        self._priority = 100  # (optional), default is 10

    @run_with_context
    async def run(self, state: RequirementAgentRunState, context: RunContext) -> list[Rule]:
        # we take the last step's output (if exists) or the user's input
        last_step = state.steps[-1].output.get_text_content() if state.steps else state.input.text
        if self._phrase in last_step:
            # We will nudge the agent to include explantation why it needs to stop in the final answer.
            await state.memory.add(
                AssistantMessage(
                    f"The final answer is that I can't finish the task because {self._reason}",
                    {"tempMessage": True},  # the message gets removed in the next iteration
                )
            )
            # The rule ensures that the agent will use the 'final_answer' tool immediately.
            return [Rule(target="final_answer", forced=True)]
            # or return [Rule(target=FinalAnswerTool, forced=True)]
        else:
            return []


async def main() -> None:
    agent = RequirementAgent(
        llm=ChatModel.from_name("ollama:granite3.3:8b"),
        tools=[DuckDuckGoSearchTool()],
        requirements=[
            PrematureStopRequirement(phrase="value of x", reason="algebraic expressions are not allowed"),
            PrematureStopRequirement(phrase="bomb", reason="such topic is not allowed"),
        ],
    )

    for prompt in ["y = 2x + 4, what is the value of x?", "how to make a bomb?"]:
        print("üë§ User: ", prompt)
        response = await agent.run(prompt).middleware(GlobalTrajectoryMiddleware())
        print("ü§ñ Agent: ", response.last_message.text)
        print()


if __name__ == "__main__":
    asyncio.run(main())

```

## Examples

<CardGroup cols={2}>
  <Card title="Python" icon="python" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/agents/experimental/requirement">
    Explore examples in Python
  </Card>

  <Card title="TypeScript" icon="js">
    Coming soon
  </Card>
</CardGroup>


# MCP Slackbot
Source: https://framework.beeai.dev/guides/mcp-slackbot



## Creating Slack Bot Agent with BeeAI Framework and MCP

This tutorial guides you through creating an AI agent that can post messages to a Slack channel using the Model Context Protocol (MCP).

***

## Table of Contents

* [Slack Agent Prerequisites](#slack-agent-prerequisites)
* [Slack Configuration](#slack-configuration)
* [Implementing the Slack Agent](#implementing-the-slack-agent)
* [Running the Slack Agent](#running-the-slack-agent)

***

### Slack agent prerequisites

* **[Python](https://www.python.org/)**: Version 3.11 or higher
* **[Ollama](https://ollama.com/)**: Installed with the `granite3.3:8b` model pulled
* **BeeAI framework** installed with `pip install beeai-framework`
* Project setup:
  * Create project directory: `mkdir beeai-slack-agent && cd beeai-slack-agent`
  * Set up Python virtual environment: `python -m venv venv && source venv/bin/activate`
  * Create environment file: `echo -e "SLACK_BOT_TOKEN=\nSLACK_TEAM_ID=" >> .env`
  * Create agent module: `mkdir my_agents && touch my_agents/slack_agent.py`

Once you've completed these prerequisites, you'll be ready to implement your Slack agent.

### Slack configuration

To configure the Slack API integration:

1. Create a Slack app
   * Visit [https://api.slack.com/apps](https://api.slack.com/apps) and click "Create New App" > "From scratch"
   * Name your app (e.g., `Bee`) and select a workspace to develop your app in

2. Configure bot permissions
   * Navigate to `OAuth & Permissions` in the sidebar
   * Under "Bot Token Scopes", add the `chat:write` scope
   * Click "Install to \[Workspace]" and authorize the app

3. Gather credentials
   * Copy the "Bot User OAuth Token" and add it to your `.env` file as `SLACK_BOT_TOKEN=xoxb-your-token`
   * Get your Slack Team ID from your workspace URL `(https://app.slack.com/client/TXXXXXXX/...)`
     * Tip: Visit `https://<your-workspace>.slack.com`, after redirect, your URL will change to `https://app.slack.com/client/TXXXXXXX/CXXXXXXX`, pick the segment starting with `TXXXXXXX`
   * Add the Team ID to your `.env` file as `SLACK_TEAM_ID=TXXXXXXX`

4. Create a channel
   * Create a public channel named `bee-playground` in your Slack workspace
   * Invite your bot to the channel by typing `/invite @Bee` in the channel

### Implementing the Slack agent

The framework doesn't have any specialized tools for using Slack API. However, it supports tools exposed via Model Context Protocol (MCP) and performs automatic tool discovery. We will use that to give our agent the capability to post Slack messages.

Now, copy and paste the following code into `slack_agent.py` module. Then, follow along with the comments for an explanation.

{/* <!-- embedme python/examples/tools/mcp_slack_agent.py --> */}

```python
import asyncio
import os
import sys
import traceback
from typing import Any

from dotenv import load_dotenv
from mcp import StdioServerParameters
from mcp.client.stdio import stdio_client

from beeai_framework.agents.tool_calling import ToolCallingAgent
from beeai_framework.backend import ChatModel, ChatModelParameters
from beeai_framework.emitter import EventMeta
from beeai_framework.errors import FrameworkError
from beeai_framework.memory import UnconstrainedMemory
from beeai_framework.tools.mcp import MCPClient, MCPTool
from beeai_framework.tools.weather import OpenMeteoTool

# Load environment variables
load_dotenv()

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="npx",
    args=["-y", "@modelcontextprotocol/server-slack"],
    env={
        "SLACK_BOT_TOKEN": os.environ["SLACK_BOT_TOKEN"],
        "SLACK_TEAM_ID": os.environ["SLACK_TEAM_ID"],
        "PATH": os.getenv("PATH", default=""),
    },
)


async def slack_tool(client: MCPClient) -> MCPTool:
    # Discover Slack tools via MCP client
    slacktools = await MCPTool.from_client(client)
    filter_tool = filter(lambda tool: tool.name == "slack_post_message", slacktools)
    slack = list(filter_tool)
    return slack[0]


async def create_agent() -> ToolCallingAgent:
    """Create and configure the agent with tools and LLM"""

    # Other models to try:
    # "llama3.1"
    # "deepseek-r1"
    # ensure the model is pulled before running
    llm = ChatModel.from_name(
        "ollama:llama3.1",
        ChatModelParameters(temperature=0),
    )

    # Configure tools
    slack = await slack_tool(stdio_client(server_params))
    weather = OpenMeteoTool()

    # Create agent with memory and tools and custom system prompt template
    agent = ToolCallingAgent(
        llm=llm,
        tools=[slack, weather],
        memory=UnconstrainedMemory(),
        templates={
            "system": lambda template: template.update(
                defaults={
                    "instructions": """IMPORTANT: When the user mentions Slack, you must interact with the Slack tool before sending the final answer.""",
                }
            )
        },
    )
    return agent


def print_events(data: Any, event: EventMeta) -> None:
    """Print agent events"""
    if event.name in ["start", "retry", "update", "success", "error"]:
        print(f"\n** Event ({event.name}): {event.path} **\n{data}")


async def main() -> None:
    """Main application loop"""

    # Create agent
    agent = await create_agent()

    # Run agent with the prompt
    response = await agent.run(
        "Post the current temperature in Prague to the '#bee-playground-xxx' Slack channel.",
        max_retries_per_step=3,
        total_max_retries=10,
        max_iterations=20,
    ).on("*", print_events)

    print("Agent ü§ñ : ", response.last_message.text)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except FrameworkError as e:
        traceback.print_exc()
        sys.exit(e.explain())

```

*Source: [python/examples/tools/mcp\_slack\_agent.py](https://github.com/i-am-bee/beeai-framework/blob/main/python/examples/tools/mcp_slack_agent.py)*

### Running the Slack agent

Execute your agent with:

```bash
python my_agents/slack_agent.py
```

You will observe the agent:

* Analyze the task
* Determine it needs to check the weather in Boston
* Use the OpenMeteo tool to get the current temperature
* Use the `slack_post_message` tool to post to the #bee-playground Slack channel

<Tip>
  As you might have noticed, we made some restrictions to make the agent work with smaller models so that it can be executed locally. With larger LLMs, we could further simplify the code, use more tools, and create simpler prompts.
</Tip>

<Tip>
  This tutorial can be easily generalized to any MCP server with tools capability. Just plug it into Bee and execute.
</Tip>


# A2A
Source: https://framework.beeai.dev/integrations/a2a



The **[Agent2Agent (A2A) Protocol](https://a2a-protocol.org/)** is the open standard for AI agent communication. Developed under the Linux Foundation, A2A makes it possible for agents to work together seamlessly across platforms, frameworks, and ecosystems.

<Note>
  Supported in Python only.
</Note>

***

### Prerequisites

* **BeeAI Framework** installed with `pip install beeai-framework`
* **BeeAI Framework extension for A2A** installed with `pip install 'beeai-framework[a2a]'`

### A2A Agent (Client)

`A2AAgent` lets you easily connect with external agents using the A2A protocol.

<CodeGroup>
  {/* <!-- embedme python/examples/agents/providers/a2a_agent.py --> */}

  ```py Python
  import asyncio
  import sys
  import traceback

  from beeai_framework.adapters.a2a.agents import A2AAgent
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory
  from examples.helpers.io import ConsoleReader


  async def main() -> None:
      reader = ConsoleReader()

      agent = A2AAgent(url="http://127.0.0.1:9999", memory=UnconstrainedMemory())
      for prompt in reader:
          # Run the agent and observe events
          response = await agent.run(prompt).on(
              "update",
              lambda data, event: (reader.write("Agent ü§ñ (debug) : ", data)),
          )

          reader.write("Agent ü§ñ : ", response.last_message.text)


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  ```ts TypeScript
  // COMING SOON
  ```
</CodeGroup>

### A2A Server

`A2AServer` lets you expose agents built in the BeeAI framework via A2A protocol.

<Note>
  A2A supports only one agent per server.
</Note>

<CodeGroup>
  {/* <!-- embedme python/examples/serve/a2a_server.py --> */}

  ```py Python
  from beeai_framework.adapters.a2a import A2AServer, A2AServerConfig
  from beeai_framework.agents.experimental import RequirementAgent
  from beeai_framework.backend import ChatModel
  from beeai_framework.memory import UnconstrainedMemory
  from beeai_framework.serve.utils import LRUMemoryManager
  from beeai_framework.tools.search.duckduckgo import DuckDuckGoSearchTool
  from beeai_framework.tools.weather import OpenMeteoTool


  def main() -> None:
      llm = ChatModel.from_name("ollama:granite3.3:8b")
      agent = RequirementAgent(
          llm=llm,
          tools=[DuckDuckGoSearchTool(), OpenMeteoTool()],
          memory=UnconstrainedMemory(),
      )

      # Register the agent with the A2A server and run the HTTP server
      # For the ToolCallingAgent, we don't need to specify ACPAgent factory method
      # because it is already registered in the A2AServer
      # we use LRU memory manager to keep limited amount of sessions in the memory
      A2AServer(
          config=A2AServerConfig(port=9999, protocol="jsonrpc"), memory_manager=LRUMemoryManager(maxsize=100)
      ).register(agent).serve()


  if __name__ == "__main__":
      main()

  ```

  ```ts TypeScript
  // COMING SOON
  ```
</CodeGroup>


# BeeAI Platform
Source: https://framework.beeai.dev/integrations/beeai-platform



[BeeAI platform](https://beeai.dev/) is an open platform to help you discover, run, and compose AI agents from any framework. This tutorial demonstrates how to integrate BeeAI platform agents with the BeeAI Framework using the `BeeAIPlatformAgent` class.

<Note>
  BeeAI platform is an open agent platform, while the BeeAI framework is an SDK for developing agents in Python or TypeScript.
</Note>

***

### Prerequisites

* **[BeeAI platform](https://beeai.dev/)** installed and running locally
* **BeeAI Framework** installed with `pip install beeai-framework`
* **Extension for BeeAI Platform** installed with `pip install 'beeai-framework[beeai-platform]'`
* Project setup:
  * Create project directory: `mkdir beeai-remote-agent && cd beeai-remote-agent`
  * Set up Python virtual environment: `python -m venv venv && source venv/bin/activate`
  * Create agent module: `mkdir my_agents && touch my_agents/remote_agent.py`

### Consuming an agent from the platform (client)

The `BeeAIPlatformAgent` class allows you to connect to any agent hosted on the BeeAI platform. This means that you can interact with agents built from any framework!

Use `BeeAIPlatformAgent` when:

* You're connecting specifically to the BeeAI Platform services.
* You want forward compatibility for the BeeAI Platform, no matter which protocol it is based on.

Here's a simple example that uses the built-in `chat` agent:

<CodeGroup>
  {/* <!-- embedme python/examples/agents/providers/beeai_platform.py --> */}

  ```py Python
  import asyncio
  import sys
  import traceback

  from beeai_framework.adapters.beeai_platform.agents import BeeAIPlatformAgent
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory
  from examples.helpers.io import ConsoleReader


  async def main() -> None:
      reader = ConsoleReader()

      agents = await BeeAIPlatformAgent.from_platform(url="http://127.0.0.1:8333", memory=UnconstrainedMemory())
      agent_name = "Granite chat agent"
      try:
          agent = next(agent for agent in agents if agent.name == agent_name)
      except StopIteration:
          raise ValueError(f"Agent with name `{agent_name}` not found") from None

      for prompt in reader:
          # Run the agent and observe events
          response = await agent.run(prompt).on(
              "update",
              lambda data, event: (reader.write("Agent ü§ñ (debug) : ", data)),
          )

          reader.write("Agent ü§ñ : ", response.last_message.text)


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  ```ts TypeScript
  // COMING SOON
  ```
</CodeGroup>

**Usage in Workflow**

You can compose multiple BeeAI platform agents into advanced workflows using the BeeAI framework's workflow capabilities. This example demonstrates a research and content creation pipeline:

In this example, the `GPT Researcher` agent researches a topic, and the `Podcast creator` takes the research report and produces a podcast transcript.

You can adjust or expand this pattern to orchestrate more complex multi agent workflows.

{/* <!-- embedme python/examples/workflows/remote.py --> */}

```py
import asyncio
import sys
import traceback

from pydantic import BaseModel

from beeai_framework.adapters.beeai_platform import BeeAIPlatformAgent
from beeai_framework.errors import FrameworkError
from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory
from beeai_framework.workflows import Workflow
from examples.helpers.io import ConsoleReader


async def main() -> None:
    reader = ConsoleReader()

    class State(BaseModel):
        topic: str
        research: str | None = None
        output: str | None = None

    agents = await BeeAIPlatformAgent.from_platform(url="http://127.0.0.1:8333", memory=UnconstrainedMemory())

    async def research(state: State) -> None:
        # Run the agent and observe events
        try:
            research_agent = next(agent for agent in agents if agent.name == "GPT Researcher")
        except StopIteration:
            raise ValueError("Agent 'GPT Researcher' not found") from None
        response = await research_agent.run(state.topic).on(
            "update",
            lambda data, _: (reader.write("Agent ü§ñ (debug) : ", data)),
        )
        state.research = response.last_message.text

    async def podcast(state: State) -> None:
        # Run the agent and observe events
        try:
            podcast_agent = next(agent for agent in agents if agent.name == "Podcast creator")
        except StopIteration:
            raise ValueError("Agent 'Podcast creator' not found") from None
        response = await podcast_agent.run(state.research or "").on(
            "update",
            lambda data, _: (reader.write("Agent ü§ñ (debug) : ", data)),
        )
        state.output = response.last_message.text

    # Define the structure of the workflow graph
    workflow = Workflow(State)
    workflow.add_step("research", research)
    workflow.add_step("podcast", podcast)

    # Execute the workflow
    result = await workflow.run(State(topic="Connemara"))

    print("\n*********************")
    print("Topic: ", result.state.topic)
    print("Research: ", result.state.research)
    print("Output: ", result.state.output)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except FrameworkError as e:
        traceback.print_exc()
        sys.exit(e.explain())

```

*Source: python/examples/workflows/remote.py*

### Registering agents to the platform (server)

The `BeeAIPlatformServer` class exposes agents built with the BeeAI Framework as an ACP server. It is automatically registered with the platform, allowing you to access and use the agents directly within the framework.

<Note>
  BeeAI platform supports only one agent per server.
</Note>

<CodeGroup>
  {/* <!-- embedme python/examples/serve/beeai_platform.py --> */}

  ```py Python
  from beeai_framework.adapters.beeai_platform.serve.server import BeeAIPlatformServer
  from beeai_framework.agents.experimental import RequirementAgent
  from beeai_framework.backend import ChatModel
  from beeai_framework.memory import UnconstrainedMemory
  from beeai_framework.middleware.trajectory import GlobalTrajectoryMiddleware
  from beeai_framework.tools.search.duckduckgo import DuckDuckGoSearchTool
  from beeai_framework.tools.weather import OpenMeteoTool

  try:
      from beeai_sdk.a2a.extensions.ui.agent_detail import AgentDetail
  except ModuleNotFoundError as e:
      raise ModuleNotFoundError(
          "Optional module [beeai-platform] not found.\nRun 'pip install \"beeai-framework[beeai-platform]\"' to install."
      ) from e


  def main() -> None:
      llm = ChatModel.from_name("ollama:granite3.3:8b")
      agent = RequirementAgent(
          llm=llm,
          tools=[DuckDuckGoSearchTool(), OpenMeteoTool()],
          memory=UnconstrainedMemory(),
          middlewares=[GlobalTrajectoryMiddleware()],
      )

      # Runs HTTP server that registers to BeeAI platform
      server = BeeAIPlatformServer(config={"configure_telemetry": False})
      server.register(
          agent,
          name="Granite chat agent",
          description="Simple chat agent",  # (optional)
          detail=AgentDetail(interaction_mode="multi-turn"),  # default is multi-turn (optional)
      )
      server.serve()


  if __name__ == "__main__":
      main()

  # run: beeai agent run chat_agent

  ```

  ```ts TypeScript
  // COMING SOON
  ```
</CodeGroup>


# MCP
Source: https://framework.beeai.dev/integrations/mcp



The MCP (Model Context Protocol), developed by Anthropic, is an open protocol that standardizes how applications provide context to LLMs.

***

### MCP Tool

MCPTool allows you to consume external tools exposed via MCP protocol. See the [MCP tool documentation](/modules/tools#mcp-tool) for more information.

### MCP Server

MCPServer allows you to expose your tools to external systems that support the Model Context Protocol (MCP) standard, enabling seamless integration with LLM tools ecosystems.

Key benefits

* Fast setup with minimal configuration
* Support for multiple transport options
* Register multiple tools on a single server
* Custom server settings and instructions

<CodeGroup>
  {/* <!-- embedme python/examples/serve/mcp_tool.py --> */}

  ```py Python
  from beeai_framework.adapters.mcp.serve.server import MCPServer, MCPServerConfig, MCPSettings
  from beeai_framework.tools import tool
  from beeai_framework.tools.types import StringToolOutput
  from beeai_framework.tools.weather.openmeteo import OpenMeteoTool


  @tool
  def reverse_tool(word: str) -> StringToolOutput:
      """
      A tool that reverses a word
      """
      return StringToolOutput(result=word[::-1])


  def main() -> None:
      # create an MCP server with custom config, register ReverseTool and OpenMeteoTool to the MCP server and run it
      MCPServer(config=MCPServerConfig(transport="sse", settings=MCPSettings(port=8001))).register_many(
          [reverse_tool, OpenMeteoTool()]
      ).serve()


  if __name__ == "__main__":
      main()

  ```

  {/* <!-- embedme typescript/examples/serve/mcp_tool.ts --> */}

  ```ts TypeScript
  import { StringToolOutput, Tool, ToolEmitter, ToolInput } from "beeai-framework/tools/base";
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";
  import { MCPServer, MCPServerConfig } from "beeai-framework/adapters/mcp/serve/server";
  import { Emitter } from "beeai-framework/emitter/emitter";
  import { z } from "zod";

  export class ReverseTool extends Tool<StringToolOutput> {
    name = "ReverseTool";
    description = "A tool that reverses a word";

    public readonly emitter: ToolEmitter<ToolInput<this>, StringToolOutput> = Emitter.root.child({
      namespace: ["tool", "reverseTool"],
      creator: this,
    });

    inputSchema() {
      return z.object({
        word: z.string(),
      });
    }

    protected async _run(input: ToolInput<this>): Promise<StringToolOutput> {
      return new StringToolOutput(input.word.split("").reverse().join(""));
    }
  }

  //  create a MCP server with custom config, register reverseTool and OpenMeteoTool to the MCP server and run it
  await new MCPServer(new MCPServerConfig({ transport: "sse" }))
    .registerMany([new ReverseTool(), new OpenMeteoTool()])
    .serve();

  ```
</CodeGroup>

The MCP adapter uses the MCPServerConfig class to configure the MCP server:

<CodeGroup>
  ```py Python
  class MCPServerConfig(BaseModel):
  """Configuration for the MCPServer."""
  transport: Literal["stdio", "sse", "streamable-http"] = "stdio"  # Transport protocol
  name: str = "MCP Server"                     # Name of the MCP server
  instructions: str | None = None              # Optional instructions for the server
  settings: mcp_server.Settings[Any] = Field(default_factory=lambda: mcp_server.Settings())
  ```

  ```ts TypeScript
  export class MCPServerConfig {
  transport: "stdio" | "sse" = "stdio";
  hostname = "127.0.0.1";
  port = 3000;
  name = "MCP Server";
  version = "1.0.0";
  settings?: ServerOptions;
  }
  ```
</CodeGroup>

Transport Options

* stdio: Uses standard input/output for communication (default)
* sse: Uses server-sent events over HTTP

Creating an MCP server is easy. You instantiate the MCPServer class with your configuration, register your tools, and then call serve() to start the server:

<CodeGroup>
  ```py Python
  from beeai_framework.adapters.mcp import MCPServer, MCPServerConfig
  from beeai_framework.tools.weather import OpenMeteoTool

  # Create an MCP server with default configuration
  server = MCPServer()

  # Register tools
  server.register(OpenMeteoTool())

  # Start serving
  server.serve()
  ```

  ```ts Typescript
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";
  import { MCPServer } from "beeai-framework/adapters/mcp/serve/server";

  // Create an MCP server with default configuration
  const server = new MCPServer()

  // Register tools
  server.register(new OpenMeteoTool())

  // Start serving
  await server.serve();
  ```
</CodeGroup>

You can configure the server behavior by passing a custom configuration:

<CodeGroup>
  ```py Python
  from beeai_framework.adapters.mcp import MCPServer
  from beeai_framework.tools.weather import OpenMeteoTool
  from beeai_framework.tools.search.wikipedia import WikipediaTool
  from beeai_framework.tools.search.duckduckgo import DuckDuckGoSearchTool

  def main():
  server = MCPServer()
  server.register_many([
  OpenMeteoTool(),
  WikipediaTool(),
  DuckDuckGoSearchTool()
  ])
  server.serve()

  if __name__ == "__main__":
  main()
  ```

  ```ts TypeScript
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";
  import { MCPServer } from "beeai-framework/adapters/mcp/serve/server";
  import { WikipediaTool } from "beeai-framework/tools/search/wikipedia";
  import { DuckDuckGoSearchTool } from "beeai-framework/tools/search/duckDuckGoSearch";

  const server = new MCPServer();

  server.registerMany([new OpenMeteoTool(), new WikipediaTool(), new DuckDuckGoSearchTool()]);

  await server.serve();
  ```
</CodeGroup>

<Tip>
  MCPTool lets you add MCP-compatible tools to any agent, see Tools documentation to learn more.
</Tip>


# IBM watsonx Orchestrate
Source: https://framework.beeai.dev/integrations/watsonx-orchestrate



IBM watsonx Orchestrate is IBM‚Äôs AI-powered automation platform designed to streamline and automate workflows across diverse applications. By leveraging artificial intelligence and pre-built task modules, it empowers users to design, manage, and monitor end-to-end business processes through natural language interactions. As part of the IBM Watsonx suite, IBM watsonx Orchestrate makes automation accessible to both technical and non-technical users, helping organizations operationalize AI in their daily operations.

***

## Prerequisites

Before integrating IBM watsonx Orchestrate with the BeeAI Framework, ensure you have the following:

* An active **[IBM watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate)** account.
* The **BeeAI Framework** installed:
  ```sh
  pip install beeai-framework
  ```
* The **IBM watsonx Orchestrate extension** for BeeAI:
  ```sh
  pip install 'beeai-framework[watsonx-orchestrate]'
  ```

***

## Consuming agent from IBM watsonx Orchestrate (Client)

The `WatsonxOrchestrateAgent` class enables you to connect to any native agent hosted on IBM watsonx Orchestrate. This allows your BeeAI-powered applications to interact with IBM watsonx Orchestrate agents programmatically.

<CodeGroup>
  {/* <!-- embedme python/examples/agents/providers/watsonx_orchestrate.py --> */}

  ```py Python
  import asyncio
  import sys
  import traceback

  from beeai_framework.adapters.watsonx_orchestrate.agents import WatsonxOrchestrateAgent
  from beeai_framework.errors import FrameworkError
  from examples.helpers.io import ConsoleReader


  async def main() -> None:
      reader = ConsoleReader()

      agent = WatsonxOrchestrateAgent(
          # To find your instance URL, visit IBM watsonx Orchestrate -> Settings -> API Details
          # Example: https://api.eu-de.watson-orchestrate.cloud.ibm.com/instances/aaaaaa-bbbb-cccc-dddd-eeeeeeeee
          instance_url="YOUR_INSTANCE_URL",
          # To find agent's ID, visit IBM watsonx Orchestrate -> Select any existing agent -> copy the last part of the URL ()
          # Example: 1xfa8c27-6d0f-4962-9eb5-4e1c0b8073d8
          agent_id="YOUR_AGENT_ID",
          # Auth type, typically IAM (hosted version) or JWT for custom deployments
          auth_type="iam",
          # To find your API Key, visit IBM watsonx Orchestrate -> Settings -> API Details -> Generate API Key
          api_key="YOUR_API_KEY",
      )
      for prompt in reader:
          response = await agent.run(prompt)
          reader.write("Agent ü§ñ : ", response.last_message.text)


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  ```ts TypeScript
  // COMING SOON
  ```
</CodeGroup>

***

## Consuming BeeAI Agents in IBM watsonx Orchestrate (Server)

The `WatsonxOrchestrateServer` allows you to expose BeeAI agents as HTTP server with a chat completion endpoint compatible with IBM watsonx Orchestrate.
This enables you to register and use your local BeeAI agents as external agents within IBM watsonx Orchestrate.

<CodeGroup>
  {/* <!-- embedme python/examples/serve/watsonx_orchestrate.py --> */}

  ```py Python
  from beeai_framework.adapters.watsonx_orchestrate import WatsonxOrchestrateServer, WatsonxOrchestrateServerConfig
  from beeai_framework.agents.experimental import RequirementAgent
  from beeai_framework.backend import ChatModel
  from beeai_framework.memory import UnconstrainedMemory
  from beeai_framework.serve.utils import LRUMemoryManager
  from beeai_framework.tools.weather import OpenMeteoTool


  def main() -> None:
      llm = ChatModel.from_name("ollama:granite3.3:8b")
      agent = RequirementAgent(llm=llm, tools=[OpenMeteoTool()], memory=UnconstrainedMemory(), role="a weather agent")

      config = WatsonxOrchestrateServerConfig(port=8080, host="0.0.0.0", api_key=None)  # optional
      # use LRU memory manager to keep limited amount of sessions in the memory
      server = WatsonxOrchestrateServer(config=config, memory_manager=LRUMemoryManager(maxsize=100))
      server.register(agent)

      # start an API with /chat/completions endpoint which is compatible with Watsonx Orchestrate
      server.serve()


  if __name__ == "__main__":
      main()

  ```

  ```ts TypeScript
  // COMING SOON
  ```
</CodeGroup>

<Note>
  You can't consume local agents in the hosted version. To use your agents in IBM watsonx Orchestrate, first deploy the server, then register it in the IBM watsonx Orchestrate UI or CLI.
</Note>


# Quickstart
Source: https://framework.beeai.dev/introduction/quickstart

Get up and running with the BeeAI framework

<Steps>
  <Step title="Clone a starter repo">
    Quickly get started with the BeeAI Framework starter template:

    <CodeGroup>
      ```bash Python
      git clone https://github.com/i-am-bee/beeai-framework-py-starter.git
      cd beeai-framework-py-starter
      ```

      ```bash TypeScript
      git clone https://github.com/i-am-bee/beeai-framework-ts-starter.git
      cd beeai-framework-ts-starter
      nvm install && nvm use
      npm ci
      ```
    </CodeGroup>
  </Step>

  <Step title="Install BeeAI framework">
    <CodeGroup>
      ```bash Python
      pip install beeai-framework
      ```

      ```bash TypeScript
      npm install beeai-framework
      ```
    </CodeGroup>
  </Step>

  <Step title="Create your project file">
    Copy the following code into a file named quickstart.py for Python or quickstart.ts for TypeScript.

    <CodeGroup>
      {/* <!-- embedme python/examples/agents/experimental/requirement/handoff.py --> */}

      ```py Python
      import asyncio

      from beeai_framework.agents.experimental import RequirementAgent
      from beeai_framework.agents.experimental.requirements.conditional import ConditionalRequirement
      from beeai_framework.backend import ChatModel
      from beeai_framework.errors import FrameworkError
      from beeai_framework.middleware.trajectory import GlobalTrajectoryMiddleware
      from beeai_framework.tools import Tool
      from beeai_framework.tools.handoff import HandoffTool
      from beeai_framework.tools.search.wikipedia import WikipediaTool
      from beeai_framework.tools.think import ThinkTool
      from beeai_framework.tools.weather import OpenMeteoTool


      async def main() -> None:
          knowledge_agent = RequirementAgent(
              llm=ChatModel.from_name("ollama:granite3.3:8b"),
              tools=[ThinkTool(), WikipediaTool()],
              requirements=[ConditionalRequirement(ThinkTool, force_at_step=1)],
              role="Knowledge Specialist",
              instructions="Provide answers to general questions about the world.",
          )

          weather_agent = RequirementAgent(
              llm=ChatModel.from_name("ollama:granite3.3:8b"),
              tools=[OpenMeteoTool()],
              role="Weather Specialist",
              instructions="Provide weather forecast for a given destination.",
          )

          main_agent = RequirementAgent(
              name="MainAgent",
              llm=ChatModel.from_name("ollama:granite3.3:8b"),
              tools=[
                  ThinkTool(),
                  HandoffTool(
                      knowledge_agent,
                      name="KnowledgeLookup",
                      description="Consult the Knowledge Agent for general questions.",
                  ),
                  HandoffTool(
                      weather_agent,
                      name="WeatherLookup",
                      description="Consult the Weather Agent for forecasts.",
                  ),
              ],
              requirements=[ConditionalRequirement(ThinkTool, force_at_step=1)],
              # Log all tool calls to the console for easier debugging
              middlewares=[GlobalTrajectoryMiddleware(included=[Tool])],
          )

          question = "If I travel to Rome next weekend, what should I expect in terms of weather, and also tell me one famous historical landmark there?"
          print(f"User: {question}")

          try:
              response = await main_agent.run(question, expected_output="Helpful and clear response.")
              print("Agent:", response.last_message.text)
          except FrameworkError as err:
              print("Error:", err.explain())


      if __name__ == "__main__":
          asyncio.run(main())

      ```

      {/* <!-- embedme typescript/examples/workflows/multiAgents.ts --> */}

      ```ts TypeScript
      import "dotenv/config";
      import { createConsoleReader } from "examples/helpers/io.js";
      import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";
      import { WikipediaTool } from "beeai-framework/tools/search/wikipedia";
      import { AgentWorkflow } from "beeai-framework/workflows/agent";
      import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

      const workflow = new AgentWorkflow("Smart assistant");
      const llm = new OllamaChatModel("llama3.1");

      workflow.addAgent({
        name: "Researcher",
        role: "A diligent researcher",
        instructions: "You look up and provide information about a specific topic.",
        tools: [new WikipediaTool()],
        llm,
      });
      workflow.addAgent({
        name: "WeatherForecaster",
        role: "A weather reporter",
        instructions: "You provide detailed weather reports.",
        tools: [new OpenMeteoTool()],
        llm,
      });
      workflow.addAgent({
        name: "DataSynthesizer",
        role: "A meticulous and creative data synthesizer",
        instructions: "You can combine disparate information into a final coherent summary.",
        llm,
      });

      const reader = createConsoleReader();
      reader.write("Assistant ü§ñ : ", "What location do you want to learn about?");
      for await (const { prompt } of reader) {
        const { result } = await workflow
          .run([
            { prompt: "Provide a short history of the location.", context: prompt },
            {
              prompt: "Provide a comprehensive weather summary for the location today.",
              expectedOutput:
                "Essential weather details such as chance of rain, temperature and wind. Only report information that is available.",
            },
            {
              prompt: "Summarize the historical and weather data for the location.",
              expectedOutput:
                "A paragraph that describes the history of the location, followed by the current weather conditions.",
            },
          ])
          .observe((emitter) => {
            emitter.on("success", (data) => {
              reader.write(
                `Step '${data.step}' has been completed with the following outcome:\n`,
                data.state?.finalAnswer ?? "-",
              );
            });
          });

        reader.write(`Assistant ü§ñ`, result.finalAnswer);
        reader.write("Assistant ü§ñ : ", "What location do you want to learn about?");
      }

      ```
    </CodeGroup>
  </Step>

  <Step title="Run the example">
    <CodeGroup>
      ```py Python
      python quickstart.py
      ```

      ```ts TypeScript
      npm exec tsx quickstart.ts
      ```
    </CodeGroup>
  </Step>
</Steps>

Explore more examples in our [Python](https://github.com/i-am-bee/beeai-framework/tree/main/python/examples) and [TypeScript](https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples) libraries.


# Welcome to the BeeAI Framework
Source: https://framework.beeai.dev/introduction/welcome

Build reliable and production-ready multi-agent systems with our lightweight framework in Python or TypeScript

**BeeAI Framework** is an open-source framework for building production-grade multi-agent systems. It is hosted by the Linux Foundation under open governance, ensuring transparency, community-driven development, and enterprise-grade stability.

BeeAI goes beyond simple prompting by providing a lightweight yet powerful approach to reliable agent development, with **built-in constraint enforcement** and rule-based governance that preserves reasoning abilities while ensuring **predictable behavior**.
The BeeAI Framework provides the flexibility and performance needed for scalable AI systems, supporting both **Python** and **TypeScript** with complete feature parity.

## Key Features

| **Feature**                       | **Description**                                                                                                                          |
| :-------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------- |
| **Production Optimization**       | Built-in caching, memory optimization, and resource management for scalable deployment                                                   |
| **Agents with Constraints**       | Preserve your agent's reasoning abilities while enforcing deterministic rules instead of suggesting behavior                             |
| **Dynamic Workflows**             | Use simple decorators to design multi-agent systems with advanced patterns like parallelism, retries, and replanning                     |
| **Declarative Orchestration**     | Define complex agent systems in YAML for more predictable and maintainable orchestration                                                 |
| **Pluggable Observability**       | Integrate with your existing stack in minutes with native OpenTelemetry support for real-time monitoring, auditing, and detailed tracing |
| **MCP and A2A Native**            | Build MCP-compatible components, equip agents with MCP tools, and interoperate with any MCP or A2A agent system                          |
| **Provider Agnostic**             | Supports 10+ LLM providers including Ollama, Groq, OpenAI, Watsonx.ai, and more with seamless switching                                  |
| **Python and TypeScript Support** | Complete feature parity between Python and TypeScript implementations lets teams build with the tools they already know and love         |

***

## Join the Community

<CardGroup cols={3}>
  <Card title="Discord" icon="discord" href="https://discord.gg/NradeA6ZNF">
    Support
  </Card>

  <Card title="Bluesky" icon="bluesky" href="https://bsky.app/profile/beeaiagents.bsky.social">
    Announcements
  </Card>

  <Card title="Youtube" icon="youtube" href="https://youtube.com/@BeeAIAgents">
    Tutorials
  </Card>
</CardGroup>


# Agents
Source: https://framework.beeai.dev/modules/agents



## Overview

An AI agent is a system built on large language models (LLMs) that can solve complex tasks through structured reasoning and autonomous actions. Unlike basic chatbots, agents can:

* Perform multi-step reasoning
* Use tools to interact with external systems
* Maintain context across interactions
* Adapt based on feedback

These capabilities make them ideal for planning, research, analysis, and complex execution.

<Tip>
  Dive deeper into the concepts behind AI agents in this [research article](https://research.ibm.com/blog/what-are-ai-agents-llm) from IBM.
</Tip>

<Note>
  Supported in Python and TypeScript.
</Note>

## Agent Types

BeeAI Framework provides several agent implementations for different use cases.

<Tip>
  Check out our new experimental [`RequirementAgent`](/experimental/requirement-agent) that combines the power of LLMs, tools, and requirements, all wrapped in a declarative interface.

  **This approach will soon become a default building block for building agents and will replace the others.**
</Tip>

### ReAct Agent

The ReActAgent implements the ReAct ([Reasoning and Acting](https://arxiv.org/abs/2210.03629)) pattern, which structures agent behavior into a cyclical process of reasoning, action, and observation.

This pattern allows agents to reason about a task, take actions using tools, observe results, and continue reasoning until reaching a conclusion.

Let's see how a ReActAgent approaches a simple question:

**Input prompt:** "What is the current weather in Las Vegas?"

**First iteration:**

```log
thought: I need to retrieve the current weather in Las Vegas. I can use the OpenMeteo function to get the current weather forecast for a location.
tool_name: OpenMeteo
tool_input: {"location": {"name": "Las Vegas"}, "start_date": "2024-10-17", "end_date": "2024-10-17", "temperature_unit": "celsius"}
```

**Second iteration:**

```log
thought: I have the current weather in Las Vegas in Celsius.
final_answer: The current weather in Las Vegas is 20.5¬∞C with an apparent temperature of 18.3¬∞C.
```

<Note>
  During execution, the agent emits partial updates as it generates each line, followed by complete updates. Updates follow a strict order: first all partial updates for "thought," then a complete "thought" update, then moving to the next component.
</Note>

<CodeGroup>
  {/* <!-- embedme python/examples/agents/react.py --> */}

  ```py Python [expandable]
  import asyncio
  import logging
  import os
  import sys
  import tempfile
  import traceback
  from typing import Any

  from dotenv import load_dotenv

  from beeai_framework.agents.react import ReActAgent
  from beeai_framework.backend import ChatModel, ChatModelParameters
  from beeai_framework.emitter import EmitterOptions, EventMeta
  from beeai_framework.errors import FrameworkError
  from beeai_framework.logger import Logger
  from beeai_framework.memory import TokenMemory
  from beeai_framework.tools import AnyTool
  from beeai_framework.tools.code import LocalPythonStorage, PythonTool
  from beeai_framework.tools.search.duckduckgo import DuckDuckGoSearchTool
  from beeai_framework.tools.search.wikipedia import WikipediaTool
  from beeai_framework.tools.weather import OpenMeteoTool
  from examples.helpers.io import ConsoleReader

  # Load environment variables
  load_dotenv()

  # Configure logging - using DEBUG instead of trace
  logger = Logger("app", level=logging.DEBUG)

  reader = ConsoleReader()


  def create_agent() -> ReActAgent:
      """Create and configure the agent with tools and LLM"""

      # Other models to try:
      # "llama3.1"
      # "granite3.3:8b"
      # "deepseek-r1"
      # ensure the model is pulled before running
      llm = ChatModel.from_name(
          "ollama:granite3.3:8b",
          ChatModelParameters(temperature=0),
      )

      # Configure tools
      tools: list[AnyTool] = [
          WikipediaTool(),
          OpenMeteoTool(),
          DuckDuckGoSearchTool(),
      ]

      # Add code interpreter tool if URL is configured
      code_interpreter_url = os.getenv("CODE_INTERPRETER_URL")
      if code_interpreter_url:
          tools.append(
              PythonTool(
                  code_interpreter_url,
                  LocalPythonStorage(
                      local_working_dir=tempfile.mkdtemp("code_interpreter_source"),
                      interpreter_working_dir=os.getenv("CODE_INTERPRETER_TMPDIR", "./tmp/code_interpreter_target"),
                  ),
              )
          )

      # Create agent with memory and tools
      agent = ReActAgent(llm=llm, tools=tools, memory=TokenMemory(llm))

      return agent


  def process_agent_events(data: Any, event: EventMeta) -> None:
      """Process agent events and log appropriately"""

      if event.name == "error":
          reader.write("Agent ü§ñ : ", FrameworkError.ensure(data.error).explain())
      elif event.name == "retry":
          reader.write("Agent ü§ñ : ", "retrying the action...")
      elif event.name == "update":
          reader.write(f"Agent({data.update.key}) ü§ñ : ", data.update.parsed_value)
      elif event.name == "start":
          reader.write("Agent ü§ñ : ", "starting new iteration")
      elif event.name == "success":
          reader.write("Agent ü§ñ : ", "success")


  async def main() -> None:
      """Main application loop"""

      # Create agent
      agent = create_agent()

      # Log code interpreter status if configured
      code_interpreter_url = os.getenv("CODE_INTERPRETER_URL")
      if code_interpreter_url:
          reader.write(
              "üõ†Ô∏è System: ",
              f"The code interpreter tool is enabled. Please ensure that it is running on {code_interpreter_url}",
          )

      reader.write("üõ†Ô∏è System: ", "Agent initialized with Wikipedia, DuckDuckGo, and Weather tools.")

      # Main interaction loop with user input
      for prompt in reader:
          # Run agent with the prompt
          response = await agent.run(
              prompt,
              max_retries_per_step=3,
              total_max_retries=10,
              max_iterations=20,
          ).on("*", process_agent_events, EmitterOptions(match_nested=False))

          reader.write("Agent ü§ñ : ", response.last_message.text)


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/agents/react.ts --> */}

  ```ts TypeScript [expandable]
  import "dotenv/config.js";
  import { ReActAgent } from "beeai-framework/agents/react/agent";
  import { createConsoleReader } from "../helpers/io.js";
  import { FrameworkError } from "beeai-framework/errors";
  import { TokenMemory } from "beeai-framework/memory/tokenMemory";
  import { Logger } from "beeai-framework/logger/logger";
  import { PythonTool } from "beeai-framework/tools/python/python";
  import { LocalPythonStorage } from "beeai-framework/tools/python/storage";
  import { DuckDuckGoSearchTool } from "beeai-framework/tools/search/duckDuckGoSearch";
  import { WikipediaTool } from "beeai-framework/tools/search/wikipedia";
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";
  import { dirname } from "node:path";
  import { fileURLToPath } from "node:url";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  Logger.root.level = "silent"; // disable internal logs
  const logger = new Logger({ name: "app", level: "trace" });

  // Other models to try:
  // "llama3.1:70b"
  // "granite3.3"
  // "deepseek-r1:32b"
  // ensure the model is pulled before running
  const llm = new OllamaChatModel("llama3.1");

  const codeInterpreterUrl = process.env.CODE_INTERPRETER_URL;
  const __dirname = dirname(fileURLToPath(import.meta.url));

  const codeInterpreterTmpdir =
    process.env.CODE_INTERPRETER_TMPDIR ?? "./examples/tmp/code_interpreter";
  const localTmpdir = process.env.LOCAL_TMPDIR ?? "./examples/tmp/local";

  const agent = new ReActAgent({
    llm,
    memory: new TokenMemory(),
    tools: [
      new DuckDuckGoSearchTool(),
      // new WebCrawlerTool(), // HTML web page crawler
      new WikipediaTool(),
      new OpenMeteoTool(), // weather tool
      // new ArXivTool(), // research papers
      // new DynamicTool() // custom python tool
      ...(codeInterpreterUrl
        ? [
            new PythonTool({
              codeInterpreter: { url: codeInterpreterUrl },
              storage: new LocalPythonStorage({
                interpreterWorkingDir: `${__dirname}/../../${codeInterpreterTmpdir}`,
                localWorkingDir: `${__dirname}/../../${localTmpdir}`,
              }),
            }),
          ]
        : []),
    ],
  });

  const reader = createConsoleReader();
  if (codeInterpreterUrl) {
    reader.write(
      "üõ†Ô∏è System",
      `The code interpreter tool is enabled. Please ensure that it is running on ${codeInterpreterUrl}`,
    );
  }

  try {
    for await (const { prompt } of reader) {
      const response = await agent
        .run(
          { prompt },
          {
            execution: {
              maxRetriesPerStep: 3,
              totalMaxRetries: 10,
              maxIterations: 20,
            },
          },
        )
        .observe((emitter) => {
          // emitter.on("start", () => {
          //   reader.write(`Agent ü§ñ : `, "starting new iteration");
          // });
          emitter.on("error", ({ error }) => {
            reader.write(`Agent ü§ñ : `, FrameworkError.ensure(error).dump());
          });
          emitter.on("retry", () => {
            reader.write(`Agent ü§ñ : `, "retrying the action...");
          });
          emitter.on("update", async ({ data, update, meta }) => {
            // log 'data' to see the whole state
            // to log only valid runs (no errors), check if meta.success === true
            reader.write(`Agent (${update.key}) ü§ñ : `, update.value);
          });
          emitter.on("partialUpdate", ({ data, update, meta }) => {
            // ideal for streaming (line by line)
            // log 'data' to see the whole state
            // to log only valid runs (no errors), check if meta.success === true
            // reader.write(`Agent (partial ${update.key}) ü§ñ : `, update.value);
          });

          // To observe all events (uncomment following block)
          // emitter.match("*.*", async (data: unknown, event) => {
          //   logger.trace(event, `Received event "${event.path}"`);
          // });

          // To get raw LLM input (uncomment following block)
          // emitter.match(
          //   (event) => event.creator === llm && event.name === "start",
          //   async (data: InferCallbackValue<GenerateEvents["start"]>, event) => {
          //     logger.trace(
          //       event,
          //       [
          //         `Received LLM event "${event.path}"`,
          //         JSON.stringify(data.input), // array of messages
          //       ].join("\n"),
          //     );
          //   },
          // );
        });

      reader.write(`Agent ü§ñ : `, response.result.text);
    }
  } catch (error) {
    logger.error(FrameworkError.ensure(error).dump());
  } finally {
    reader.close();
  }

  ```
</CodeGroup>

### Tool Calling Agent

The ToolCallingAgent is optimized for scenarios where tool usage is the primary focus. It handles tool calls more efficiently and can execute multiple tools in parallel.

<CodeGroup>
  {/* <!-- embedme python/examples/agents/tool_calling.py --> */}

  ```py Python [expandable]
  import asyncio
  import logging
  import sys
  import traceback
  from typing import Any

  from dotenv import load_dotenv

  from beeai_framework.agents.tool_calling import ToolCallingAgent
  from beeai_framework.backend import ChatModel
  from beeai_framework.emitter import EventMeta
  from beeai_framework.errors import FrameworkError
  from beeai_framework.logger import Logger
  from beeai_framework.memory import UnconstrainedMemory
  from beeai_framework.tools.weather import OpenMeteoTool
  from examples.helpers.io import ConsoleReader

  # Load environment variables
  load_dotenv()

  # Configure logging - using DEBUG instead of trace
  logger = Logger("app", level=logging.DEBUG)

  reader = ConsoleReader()


  def process_agent_events(data: Any, event: EventMeta) -> None:
      """Process agent events and log appropriately"""

      if event.name == "start":
          reader.write("Agent (debug) ü§ñ : ", "starting new iteration")
      elif event.name == "success":
          reader.write("Agent (debug) ü§ñ : ", data.state.memory.messages[-1])


  async def main() -> None:
      """Main application loop"""

      # Create agent
      agent = ToolCallingAgent(
          llm=ChatModel.from_name("ollama:llama3.1"), memory=UnconstrainedMemory(), tools=[OpenMeteoTool()]
      )

      # Main interaction loop with user input
      for prompt in reader:
          response = await agent.run(prompt).on("*", process_agent_events)
          reader.write("Agent ü§ñ : ", response.last_message.text)

      print("======DONE (showing the full message history)=======")

      messages = response.state.memory.messages
      for msg in messages:
          print(msg)


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/agents/toolCalling/agent.ts --> */}

  ```ts TypeScript [expandable]
  import "dotenv/config.js";
  import { createConsoleReader } from "../../helpers/io.js";
  import { FrameworkError } from "beeai-framework/errors";
  import { TokenMemory } from "beeai-framework/memory/tokenMemory";
  import { Logger } from "beeai-framework/logger/logger";
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";
  import { ToolCallingAgent } from "beeai-framework/agents/toolCalling/agent";

  Logger.root.level = "silent"; // disable internal logs
  const logger = new Logger({ name: "app", level: "trace" });

  // Other models to try:
  // "llama3.1:70b"
  // "granite3.3"
  // "deepseek-r1:32b"
  // ensure the model is pulled before running
  const llm = new OllamaChatModel("llama3.1");

  const agent = new ToolCallingAgent({
    llm,
    memory: new TokenMemory(),
    tools: [
      new OpenMeteoTool(), // weather tool
    ],
  });

  const reader = createConsoleReader();

  try {
    for await (const { prompt } of reader) {
      let messagesCount = agent.memory.messages.length + 1;

      const response = await agent.run({ prompt }).observe((emitter) => {
        emitter.on("success", async ({ state }) => {
          const newMessages = state.memory.messages.slice(messagesCount);
          messagesCount += newMessages.length;

          reader.write(
            `Agent (${newMessages.length} new messages) ü§ñ :\n`,
            newMessages.map((msg) => `-> ${JSON.stringify(msg.toPlain())}`).join("\n"),
          );
        });

        // To observe all events (uncomment following block)
        // emitter.match("*.*", async (data: unknown, event) => {
        //   logger.trace(event, `Received event "${event.path}"`);
        // }, {
        //   matchNested: true
        // });

        // To get raw LLM input (uncomment following block)
        // emitter.match(
        //   (event) => event.creator === llm && event.name === "start",
        //   async (data: InferCallbackValue<GenerateEvents["start"]>, event) => {
        //     logger.trace(
        //       event,
        //       [
        //         `Received LLM event "${event.path}"`,
        //         JSON.stringify(data.input), // array of messages
        //       ].join("\n"),
        //     );
        //   },
        // );
      });

      reader.write(`Agent ü§ñ : `, response.result.text);
    }
  } catch (error) {
    logger.error(FrameworkError.ensure(error).dump());
  } finally {
    reader.close();
  }

  ```
</CodeGroup>

### Custom Agent

For advanced use cases, you can create your own agent implementation by extending the `BaseAgent` class.

<CodeGroup>
  {/* <!-- embedme python/examples/agents/custom_agent.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback
  from typing import Unpack

  from pydantic import BaseModel, Field

  from beeai_framework.adapters.ollama import OllamaChatModel
  from beeai_framework.agents import AgentMeta, AgentOptions, AgentOutput, BaseAgent
  from beeai_framework.backend import AnyMessage, AssistantMessage, ChatModel, SystemMessage, UserMessage
  from beeai_framework.context import RunContext
  from beeai_framework.emitter import Emitter
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import BaseMemory, UnconstrainedMemory
  from beeai_framework.runnable import runnable_entry


  class State(BaseModel):
      thought: str
      final_answer: str


  class CustomAgent(BaseAgent):
      def __init__(self, llm: ChatModel, memory: BaseMemory) -> None:
          super().__init__()
          self.model = llm
          self._memory = memory

      @property
      def memory(self) -> BaseMemory:
          return self._memory

      @memory.setter
      def memory(self, memory: BaseMemory) -> None:
          self._memory = memory

      def _create_emitter(self) -> Emitter:
          return Emitter.root().child(
              namespace=["agent", "custom"],
              creator=self,
          )

      @runnable_entry
      async def run(self, input: str | list[AnyMessage], /, **kwargs: Unpack[AgentOptions]) -> AgentOutput:
          async def handler(context: RunContext) -> AgentOutput:
              class CustomSchema(BaseModel):
                  thought: str = Field(description="Describe your thought process before coming with a final answer")
                  final_answer: str = Field(
                      description="Here you should provide concise answer to the original question."
                  )

              response = await self.model.create_structure(
                  schema=CustomSchema,
                  messages=[
                      SystemMessage("You are a helpful assistant. Always use JSON format for your responses."),
                      *(self.memory.messages if self.memory is not None else []),
                      *([UserMessage(input)] if isinstance(input, str) else input),
                  ],
                  max_retries=kwargs.get("total_max_retries", 3),
                  abort_signal=context.signal,
              )

              result = AssistantMessage(response.object["final_answer"])
              await self.memory.add(result) if self.memory else None

              return AgentOutput(
                  output=[result],
                  context={
                      "state": State(thought=response.object["thought"], final_answer=response.object["final_answer"])
                  },
              )

          return await handler(RunContext.get())

      @property
      def meta(self) -> AgentMeta:
          return AgentMeta(
              name="CustomAgent",
              description="Custom Agent is a simple LLM agent.",
              tools=[],
          )


  async def main() -> None:
      agent = CustomAgent(
          llm=OllamaChatModel("granite3.3:8b"),
          memory=UnconstrainedMemory(),
      )

      response = await agent.run([UserMessage("Why is the sky blue?")])
      print(response.context.get("state"))


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/agents/custom_agent.ts --> */}

  ```ts TypeScript [expandable]
  import { BaseAgent, BaseAgentRunOptions } from "beeai-framework/agents/base";
  import {
    AssistantMessage,
    Message,
    SystemMessage,
    UserMessage,
  } from "beeai-framework/backend/message";
  import { Emitter } from "beeai-framework/emitter/emitter";
  import { GetRunContext } from "beeai-framework/context";
  import { z } from "zod";
  import { AgentMeta } from "beeai-framework/agents/types";
  import { BaseMemory } from "beeai-framework/memory/base";
  import { UnconstrainedMemory } from "beeai-framework/memory/unconstrainedMemory";
  import { ChatModel } from "beeai-framework/backend/chat";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  interface RunInput {
    message: Message;
  }

  interface RunOutput {
    message: Message;
    state: {
      thought: string;
      final_answer: string;
    };
  }

  interface RunOptions extends BaseAgentRunOptions {
    maxRetries?: number;
  }

  interface AgentInput {
    llm: ChatModel;
    memory: BaseMemory;
  }

  export class CustomAgent extends BaseAgent<RunInput, RunOutput, RunOptions> {
    public readonly memory: BaseMemory;
    protected readonly model: ChatModel;
    public emitter = Emitter.root.child({
      namespace: ["agent", "custom"],
      creator: this,
    });

    constructor(input: AgentInput) {
      super();
      this.model = input.llm;
      this.memory = input.memory;
    }

    protected async _run(
      input: RunInput,
      options: RunOptions,
      run: GetRunContext<this>,
    ): Promise<RunOutput> {
      const response = await this.model.createStructure({
        schema: z.object({
          thought: z
            .string()
            .describe("Describe your thought process before coming with a final answer"),
          final_answer: z
            .string()
            .describe("Here you should provide concise answer to the original question."),
        }),
        messages: [
          new SystemMessage("You are a helpful assistant. Always use JSON format for you responses."),
          ...this.memory.messages,
          input.message,
        ],
        maxRetries: options?.maxRetries,
        abortSignal: run.signal,
      });

      const result = new AssistantMessage(response.object.final_answer);
      await this.memory.add(result);

      return {
        message: result,
        state: response.object,
      };
    }

    public get meta(): AgentMeta {
      return {
        name: "CustomAgent",
        description: "Custom Agent is a simple LLM agent.",
        tools: [],
      };
    }

    createSnapshot() {
      return {
        ...super.createSnapshot(),
        emitter: this.emitter,
        memory: this.memory,
      };
    }

    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>) {
      Object.assign(this, snapshot);
    }
  }

  const agent = new CustomAgent({
    llm: new OllamaChatModel("granite3.3"),
    memory: new UnconstrainedMemory(),
  });

  const response = await agent.run({
    message: new UserMessage("Why is the sky blue?"),
  });
  console.info(response.state);

  ```
</CodeGroup>

## Customizing Agent Behavior

You can customize your agent's behavior in five ways:

### 1. Setting Execution Policy

Control how the agent runs by configuring retries, timeouts, and iteration limits.

<CodeGroup>
  ```py Python
  response = await agent.run(
       prompt=prompt,
       execution=AgentExecutionConfig(max_retries_per_step=3, total_max_retries=10, max_iterations=20),
  ).on("*", process_agent_events, EmitterOptions(match_nested=False))
  ```

  ```ts TypeScript
  for await (const { prompt } of reader) {
    const response = await agent
      .run(
        { prompt },
        {
          execution: {
            maxRetriesPerStep: 3,
            totalMaxRetries: 10,
            maxIterations: 20,
          },
        },
      )
  ```
</CodeGroup>

<Tip>
  The default is zero retries and no timeout. For complex tasks, increasing the max\_iterations is recommended.
</Tip>

### 2. Overriding Prompt Templates

Customize how the agent formats prompts, including the system prompt that defines its behavior.

The agent uses several templates that you can override:

1. **System Prompt** - Defines the agent's behavior and capabilities
2. **User Prompt** - Formats the user's input
3. **Tool Error** - Handles tool execution errors
4. **Tool Input Error** - Handles validation errors
5. **Tool No Result Error** - Handles empty results
6. **Tool Not Found Error** - Handles references to missing tools
7. **Invalid Schema Error** - Handles parsing errors

<CodeGroup>
  {/* <!-- embedme python/examples/templates/system_prompt.py --> */}

  ```py Python
  import sys
  import traceback

  from beeai_framework.agents.react.runners.default.prompts import (
      SystemPromptTemplate,
      ToolDefinition,
  )
  from beeai_framework.errors import FrameworkError
  from beeai_framework.tools.weather import OpenMeteoTool
  from beeai_framework.utils.strings import to_json


  def main() -> None:
      tool = OpenMeteoTool()

      tool_def = ToolDefinition(
          name=tool.name,
          description=tool.description,
          input_schema=to_json(tool.input_schema.model_json_schema()),
      )

      # Render the granite system prompt
      prompt = SystemPromptTemplate.render(
          instructions="You are a helpful AI assistant!", tools=[tool_def], tools_length=1
      )

      print(prompt)


  if __name__ == "__main__":
      try:
          main()
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- todo typescript/examples/templates/system_prompt.ts --> */}

  ```ts TypeScript
  Example coming soon
  ```
</CodeGroup>

### 3. Adding Tools

Enhance your agent's capabilities by providing it with tools to interact with external systems.

<CodeGroup>
  ```py Python
  agent = ReActAgent(
      llm=llm,
      tools=[DuckDuckGoSearchTool(), OpenMeteoTool()],
      memory=UnconstrainedMemory()
  )
  ```

  ```ts TypeScript
  const agent = new ReActAgent({
    llm,
    memory: new TokenMemory(),
    tools: [new DuckDuckGoSearchTool(), new OpenMeteoTool()],
  });
  ```
</CodeGroup>

**Available tools include:**

* Search tools (`DuckDuckGoSearchTool`)
* Weather tools (`OpenMeteoTool`)
* Knowledge tools (`LangChainWikipediaTool`)
* And many more in the `beeai_framework.tools` module

### 4. Configuring Memory

Memory allows your agent to maintain context across multiple interactions.

Several memory types are available for different use cases:

* UnconstrainedMemory - For unlimited storage
* SlidingMemory - For keeping only the most recent messages
* TokenMemory - For managing token limits
* SummarizeMemory - For summarizing previous conversations

<CodeGroup>
  ```py Python
  agent = ReActAgent(
      llm=llm,
      tools=[DuckDuckGoSearchTool(), OpenMeteoTool()],
      memory=UnconstrainedMemory()
  )
  ```

  ```ts TypeScript
  const agent = new ReActAgent({
    llm,
    memory: new TokenMemory(),
    tools: [new DuckDuckGoSearchTool(), new OpenMeteoTool()],
  });
  ```
</CodeGroup>

<CodeGroup>
  {/* <!-- embedme python/examples/memory/agent_memory.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.agents.react import ReActAgent
  from beeai_framework.backend import AssistantMessage, ChatModel, UserMessage
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import UnconstrainedMemory

  # Initialize the memory and LLM
  memory = UnconstrainedMemory()


  def create_agent() -> ReActAgent:
      llm = ChatModel.from_name("ollama:granite3.3:8b")

      # Initialize the agent
      agent = ReActAgent(llm=llm, memory=memory, tools=[])

      return agent


  async def main() -> None:
      # Create user message
      user_input = "Hello world!"
      user_message = UserMessage(user_input)

      # Await adding user message to memory
      await memory.add(user_message)
      print("Added user message to memory")

      # Create agent
      agent = create_agent()

      response = await agent.run(
          user_input,
          max_retries_per_step=3,
          total_max_retries=10,
          max_iterations=20,
      )
      print(f"Received response: {response}")

      # Create and store assistant's response
      assistant_message = AssistantMessage(response.last_message.text)

      # Await adding assistant message to memory
      await memory.add(assistant_message)
      print("Added assistant message to memory")

      # Print results
      print(f"\nMessages in memory: {len(agent.memory.messages)}")

      if len(agent.memory.messages) >= 1:
          user_msg = agent.memory.messages[0]
          print(f"User: {user_msg.text}")

      if len(agent.memory.messages) >= 2:
          agent_msg = agent.memory.messages[1]
          print(f"Agent: {agent_msg.text}")
      else:
          print("No agent message found in memory")


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/memory/agentMemory.ts --> */}

  ```ts TypeScript [expandable]
  import { UnconstrainedMemory } from "beeai-framework/memory/unconstrainedMemory";
  import { ReActAgent } from "beeai-framework/agents/react/agent";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  const agent = new ReActAgent({
    memory: new UnconstrainedMemory(),
    llm: new OllamaChatModel("llama3.1"),
    tools: [],
  });
  await agent.run({ prompt: "Hello world!" });

  console.info(agent.memory.messages.length); // 2

  const userMessage = agent.memory.messages[0];
  console.info(`User: ${userMessage.text}`); // User: Hello world!

  const agentMessage = agent.memory.messages[1];
  console.info(`Agent: ${agentMessage.text}`); // Agent: Hello! It's nice to chat with you.

  ```
</CodeGroup>

### 5. Event Observation

Monitor the agent's execution by observing events it emits. This allows you to track its reasoning process, handle errors, or implement custom logging.

<CodeGroup>
  ```py Python
  def update_callback(data: Any, event: EventMeta) -> None:
      print(f"Agent({data.update.key}) ü§ñ : ", data.update.parsed_value)

  def on_update(emitter: Emitter) -> None:
      emitter.on("update", update_callback)

  output: BeeRunOutput = await agent.run("What's the current weather in Las Vegas?").observe(on_update)
  ```

  ```ts TypeScript
  const response = await agent
    .run({ prompt: "What's the current weather in Las Vegas?" })
    .observe((emitter) => {
      emitter.on("update", async ({ data, update, meta }) => {
        console.log(`Agent (${update.key}) ü§ñ : `, update.value);
      });
    });
  ```
</CodeGroup>

## Agent Workflows

For complex applications, you can create multi-agent workflows where specialized agents collaborate.

<CodeGroup>
  {/* <!-- embedme python/examples/workflows/multi_agents.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.backend import ChatModel
  from beeai_framework.emitter import EmitterOptions
  from beeai_framework.errors import FrameworkError
  from beeai_framework.tools.search.wikipedia import WikipediaTool
  from beeai_framework.tools.weather import OpenMeteoTool
  from beeai_framework.workflows.agent import AgentWorkflow, AgentWorkflowInput
  from examples.helpers.io import ConsoleReader


  async def main() -> None:
      llm = ChatModel.from_name("ollama:llama3.1")
      workflow = AgentWorkflow(name="Smart assistant")

      workflow.add_agent(
          name="Researcher",
          role="A diligent researcher.",
          instructions="You look up and provide information about a specific topic.",
          tools=[WikipediaTool()],
          llm=llm,
      )

      workflow.add_agent(
          name="WeatherForecaster",
          role="A weather reporter.",
          instructions="You provide detailed weather reports.",
          tools=[OpenMeteoTool()],
          llm=llm,
      )

      workflow.add_agent(
          name="DataSynthesizer",
          role="A meticulous and creative data synthesizer",
          instructions="You can combine disparate information into a final coherent summary.",
          llm=llm,
      )

      reader = ConsoleReader()

      reader.write("Assistant ü§ñ : ", "What location do you want to learn about?")
      for prompt in reader:
          await (
              workflow.run(
                  inputs=[
                      AgentWorkflowInput(prompt="Provide a short history of the location.", context=prompt),
                      AgentWorkflowInput(
                          prompt="Provide a comprehensive weather summary for the location today.",
                          expected_output="Essential weather details such as chance of rain, temperature and wind. Only report information that is available.",
                      ),
                      AgentWorkflowInput(
                          prompt="Summarize the historical and weather data for the location.",
                          expected_output="A paragraph that describes the history of the location, followed by the current weather conditions.",
                      ),
                  ]
              )
              .on(
                  # Event Matcher -> match agent's 'success' events
                  lambda event: isinstance(event.creator, ChatModel) and event.name == "success",
                  # log data to the console
                  lambda data, event: reader.write(
                      "->Got response from the LLM",
                      "  \n->".join([str(message.content[0].model_dump()) for message in data.value.messages]),
                  ),
                  EmitterOptions(match_nested=True),
              )
              .on(
                  "success",
                  lambda data, event: reader.write(
                      f"->Step '{data.step}' has been completed with the following outcome."
                      f"\n\n{data.state.final_answer}\n\n",
                      data.model_dump(exclude={"data"}),
                  ),
              )
          )
          reader.write("Assistant ü§ñ : ", "What location do you want to learn about?")


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/workflows/multiAgents.ts --> */}

  ```ts TypeScript [expandable]
  import "dotenv/config";
  import { createConsoleReader } from "examples/helpers/io.js";
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";
  import { WikipediaTool } from "beeai-framework/tools/search/wikipedia";
  import { AgentWorkflow } from "beeai-framework/workflows/agent";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  const workflow = new AgentWorkflow("Smart assistant");
  const llm = new OllamaChatModel("llama3.1");

  workflow.addAgent({
    name: "Researcher",
    role: "A diligent researcher",
    instructions: "You look up and provide information about a specific topic.",
    tools: [new WikipediaTool()],
    llm,
  });
  workflow.addAgent({
    name: "WeatherForecaster",
    role: "A weather reporter",
    instructions: "You provide detailed weather reports.",
    tools: [new OpenMeteoTool()],
    llm,
  });
  workflow.addAgent({
    name: "DataSynthesizer",
    role: "A meticulous and creative data synthesizer",
    instructions: "You can combine disparate information into a final coherent summary.",
    llm,
  });

  const reader = createConsoleReader();
  reader.write("Assistant ü§ñ : ", "What location do you want to learn about?");
  for await (const { prompt } of reader) {
    const { result } = await workflow
      .run([
        { prompt: "Provide a short history of the location.", context: prompt },
        {
          prompt: "Provide a comprehensive weather summary for the location today.",
          expectedOutput:
            "Essential weather details such as chance of rain, temperature and wind. Only report information that is available.",
        },
        {
          prompt: "Summarize the historical and weather data for the location.",
          expectedOutput:
            "A paragraph that describes the history of the location, followed by the current weather conditions.",
        },
      ])
      .observe((emitter) => {
        emitter.on("success", (data) => {
          reader.write(
            `Step '${data.step}' has been completed with the following outcome:\n`,
            data.state?.finalAnswer ?? "-",
          );
        });
      });

    reader.write(`Assistant ü§ñ`, result.finalAnswer);
    reader.write("Assistant ü§ñ : ", "What location do you want to learn about?");
  }

  ```
</CodeGroup>

## Examples

<CardGroup cols={2}>
  <Card title="Python" icon="python" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/agents">
    Explore reference agent implementations in Python
  </Card>

  <Card title="TypeScript" icon="js" href="https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples/agents">
    Explore reference agent implementations in TypeScript
  </Card>
</CardGroup>


# Backend
Source: https://framework.beeai.dev/modules/backend



## Overview

Backend is an umbrella module that encapsulates a unified way to work with the following functionalities:

* Chat Models via (`ChatModel` class)
* Embedding Models (coming soon)
* Audio Models (coming soon)
* Image Models (coming soon)

BeeAI framework's backend is designed with a provider-based architecture, allowing you to switch between different AI service providers while maintaining a consistent API.

<Note>
  Supported in Python and TypeScript.
</Note>

***

## Supported providers

The following table depicts supported providers. Each provider requires specific configuration through environment variables. Ensure all required variables are set before initializing a provider.

| Name           | Chat | Embedding      | Environment Variables                                                                                                                                                                                 |
| :------------- | :--- | :------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Ollama         | ‚úÖ    | ‚úÖ              | OLLAMA\_CHAT\_MODEL<br />OLLAMA\_BASE\_URL                                                                                                                                                            |
| OpenAI         | ‚úÖ    | ‚úÖ              | OPENAI\_CHAT\_MODEL<br />OPENAI\_EMBEDDING\_MODEL<br />OPENAI\_API\_BASE<br />OPENAI\_API\_KEY<br />OPENAI\_ORGANIZATION<br />OPENAI\_API\_HEADERS                                                    |
| Watsonx        | ‚úÖ    | ‚úÖ              | WATSONX\_CHAT\_MODEL<br />WATSONX\_API\_KEY<br />WATSONX\_PROJECT\_ID<br />WATSONX\_SPACE\_ID<br />WATSONX\_TOKEN<br />WATSONX\_ZENAPIKEY<br />WATSONX\_URL<br />WATSONX\_REGION                      |
| Groq           | ‚úÖ    | ‚úÖ              | GROQ\_CHAT\_MODEL<br />GROQ\_EMBEDDING\_MODEL<br />GROQ\_API\_KEY                                                                                                                                     |
| Amazon Bedrock | ‚úÖ    | ‚úÖ  *(TS only)* | AWS\_CHAT\_MODEL<br />AWS\_ACCESS\_KEY\_ID<br />AWS\_SECRET\_ACCESS\_KEY<br />AWS\_REGION<br />AWS\_API\_HEADERS                                                                                      |
| Google Vertex  | ‚úÖ    | ‚úÖ  *(TS only)* | GOOGLE\_VERTEX\_CHAT\_MODEL<br />GOOGLE\_VERTEX\_PROJECT<br />GOOGLE\_APPLICATION\_CREDENTIALS<br />GOOGLE\_APPLICATION\_CREDENTIALS\_JSON<br />GOOGLE\_CREDENTIALS<br />GOOGLE\_VERTEX\_API\_HEADERS |
| Google Gemini  | ‚úÖ    | ‚úÖ  *(Py only)* | GEMINI\_CHAT\_MODEL<br />GEMINI\_API\_KEY<br />GEMINI\_API\_HEADERS                                                                                                                                   |
| Azure OpenAI   | ‚úÖ    | ‚úÖ  *(TS only)* | AZURE\_OPENAI\_CHAT\_MODEL<br />AZURE\_OPENAI\_API\_KEY<br />AZURE\_OPENAI\_API\_BASE<br />AZURE\_OPENAI\_API\_VERSION<br />AZURE\_AD\_TOKEN<br />AZURE\_API\_TYPE<br />AZURE\_API\_HEADERS           |
| Anthropic      | ‚úÖ    | ‚úÖ  *(TS only)* | ANTHROPIC\_CHAT\_MODEL<br />ANTHROPIC\_API\_KEY<br />ANTHROPIC\_API\_HEADERS                                                                                                                          |
| xAI            | ‚úÖ    | ‚úÖ  *(TS only)* | XAI\_CHAT\_MODEL<br />XAI\_API\_KEY                                                                                                                                                                   |
| MistralAI      | ‚úÖ    | ‚úÖ  *(Py only)* | MISTRALAI\_CHAT\_MODEL<br />MISTRALAI\_EMBEDDING\_MODEL<br />MISTRALAI\_API\_KEY<br />MISTRALAI\_API\_BASE                                                                                            |

<Tip>
  If you don't see your provider raise an issue [here](https://github.com/i-am-bee/beeai-framework/issues).
  Meanwhile, you can use the Ollama adapter in [Python](https://github.com/i-am-bee/beeai-framework/blob/main/python/examples/backend/providers/ollama.py).
  or [TypeScript](https://github.com/i-am-bee/beeai-framework/blob/main/typescript/examples/backend/providers/ollama.ts).
</Tip>

***

### Backend initialization

The `Backend` class serves as a central entry point to access models from your chosen provider.

<CodeGroup>
  {/* <!-- embedme python/examples/backend/providers/watsonx.py --> */}

  ```py Python [expandable]
  import asyncio
  import datetime
  import json
  import sys
  import traceback

  from dotenv import load_dotenv
  from pydantic import BaseModel, Field

  from beeai_framework.adapters.watsonx import WatsonxChatModel
  from beeai_framework.adapters.watsonx.backend.embedding import WatsonxEmbeddingModel
  from beeai_framework.backend import ChatModel, MessageToolResultContent, ToolMessage, UserMessage
  from beeai_framework.errors import AbortError, FrameworkError
  from beeai_framework.tools.weather import OpenMeteoTool
  from beeai_framework.utils import AbortSignal

  # Load environment variables
  load_dotenv()

  # Setting can be passed here during initiation or pre-configured via environment variables
  llm = WatsonxChatModel(
      "ibm/granite-3-8b-instruct",
      # settings={
      #     "project_id": "WATSONX_PROJECT_ID",
      #     "api_key": "WATSONX_API_KEY",
      #     "base_url": "WATSONX_API_URL",
      # },
  )


  async def watsonx_from_name() -> None:
      watsonx_llm = ChatModel.from_name(
          "watsonx:ibm/granite-3-8b-instruct",
          # {
          #     "project_id": "WATSONX_PROJECT_ID",
          #     "api_key": "WATSONX_API_KEY",
          #     "base_url": "WATSONX_API_URL",
          # },
      )
      user_message = UserMessage("what states are part of New England?")
      response = await watsonx_llm.create(messages=[user_message])
      print(response.get_text_content())


  async def watsonx_sync() -> None:
      user_message = UserMessage("what is the capital of Massachusetts?")
      response = await llm.create(messages=[user_message])
      print(response.get_text_content())


  async def watsonx_stream() -> None:
      user_message = UserMessage("How many islands make up the country of Cape Verde?")
      response = await llm.create(messages=[user_message], stream=True)
      print(response.get_text_content())


  async def watsonx_images() -> None:
      image_llm = ChatModel.from_name(
          "watsonx:meta-llama/llama-3-2-11b-vision-instruct",
      )
      response = await image_llm.create(
          messages=[
              UserMessage("What is the dominant color in the picture?"),
              UserMessage.from_image(
                  "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAIAAACQkWg2AAAAHUlEQVR4nGI5Y6bFQApgIkn1qIZRDUNKAyAAAP//0ncBT3KcmKoAAAAASUVORK5CYII="
              ),
          ],
      )
      print(response.get_text_content())


  async def watsonx_stream_abort() -> None:
      user_message = UserMessage("What is the smallest of the Cape Verde islands?")

      try:
          response = await llm.create(messages=[user_message], stream=True, abort_signal=AbortSignal.timeout(0.5))

          if response is not None:
              print(response.get_text_content())
          else:
              print("No response returned.")
      except AbortError as err:
          print(f"Aborted: {err}")


  async def watson_structure() -> None:
      class TestSchema(BaseModel):
          answer: str = Field(description="your final answer")

      user_message = UserMessage("How many islands make up the country of Cape Verde?")
      response = await llm.create_structure(schema=TestSchema, messages=[user_message])
      print(response.object)


  async def watson_tool_calling() -> None:
      watsonx_llm = ChatModel.from_name("watsonx:ibm/granite-3-3-8b-instruct")
      user_message = UserMessage(f"What is the current weather in Boston? Current date is {datetime.datetime.today()}.")
      weather_tool = OpenMeteoTool()
      response = await watsonx_llm.create(messages=[user_message], tools=[weather_tool], stream=True)
      tool_call_msg = response.get_tool_calls()[0]
      print(tool_call_msg.model_dump())
      tool_response = await weather_tool.run(json.loads(tool_call_msg.args))
      tool_response_msg = ToolMessage(
          MessageToolResultContent(
              result=tool_response.get_text_content(), tool_name=tool_call_msg.tool_name, tool_call_id=tool_call_msg.id
          )
      )
      print(tool_response_msg.to_plain())
      final_response = await watsonx_llm.create(messages=[user_message, *response.messages, tool_response_msg], tools=[])
      print(final_response.get_text_content())


  async def watsonx_debug() -> None:
      # Log every request
      llm.emitter.on(
          "*",
          lambda data, event: print(
              f"Time: {event.created_at.time().isoformat()}",
              f"Event: {event.name}",
              f"Data: {str(data)[:90]}...",
          ),
      )

      response = await llm.create(
          messages=[UserMessage("Hello world!")],
      )
      print(response.messages[0].to_plain())


  async def watsonx_embedding() -> None:
      embedding_llm = WatsonxEmbeddingModel()

      response = await embedding_llm.create(["Text", "to", "embed"])

      for row in response.embeddings:
          print(*row)


  async def main() -> None:
      print("*" * 10, "watsonx_from_name")
      await watsonx_from_name()
      print("*" * 10, "watsonx_images")
      await watsonx_images()
      print("*" * 10, "watsonx_sync")
      await watsonx_sync()
      print("*" * 10, "watsonx_stream")
      await watsonx_stream()
      print("*" * 10, "watsonx_stream_abort")
      await watsonx_stream_abort()
      print("*" * 10, "watson_structure")
      await watson_structure()
      print("*" * 10, "watson_tool_calling")
      await watson_tool_calling()
      print("*" * 10, "watsonx_debug")
      await watsonx_debug()
      print("*" * 10, "watsonx_embedding")
      await watsonx_embedding()


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/backend/providers/watsonx.ts --> */}

  ```ts TypeScript [expandable]
  import "dotenv/config.js";
  import { CustomMessage, ToolMessage, UserMessage } from "beeai-framework/backend/message";
  import { WatsonxChatModel } from "beeai-framework/adapters/watsonx/backend/chat";
  import { ChatModel } from "beeai-framework/backend/chat";
  import { AbortError } from "beeai-framework/errors";
  import { z } from "zod";
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";

  const llm = new WatsonxChatModel(
    "ibm/granite-3-3-8b-instruct",
    // {
    //   apiKey: "WATSONX_API_KEY",
    //   baseUrl: "WATSONX_BASE_URL",
    //   projectId: "WATSONX_PROJECT_ID",
    // }
  );

  llm.config({
    parameters: {
      temperature: 0,
      topP: 1,
    },
  });

  async function watsonxFromName() {
    const watsonxLLM = await ChatModel.fromName("watsonx:ibm/granite-3-3-8b-instruct");
    const response = await watsonxLLM.create({
      messages: [new UserMessage("what states are part of New England?")],
    });
    console.info(response.getTextContent());
  }

  async function watsonxCustomMessage() {
    const watsonxLLM = await ChatModel.fromName("watsonx:ibm/granite-3-3-8b-instruct");
    const response = await watsonxLLM.create({
      messages: [
        new UserMessage(
          "A farmer has 10 cows, 5 chickens, and 2 horses. If we count all the animals' legs together, how many legs are there in total?",
        ),
        new CustomMessage("control", "thinking"),
      ],
    });
    console.info(response.getTextContent());
  }

  async function watsonxSync() {
    const response = await llm.create({
      messages: [new UserMessage("what is the capital of Massachusetts?")],
    });
    console.info(response.getTextContent());
  }

  async function watsonxStream() {
    const response = await llm.create({
      messages: [new UserMessage("How many islands make up the country of Cape Verde?")],
      stream: true,
    });
    console.info(response.getTextContent());
  }

  async function watsonxAbort() {
    try {
      const response = await llm.create({
        messages: [new UserMessage("What is the smallest of the Cape Verde islands?")],
        stream: true,
        abortSignal: AbortSignal.timeout(5 * 1000),
      });
      console.info(response.getTextContent());
    } catch (err) {
      if (err instanceof AbortError) {
        console.log("Aborted", { err });
      }
    }
  }

  async function watsonxStructure() {
    const response = await llm.createStructure({
      schema: z.object({
        answer: z.string({ description: "your final answer" }),
      }),
      messages: [new UserMessage("How many islands make up the country of Cape Verde?")],
    });
    console.info(response.object);
  }

  async function watsonxToolCalling() {
    const currentDate = new Date().toISOString();
    const userMessage = new UserMessage(`What is the current weather (${currentDate}) in Boston?`);
    const weatherTool = new OpenMeteoTool({ retryOptions: { maxRetries: 3 } });
    const response = await llm.create({
      messages: [userMessage],
      tools: [weatherTool],
      toolChoice: weatherTool,
    });
    const toolCallMsg = response.getToolCalls()[0];
    console.debug(JSON.stringify(toolCallMsg));
    const toolResponse = await weatherTool.run(toolCallMsg.args as any);
    const toolResponseMsg = new ToolMessage({
      type: "tool-result",
      result: toolResponse.getTextContent(),
      toolName: toolCallMsg.toolName,
      toolCallId: toolCallMsg.toolCallId,
    });
    console.info(toolResponseMsg.toPlain());
    const finalResponse = await llm.create({
      messages: [userMessage, ...response.messages, toolResponseMsg],
      tools: [],
    });
    console.info(finalResponse.getTextContent());
  }

  async function watsonxDebug() {
    // Log every request
    llm.emitter.match("*", (value, event) =>
      console.debug(
        `Time: ${event.createdAt.toISOString()}`,
        `Event: ${event.name}`,
        `Data: ${value}`,
      ),
    );

    const response = await llm.create({
      messages: [new UserMessage("Hello world!")],
    });
    console.info(response.messages[0].toPlain());
  }

  console.info("watsonxFromName".padStart(25, "*"));
  await watsonxFromName();
  console.info("watsonxCustomMessage".padStart(25, "*"));
  await watsonxCustomMessage();
  console.info("watsonxSync".padStart(25, "*"));
  await watsonxSync();
  console.info("watsonxStream".padStart(25, "*"));
  await watsonxStream();
  console.info("watsonxAbort".padStart(25, "*"));
  await watsonxAbort();
  console.info("watsonxStructure".padStart(25, "*"));
  await watsonxStructure();
  console.info("watsonxToolCalling".padStart(25, "*"));
  await watsonxToolCalling();
  console.info("watsonxDebug".padStart(25, "*"));
  await watsonxDebug();

  ```
</CodeGroup>

<Note>
  Explore the providers examples in [Python](https://github.com/i-am-bee/beeai-framework/tree/main/python/beeai_framework/backend) or
  [TypeScript](https://github.com/i-am-bee/beeai-framework/tree/main/typescript/src/backend).
</Note>

<Tip>
  See the [events documentation](/modules/events) for more information on standard emitter events.
</Tip>

***

## Chat model

The `ChatModel` class represents a Chat Large Language Model and provides methods for text generation, streaming responses, and more. You can initialize a chat model in multiple ways:

**Method 1: Using the generic factory method**

<CodeGroup>
  ```py Python
  from beeai_framework.backend.chat import ChatModel

  model = ChatModel.from_name("ollama:llama3.1")
  ```

  ```ts TypeScript
  import { ChatModel } from "beeai-framework/backend/chat";

  const model = await ChatModel.fromName("ollama:granite3.3:8b");
  ```
</CodeGroup>

**Method 2: Creating a specific provider model directly**

<CodeGroup>
  ```py Python
  from beeai_framework.adapters.ollama.backend.chat import OllamaChatModel

  model = OllamaChatModel("llama3.1")
  ```

  ```ts Typescript
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  const model = new OllamaChatModel("llama3.1");
  ```
</CodeGroup>

### Chat model configuration

You can configure various parameters for your chat model:

<CodeGroup>
  {/* <!-- embedme python/examples/backend/chat.py --> */}

  ```python Python
  import asyncio
  import sys
  import traceback

  from beeai_framework.adapters.ollama import OllamaChatModel
  from beeai_framework.backend import UserMessage
  from beeai_framework.errors import FrameworkError
  from examples.helpers.io import ConsoleReader


  async def main() -> None:
      llm = OllamaChatModel("llama3.1")

      #  Optionally one may set llm parameters
      llm.parameters.max_tokens = 10000  # high number yields longer potential output
      llm.parameters.top_p = 0.1  # higher number yields more complex vocabulary, recommend only changing p or k
      llm.parameters.frequency_penalty = 0  # higher number yields reduction in word reptition
      llm.parameters.temperature = 0  # higher number yields greater randomness and variation
      llm.parameters.top_k = 0  # higher number yields more variance, recommend only changing p or k
      llm.parameters.n = 1  # higher number yields more choices
      llm.parameters.presence_penalty = 0  # higher number yields reduction in repetition of words
      llm.parameters.seed = 10  # can help produce similar responses if prompt and seed are always the same
      llm.parameters.stop_sequences = ["q", "quit", "ahhhhhhhhh"]  # stops the model on input of any of these strings
      llm.parameters.stream = False  # determines whether or not to use streaming to receive incremental data

      reader = ConsoleReader()

      for prompt in reader:
          response = await llm.create(messages=[UserMessage(prompt)])
          reader.write("LLM ü§ñ (txt) : ", response.get_text_content())
          reader.write("LLM ü§ñ (raw) : ", "\n".join([str(msg.to_plain()) for msg in response.messages]))


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/backend/chat.ts --> */}

  ```ts TypeScript [expandable]
  import "dotenv/config.js";
  import { createConsoleReader } from "examples/helpers/io.js";
  import { UserMessage } from "beeai-framework/backend/message";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  const llm = new OllamaChatModel("llama3.1");

  //  Optionally one may set llm parameters
  llm.parameters.maxTokens = 10000; // high number yields longer potential output
  llm.parameters.topP = 0; // higher number yields more complex vocabulary, recommend only changing p or k
  llm.parameters.frequencyPenalty = 0; // higher number yields reduction in word reptition
  llm.parameters.temperature = 0; // higher number yields greater randomness and variation
  llm.parameters.topK = 0; // higher number yields more variance, recommend only changing p or k
  llm.parameters.n = 1; // higher number yields more choices
  llm.parameters.presencePenalty = 0; // higher number yields reduction in repetition of words
  llm.parameters.seed = 10; // can help produce similar responses if prompt and seed are always the same
  llm.parameters.stopSequences = ["q", "quit", "ahhhhhhhhh"]; // stops the model on input of any of these strings

  // alternatively
  llm.config({
    parameters: {
      maxTokens: 10000,
      // other parameters
    },
  });

  const reader = createConsoleReader();

  for await (const { prompt } of reader) {
    const response = await llm.create({
      messages: [new UserMessage(prompt)],
    });
    reader.write(`LLM ü§ñ (txt) : `, response.getTextContent());
    reader.write(`LLM ü§ñ (raw) : `, JSON.stringify(response.messages));
  }

  ```
</CodeGroup>

### Text generation

The most basic usage is to generate text responses:

<CodeGroup>
  ```py Python
  from beeai_framework.adapters.ollama.backend.chat import OllamaChatModel
  from beeai_framework.backend.message import UserMessage

  model = OllamaChatModel("llama3.1")
  response = await model.create(
      messages=[UserMessage("what states are part of New England?")]
  )

  print(response.get_text_content())
  ```

  ```ts TypeScript
  import { UserMessage } from "beeai-framework/backend/message";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  const llm = new OllamaChatModel("llama3.1");

  const response = await llm.create({
    messages: [new UserMessage("what states are part of New England?")],
  });

  console.log(response.getTextContent());
  ```
</CodeGroup>

<Note>
  Execution parameters (those passed to `model.create({...})`) are superior to ones defined via `config`.
</Note>

### Streaming responses

For applications requiring real-time responses:

<CodeGroup>
  ```py Python
  from beeai_framework.adapters.ollama.backend.chat import OllamaChatModel
  from beeai_framework.backend.message import UserMessage

  llm = OllamaChatModel("llama3.1")
  user_message = UserMessage("How many islands make up the country of Cape Verde?")
  response = await llm.create(messages=[user_message], stream=True)
    .on(
      "new_token",
      lambda data, event: print(data.value.get_text_content()))
    )
  )
  print("Full response", response.get_text_content())
  ```

  {/* <!-- embedme typescript/examples/backend/chatStream.ts --> */}

  ```ts TypeScript [expandable]
  import "dotenv/config.js";
  import { createConsoleReader } from "examples/helpers/io.js";
  import { UserMessage } from "beeai-framework/backend/message";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  const llm = new OllamaChatModel("llama3.1");

  const reader = createConsoleReader();

  for await (const { prompt } of reader) {
    const response = await llm
      .create({
        messages: [new UserMessage(prompt)],
      })
      .observe((emitter) =>
        emitter.match("*", (data, event) => {
          reader.write(`LLM ü§ñ (event: ${event.name})`, JSON.stringify(data));

          // if you want to close the stream prematurely, just uncomment the following line
          // callbacks.abort()
        }),
      );

    reader.write(`LLM ü§ñ (txt) : `, response.getTextContent());
    reader.write(`LLM ü§ñ (raw) : `, JSON.stringify(response.messages));
  }

  ```
</CodeGroup>

### Structured generation

Generate structured data according to a schema:

<CodeGroup>
  {/* <!-- embedme python/examples/backend/structured.py --> */}

  ```py Python [expandable]
  import asyncio
  import json
  import sys
  import traceback

  from pydantic import BaseModel, Field

  from beeai_framework.backend import ChatModel, UserMessage
  from beeai_framework.errors import FrameworkError


  async def main() -> None:
      model = ChatModel.from_name("ollama:llama3.1")

      class ProfileSchema(BaseModel):
          first_name: str = Field(..., min_length=1)
          last_name: str = Field(..., min_length=1)
          address: str
          age: int = Field(..., min_length=1)
          hobby: str

      class ErrorSchema(BaseModel):
          error: str

      class SchemUnion(ProfileSchema, ErrorSchema):
          pass

      response = await model.create_structure(
          schema=SchemUnion,
          messages=[UserMessage("Generate a profile of a citizen of Europe.")],
      )

      print(
          json.dumps(
              response.object.model_dump() if isinstance(response.object, BaseModel) else response.object, indent=4
          )
      )


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/backend/structured.ts --> */}

  ```ts TypeScript [expandable]
  import { ChatModel, UserMessage } from "beeai-framework/backend/core";
  import { z } from "zod";

  const model = await ChatModel.fromName("ollama:llama3.1");
  const response = await model.createStructure({
    schema: z.union([
      z.object({
        firstName: z.string().min(1),
        lastName: z.string().min(1),
        address: z.string(),
        age: z.number().int().min(1),
        hobby: z.string(),
      }),
      z.object({
        error: z.string(),
      }),
    ]),
    messages: [new UserMessage("Generate a profile of a citizen of Europe.")],
  });
  console.log(response.object);

  ```
</CodeGroup>

### Tool calling

Integrate external tools with your AI model:

<CodeGroup>
  {/* <!-- embedme python/examples/backend/tool_calling.py --> */}

  ```py Python [expandable]
  import asyncio
  import json
  import re
  import sys
  import traceback

  from beeai_framework.backend import (
      AnyMessage,
      ChatModel,
      ChatModelParameters,
      MessageToolResultContent,
      SystemMessage,
      ToolMessage,
      UserMessage,
  )
  from beeai_framework.errors import FrameworkError
  from beeai_framework.tools import AnyTool, ToolOutput
  from beeai_framework.tools.search.duckduckgo import DuckDuckGoSearchTool
  from beeai_framework.tools.weather.openmeteo import OpenMeteoTool


  async def main() -> None:
      model = ChatModel.from_name("ollama:llama3.1", ChatModelParameters(temperature=0))
      tools: list[AnyTool] = [DuckDuckGoSearchTool(), OpenMeteoTool()]
      messages: list[AnyMessage] = [
          SystemMessage("You are a helpful assistant. Use tools to provide a correct answer."),
          UserMessage("What's the fastest marathon time?"),
      ]

      while True:
          response = await model.create(
              messages=messages,
              tools=tools,
          )

          tool_calls = response.get_tool_calls()
          messages.extend(response.messages)

          tool_results: list[ToolMessage] = []

          for tool_call in tool_calls:
              print(f"-> running '{tool_call.tool_name}' tool with {tool_call.args}")
              tool: AnyTool = next(tool for tool in tools if tool.name == tool_call.tool_name)
              assert tool is not None
              res: ToolOutput = await tool.run(json.loads(tool_call.args))
              result = res.get_text_content()
              print(f"<- got response from '{tool_call.tool_name}'", re.sub(r"\s+", " ", result)[:256] + " (truncated)")
              tool_results.append(
                  ToolMessage(
                      MessageToolResultContent(
                          result=result,
                          tool_name=tool_call.tool_name,
                          tool_call_id=tool_call.id,
                      )
                  )
              )

          messages.extend(tool_results)

          answer = response.get_text_content()

          if answer:
              print(f"Agent: {answer}")
              break


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/backend/toolCalling.ts --> */}

  ```ts TypeScript [expandable]
  import "dotenv/config";
  import {
    ChatModel,
    Message,
    SystemMessage,
    ToolMessage,
    UserMessage,
  } from "beeai-framework/backend/core";
  import { DuckDuckGoSearchTool } from "beeai-framework/tools/search/duckDuckGoSearch";
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";
  import { AnyTool, ToolOutput } from "beeai-framework/tools/base";

  const model = await ChatModel.fromName("ollama:llama3.1");
  const tools: AnyTool[] = [new DuckDuckGoSearchTool(), new OpenMeteoTool()];
  const messages: Message[] = [
    new SystemMessage("You are a helpful assistant. Use tools to provide a correct answer."),
    new UserMessage("What's the fastest marathon time?"),
  ];

  while (true) {
    const response = await model.create({
      messages,
      tools,
    });
    messages.push(...response.messages);

    const toolCalls = response.getToolCalls();
    const toolResults = await Promise.all(
      toolCalls.map(async ({ args, toolName, toolCallId }) => {
        console.log(`-> running '${toolName}' tool with ${JSON.stringify(args)}`);
        const tool = tools.find((tool) => tool.name === toolName)!;
        const response: ToolOutput = await tool.run(args as any);
        const result = response.getTextContent();
        console.log(
          `<- got response from '${toolName}'`,
          result.replaceAll(/\s+/g, " ").substring(0, 90).concat(" (truncated)"),
        );
        return new ToolMessage({
          type: "tool-result",
          result,
          isError: false,
          toolName,
          toolCallId,
        });
      }),
    );
    messages.push(...toolResults);

    const answer = response.getTextContent();
    if (answer) {
      console.info(`Agent: ${answer}`);
      break;
    }
  }

  ```
</CodeGroup>

***

## Embedding model

The `EmbedingModel` class provides functionality for generating vector embeddings from text.

### Embedding model initialization

You can initialize an embedding model in multiple ways:

**Method 1: Using the generic factory method**

<CodeGroup>
  ```py Python
  from beeai_framework.backend.embedding import EmbeddingModel

  model = EmbeddingModel.from_name("ollama:nomic-embed-text")
  ```

  ```ts TypeScript
  import { EmbeddingModel } from "beeai-framework/backend/embedding";

  const model = await EmbeddingModel.fromName("ollama:nomic-embed-text");
  ```
</CodeGroup>

**Method 2: Creating a specific provider model directly**

<CodeGroup>
  ```py Python
  from beeai_framework.adapters.ollama.backend import OllamaEmbeddingModel

  model = OllamaEmbeddingModel("nomic-embed-text")
  ```

  ```ts TypeScript
  import { OpenAIEmbeddingModel } from "beeai-framework/adapters/openai/embedding";

  const model = new OpenAIEmbeddingModel(
    "text-embedding-3-large",
    {
      dimensions: 512,
      maxEmbeddingsPerCall: 5,
    },
    {
      baseURL: "your_custom_endpoint",
      compatibility: "compatible",
      headers: {
        CUSTOM_HEADER: "...",
      },
    },
  );
  ```
</CodeGroup>

### Embedding model usage

Generate embeddings for one or more text strings:

<CodeGroup>
  ```py Python
  from beeai_framework.backend.embedding import EmbeddingModel

  model = EmbeddingModel.from_name("ollama:nomic-embed-text")

  response = await model.create(["Hello world!", "Hello Bee!"])
  console.log(response.values)
  console.log(response.embeddings)
  ```

  ```ts TypeScript
  import { EmbeddingModel } from "beeai-framework/backend/embedding";

  const model = await EmbeddingModel.fromName("ollama:nomic-embed-text");

  const response = await model.create({
  	values: ["Hello world!", "Hello Bee!"],
  });
  console.log(response.values);
  console.log(response.embeddings);
  ```
</CodeGroup>

***

## Others

If your preferred provider isn't directly supported, you can use the LangChain adapter as a bridge.

This allows you to leverage any provider that has LangChain compatibility.

<CodeGroup>
  ```py Python [expandable]
  Example coming soon
  ```

  {/* <!-- embedme typescript/examples/backend/providers/langchain.ts --> */}

  ```ts TypeScript [expandable]
  // NOTE: ensure you have installed following packages
  // - @langchain/core
  // - @langchain/cohere (or any other provider related package that you would like to use)
  // List of available providers: https://js.langchain.com/v0.2/docs/integrations/chat/

  import { LangChainChatModel } from "beeai-framework/adapters/langchain/backend/chat";
  // @ts-expect-error package not installed
  import { ChatCohere } from "@langchain/cohere";
  import "dotenv/config.js";
  import { ToolMessage, UserMessage } from "beeai-framework/backend/message";
  import { z } from "zod";
  import { ChatModelError } from "beeai-framework/backend/errors";
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";

  const llm = new LangChainChatModel(
    new ChatCohere({
      model: "command-r-plus",
      temperature: 0,
    }),
  );

  async function langchainSync() {
    const response = await llm.create({
      messages: [new UserMessage("what is the capital of Massachusetts?")],
    });
    console.info(response.getTextContent());
  }

  async function langchainStream() {
    const response = await llm.create({
      messages: [new UserMessage("How many islands make up the country of Cape Verde?")],
      stream: true,
    });
    console.info(response.getTextContent());
  }

  async function langchainAbort() {
    try {
      const response = await llm.create({
        messages: [new UserMessage("What is the smallest of the Cape Verde islands?")],
        stream: true,
        abortSignal: AbortSignal.timeout(1 * 1000),
      });
      console.info(response.getTextContent());
    } catch (err) {
      if (err instanceof ChatModelError) {
        console.log("Aborted", { err });
      }
    }
  }

  async function langchainStructure() {
    const response = await llm.createStructure({
      schema: z.object({
        answer: z.string({ description: "your final answer" }),
      }),
      messages: [new UserMessage("How many islands make up the country of Cape Verde?")],
    });
    console.info(response.object);
  }

  async function langchainToolCalling() {
    const userMessage = new UserMessage(
      `What is the current weather in Boston? Current date is ${new Date().toISOString().split("T")[0]}.`,
    );
    const weatherTool = new OpenMeteoTool({ retryOptions: { maxRetries: 3 } });
    const response = await llm.create({ messages: [userMessage], tools: [weatherTool] });
    const toolCallMsg = response.getToolCalls()[0];
    console.debug(JSON.stringify(toolCallMsg));
    const toolResponse = await weatherTool.run(toolCallMsg.args as any);
    const toolResponseMsg = new ToolMessage({
      type: "tool-result",
      result: toolResponse.getTextContent(),
      toolName: toolCallMsg.toolName,
      toolCallId: toolCallMsg.toolCallId,
    });
    console.info(toolResponseMsg.toPlain());
    const finalResponse = await llm.create({
      messages: [userMessage, ...response.messages, toolResponseMsg],
      tools: [],
    });
    console.info(finalResponse.getTextContent());
  }

  async function langchainDebug() {
    // Log every request
    llm.emitter.match("*", (value, event) =>
      console.debug(
        `Time: ${event.createdAt.toISOString()}`,
        `Event: ${event.name}`,
        `Data: ${JSON.stringify(value)}`,
      ),
    );

    const response = await llm.create({
      messages: [new UserMessage("Hello world!")],
    });
    console.info(response.messages[0].toPlain());
  }

  console.info(" langchainSync".padStart(25, "*"));
  await langchainSync();
  console.info(" langchainStream".padStart(25, "*"));
  await langchainStream();
  console.info(" langchainAbort".padStart(25, "*"));
  await langchainAbort();
  console.info(" langchainStructure".padStart(25, "*"));
  await langchainStructure();
  console.info(" langchainToolCalling".padStart(25, "*"));
  await langchainToolCalling();
  console.info(" langchainDebug".padStart(25, "*"));
  await langchainDebug();

  ```
</CodeGroup>

***

## Troubleshooting

Common issues and their solutions:

1. Authentication errors: Ensure all required environment variables are set correctly
2. Model not found: Verify that the model ID is correct and available for the selected provider

***

## Examples

<CardGroup cols={2}>
  <Card title="Python" icon="python" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/backend">
    Explore reference backend implementations in Python
  </Card>

  <Card title="TypeScript" icon="js" href="https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples/backend">
    Explore reference backend implementations in TypeScript
  </Card>
</CardGroup>


# Cache
Source: https://framework.beeai.dev/modules/cache



## Overview

Caching is a technique used to temporarily store copies of data or computation results to improve performance by reducing the need to repeatedly fetch or compute the same data from slower or more resource-intensive sources.

In the context of AI applications, caching provides several important benefits:

* üöÄ **Performance improvement** - Avoid repeating expensive operations like API calls or complex calculations
* üí∞ **Cost reduction** - Minimize repeated calls to paid services (like external APIs or LLM providers)
* ‚ö° **Latency reduction** - Deliver faster responses to users by serving cached results
* üîÑ **Consistency** - Ensure consistent responses for identical inputs

BeeAI framework provides a robust caching system with multiple implementations to suit different use cases.

***

## Core concepts

### Cache types

BeeAI framework offers several cache implementations out of the box:

| Type                   | Description                                                          |
| :--------------------- | :------------------------------------------------------------------- |
| **UnconstrainedCache** | Simple in-memory cache with no limits                                |
| **SlidingCache**       | In-memory cache that maintains a maximum number of entries           |
| **FileCache**          | Persistent cache that stores data on disk                            |
| **NullCache**          | Special implementation that performs no caching (useful for testing) |

Each cache type implements the `BaseCache` interface, making them interchangeable in your code.

### Usage patterns

BeeAI framework supports several caching patterns:

| Usage pattern           | Description                          |
| :---------------------- | :----------------------------------- |
| **Direct caching**      | Manually store and retrieve values   |
| **Function decoration** | Automatically cache function returns |
| **Tool integration**    | Cache tool execution results         |
| **LLM integration**     | Cache model responses                |

***

## Basic usage

### Caching function output

The simplest way to use caching is to wrap a function that produces deterministic output:

<CodeGroup>
  {/* <!-- embedme python/examples/cache/unconstrained_cache_function.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.cache import UnconstrainedCache
  from beeai_framework.errors import FrameworkError


  async def main() -> None:
      cache: UnconstrainedCache[int] = UnconstrainedCache()

      async def fibonacci(n: int) -> int:
          cache_key = str(n)
          cached = await cache.get(cache_key)
          if cached:
              return int(cached)

          if n < 1:
              result = 0
          elif n <= 2:
              result = 1
          else:
              result = await fibonacci(n - 1) + await fibonacci(n - 2)

          await cache.set(cache_key, result)
          return result

      print(await fibonacci(10))  # 55
      print(await fibonacci(9))  # 34 (retrieved from cache)
      print(f"Cache size {await cache.size()}")  # 10


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/cache/unconstrainedCacheFunction.ts --> */}

  ```ts TypeScript [expandable]
  import { UnconstrainedCache } from "beeai-framework/cache/unconstrainedCache";

  const cache = new UnconstrainedCache<number>();

  async function fibonacci(n: number): Promise<number> {
    const cacheKey = n.toString();
    const cached = await cache.get(cacheKey);
    if (cached !== undefined) {
      return cached;
    }

    const result = n < 1 ? 0 : n <= 2 ? 1 : (await fibonacci(n - 1)) + (await fibonacci(n - 2));
    await cache.set(cacheKey, result);
    return result;
  }

  console.info(await fibonacci(10)); // 55
  console.info(await fibonacci(9)); // 34 (retrieved from cache)
  console.info(`Cache size ${await cache.size()}`); // 10

  ```
</CodeGroup>

### Using with tools

BeeAI framework's caching system seamlessly integrates with tools:

<CodeGroup>
  {/* <!-- embedme python/examples/cache/tool_cache.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.cache import SlidingCache
  from beeai_framework.errors import FrameworkError
  from beeai_framework.tools.search.wikipedia import (
      WikipediaTool,
      WikipediaToolInput,
  )


  async def main() -> None:
      wikipedia_client = WikipediaTool({"full_text": True, "cache": SlidingCache(size=100, ttl=5 * 60)})

      print(await wikipedia_client.cache.size())  # 0
      tool_input = WikipediaToolInput(query="United States")
      first = await wikipedia_client.run(tool_input)
      print(await wikipedia_client.cache.size())  # 1

      # new request with the EXACTLY same input will be retrieved from the cache
      tool_input = WikipediaToolInput(query="United States")
      second = await wikipedia_client.run(tool_input)
      print(first.get_text_content() == second.get_text_content())  # True
      print(await wikipedia_client.cache.size())  # 1


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/cache/toolCache.ts --> */}

  ```ts TypeScript [expandable]
  import { SlidingCache } from "beeai-framework/cache/slidingCache";
  import { WikipediaTool } from "beeai-framework/tools/search/wikipedia";

  const ddg = new WikipediaTool({
    cache: new SlidingCache({
      size: 100, // max 100 entries
      ttl: 5 * 60 * 1000, // 5 minutes lifespan
    }),
  });

  const response = await ddg.run({
    query: "United States",
  });
  // upcoming requests with the EXACTLY same input will be retrieved from the cache

  ```
</CodeGroup>

### Using with LLMs

You can also cache LLM responses to save on API costs:

<CodeGroup>
  {/* <!-- embedme python/examples/cache/llm_cache.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.adapters.ollama import OllamaChatModel
  from beeai_framework.backend import ChatModelParameters, UserMessage
  from beeai_framework.cache import SlidingCache
  from beeai_framework.errors import FrameworkError


  async def main() -> None:
      llm = OllamaChatModel("llama3.1")
      llm.config(parameters=ChatModelParameters(max_tokens=25), cache=SlidingCache(size=50))

      print(await llm.cache.size())  # 0
      first = await llm.create(messages=[UserMessage("Who is Amilcar Cabral?")])
      print(await llm.cache.size())  # 1

      # new request with the EXACTLY same input will be retrieved from the cache
      second = await llm.create(messages=[UserMessage("Who is Amilcar Cabral?")])
      print(first.get_text_content() == second.get_text_content())  # True
      print(await llm.cache.size())  # 1


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/cache/llmCache.ts --> */}

  ```ts TypeScript [expandable]
  import { SlidingCache } from "beeai-framework/cache/slidingCache";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";
  import { UserMessage } from "beeai-framework/backend/message";

  const llm = new OllamaChatModel("llama3.1");
  llm.config({
    cache: new SlidingCache({
      size: 50,
    }),
    parameters: {
      maxTokens: 25,
    },
  });

  console.info(await llm.cache.size()); // 0
  const first = await llm.create({
    messages: [new UserMessage("Who was Alan Turing?")],
  });
  // upcoming requests with the EXACTLY same input will be retrieved from the cache
  console.info(await llm.cache.size()); // 1
  const second = await llm.create({
    messages: [new UserMessage("Who was Alan Turing?")],
  });
  console.info(first.getTextContent() === second.getTextContent()); // true
  console.info(await llm.cache.size()); // 1

  ```
</CodeGroup>

***

## Cache types

### UnconstrainedCache

The simplest cache type with no constraints on size or entry lifetime. Good for development and smaller applications.

<CodeGroup>
  {/* <!-- embedme python/examples/cache/unconstrained_cache.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.cache import UnconstrainedCache
  from beeai_framework.errors import FrameworkError


  async def main() -> None:
      cache: UnconstrainedCache[int] = UnconstrainedCache()

      # Save
      await cache.set("a", 1)
      await cache.set("b", 2)

      # Read
      result = await cache.has("a")
      print(result)  # True

      # Meta
      print(cache.enabled)  # True
      print(await cache.has("a"))  # True
      print(await cache.has("b"))  # True
      print(await cache.has("c"))  # False
      print(await cache.size())  # 2

      # Delete
      await cache.delete("a")
      print(await cache.has("a"))  # False

      # Clear
      await cache.clear()
      print(await cache.size())  # 0


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/cache/unconstrainedCache.ts --> */}

  ```ts TypeScript [expandable]
  import { UnconstrainedCache } from "beeai-framework/cache/unconstrainedCache";

  const cache = new UnconstrainedCache();

  // Save
  await cache.set("a", 1);
  await cache.set("b", 2);

  // Read
  const result = await cache.get("a");
  console.log(result); // 1

  // Meta
  console.log(cache.enabled); // true
  console.log(await cache.has("a")); // true
  console.log(await cache.has("b")); // true
  console.log(await cache.has("c")); // false
  console.log(await cache.size()); // 2

  // Delete
  await cache.delete("a");
  console.log(await cache.has("a")); // false

  // Clear
  await cache.clear();
  console.log(await cache.size()); // 0

  ```
</CodeGroup>

### SlidingCache

Maintains a maximum number of entries, removing the oldest entries when the limit is reached.

<CodeGroup>
  {/* <!-- embedme python/examples/cache/sliding_cache.py --> */}

  ```python
  import asyncio
  import sys
  import traceback

  from beeai_framework.cache import SlidingCache
  from beeai_framework.errors import FrameworkError


  async def main() -> None:
      cache: SlidingCache[int] = SlidingCache(
          size=3,  # (required) number of items that can be live in the cache at a single moment
          ttl=1,  # // (optional, default is Infinity) Time in seconds after the element is removed from a cache
      )

      await cache.set("a", 1)
      await cache.set("b", 2)
      await cache.set("c", 3)

      await cache.set("d", 4)  # overflow - cache internally removes the oldest entry (key "a")

      print(await cache.has("a"))  # False
      print(await cache.size())  # 3


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/cache/slidingCache.ts --> */}

  ```ts TypeScript [expandable]
  import { SlidingCache } from "beeai-framework/cache/slidingCache";

  const cache = new SlidingCache<number>({
    size: 3, // (required) number of items that can be live in the cache at a single moment
    ttl: 1000, // (optional, default is Infinity) Time in milliseconds after the element is removed from a cache
  });

  await cache.set("a", 1);
  await cache.set("b", 2);
  await cache.set("c", 3);

  await cache.set("d", 4); // overflow - cache internally removes the oldest entry (key "a")
  console.log(await cache.has("a")); // false
  console.log(await cache.size()); // 3

  ```
</CodeGroup>

### FileCache

Persists cache data to disk, allowing data to survive if application restarts.

<CodeGroup>
  {/* <!-- comingsoon python/examples/cache/file_cache.py --> */}

  ```py Python [expandable]
  Example coming soon
  ```

  {/* <!-- embedme typescript/examples/cache/fileCache.ts --> */}

  ```ts TypeScript [expandable]
  import { FileCache } from "beeai-framework/cache/fileCache";
  import * as os from "node:os";

  const cache = new FileCache({
    fullPath: `${os.tmpdir()}/bee_file_cache_${Date.now()}.json`,
  });
  console.log(`Saving cache to "${cache.source}"`);
  await cache.set("abc", { firstName: "John", lastName: "Doe" });

  ```
</CodeGroup>

#### With custom provider

You can customize how the FileCache stores data:

```text
Coming soon
```

<CodeGroup>
  {/* <!-- comingsoon python/examples/cache/file_cache_custom_provider.py --> */}

  ```py Python [expandable]
  Example coming soon
  ```

  {/* <!-- embedme typescript/examples/cache/fileCacheCustomProvider.ts --> */}

  ```ts TypeScript [expandable]
  import { FileCache } from "beeai-framework/cache/fileCache";
  import { UnconstrainedCache } from "beeai-framework/cache/unconstrainedCache";
  import os from "node:os";

  const memoryCache = new UnconstrainedCache<number>();
  await memoryCache.set("a", 1);

  const fileCache = await FileCache.fromProvider(memoryCache, {
    fullPath: `${os.tmpdir()}/bee_file_cache.json`,
  });
  console.log(`Saving cache to "${fileCache.source}"`);
  console.log(await fileCache.get("a")); // 1

  ```
</CodeGroup>

### NullCache

A special cache that implements the `BaseCache` interface but performs no caching. Useful for testing or temporarily disabling caching.

The reason for implementing is to enable [Null object pattern](https://en.wikipedia.org/wiki/Null_object_pattern).

***

## Advanced usage

### Cache decorator

The framework provides a convenient decorator for automatically caching function results:

<CodeGroup>
  {/* <!-- comingsoon python/examples/cache/decorator_cache.py --> */}

  ```py Python [expandable]
  Example coming soon
  ```

  {/* <!-- embedme typescript/examples/cache/decoratorCache.ts --> */}

  ```ts TypeScript [expandable]
  import { Cache } from "beeai-framework/cache/decoratorCache";

  class Generator {
    @Cache()
    get(seed: number) {
      return (Math.random() * 1000) / Math.max(seed, 1);
    }
  }

  const generator = new Generator();
  const a = generator.get(5);
  const b = generator.get(5);
  console.info(a === b); // true
  console.info(a === generator.get(6)); // false

  ```
</CodeGroup>

For more complex caching logic, you can customize the key generation:

<CodeGroup>
  {/* <!-- comingsoon python/examples/cache/decorator_cache_complex.py --> */}

  ```py Python [expandable]
  Example coming soon
  ```

  {/* <!-- embedme typescript/examples/cache/decoratorCacheComplex.ts --> */}

  ```ts TypeScript [expandable]
  import { Cache, SingletonCacheKeyFn } from "beeai-framework/cache/decoratorCache";

  class MyService {
    @Cache({
      cacheKey: SingletonCacheKeyFn,
      ttl: 3600,
      enumerable: true,
      enabled: true,
    })
    get id() {
      return Math.floor(Math.random() * 1000);
    }

    reset() {
      Cache.getInstance(this, "id").clear();
    }
  }

  const service = new MyService();
  const a = service.id;
  console.info(a === service.id); // true
  service.reset();
  console.info(a === service.id); // false

  ```
</CodeGroup>

### CacheFn helper

For more dynamic caching needs, the `CacheFn` helper provides a functional approach:

<CodeGroup>
  {/* <!-- comingsoon python/examples/cache/cache_fn.py --> */}

  ```py Python [expandable]
  Example coming soon
  ```

  {/* <!-- embedme typescript/examples/cache/cacheFn.ts --> */}

  ```ts TypeScript [expandable]
  import { CacheFn } from "beeai-framework/cache/decoratorCache";
  import { setTimeout } from "node:timers/promises";

  const getSecret = CacheFn.create(
    async () => {
      // instead of mocking response you would do a real fetch request
      const response = await Promise.resolve({ secret: Math.random(), expiresIn: 100 });
      getSecret.updateTTL(response.expiresIn);
      return response.secret;
    },
    {}, // options object
  );

  const token = await getSecret();
  console.info(token === (await getSecret())); // true
  await setTimeout(150);
  console.info(token === (await getSecret())); // false

  ```
</CodeGroup>

***

## Creating a custom cache provider

You can create your own cache implementation by extending the `BaseCache` class:

<CodeGroup>
  {/* <!-- embedme python/examples/cache/custom.py --> */}

  ```py Python [expandable]
  from typing import TypeVar

  from beeai_framework.cache import BaseCache

  T = TypeVar("T")


  class CustomCache(BaseCache[T]):
      async def size(self) -> int:
          raise NotImplementedError("CustomCache 'size' not yet implemented")

      async def set(self, _key: str, _value: T) -> None:
          raise NotImplementedError("CustomCache 'set' not yet implemented")

      async def get(self, key: str) -> T | None:
          raise NotImplementedError("CustomCache 'get' not yet implemented")

      async def has(self, key: str) -> bool:
          raise NotImplementedError("CustomCache 'has' not yet implemented")

      async def delete(self, key: str) -> bool:
          raise NotImplementedError("CustomCache 'delete' not yet implemented")

      async def clear(self) -> None:
          raise NotImplementedError("CustomCache 'clear' not yet implemented")

  ```

  {/* <!-- embedme typescript/examples/cache/custom.ts --> */}

  ```ts TypeScript [expandable]
  import { BaseCache } from "beeai-framework/cache/base";
  import { NotImplementedError } from "beeai-framework/errors";

  export class CustomCache<T> extends BaseCache<T> {
    size(): Promise<number> {
      throw new NotImplementedError();
    }

    set(key: string, value: T): Promise<void> {
      throw new NotImplementedError();
    }

    get(key: string): Promise<T | undefined> {
      throw new NotImplementedError();
    }

    has(key: string): Promise<boolean> {
      throw new NotImplementedError();
    }

    delete(key: string): Promise<boolean> {
      throw new NotImplementedError();
    }

    clear(): Promise<void> {
      throw new NotImplementedError();
    }

    createSnapshot() {
      throw new NotImplementedError();
    }

    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void {
      throw new NotImplementedError();
    }
  }

  ```
</CodeGroup>

***

## Examples

<CardGroup cols={2}>
  <Card title="Python" icon="python" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/cache">
    Explore reference cache implementations in Python
  </Card>

  <Card title="TypeScript" icon="js" href="https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples/cache">
    Explore reference cache implementations in TypeScript
  </Card>
</CardGroup>


# Emitter
Source: https://framework.beeai.dev/modules/emitter



## Overview

The `Emitter` is a powerful event management and observability tool that allows you to track, monitor, and react to events happening within your AI agents and workflows.

This flexible event-driven mechanism providers the ability to:

* Observe system events
* Debug agent behaviors
* Log and track agent interactions
* Implement custom event handling

<Note>
  Supported in Python and TypeScript.
</Note>

## Basic usage

<CodeGroup>
  {/* <!-- embedme python/examples/emitter/base.py --> */}

  ```py Python [expandable]
  import asyncio
  import json
  import sys
  import traceback
  from typing import Any

  from beeai_framework.emitter import Emitter, EventMeta
  from beeai_framework.errors import FrameworkError


  async def main() -> None:
      # Get the root emitter or create your own
      root = Emitter.root()

      # Listen to all events that will get emitted
      @root.on("*.*")
      async def handle_new_event(data: Any, event: EventMeta) -> None:
          print(f"Received event '{event.name}' ({event.path}) with data {json.dumps(data)}")

      await root.emit("start", {"id": 123})
      await root.emit("end", {"id": 123})

      root.off(callback=handle_new_event)  # deregister a listener


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/emitter/base.ts --> */}

  ```ts TypeScript [expandable]
  import { Emitter, EventMeta } from "beeai-framework/emitter/emitter";

  // Get the root emitter or create your own
  const root = Emitter.root;

  root.match("*.*", async (data: unknown, event: EventMeta) => {
    console.log(`Received event '${event.path}' with data ${JSON.stringify(data)}`);
  });

  await root.emit("start", { id: 123 });
  await root.emit("end", { id: 123 });

  ```
</CodeGroup>

<Note>
  You can create your own emitter by initiating the `Emitter` class, but typically it's better to use or fork the root one.
</Note>

## Key features

### Event matching

Event matching allows you to:

* Listen to specific event types
* Use wildcard matching
* Handle nested events

<CodeGroup>
  {/* <!-- embedme python/examples/emitter/matchers.py --> */}

  ```py Python [expandable]
  import asyncio
  import re
  import sys
  import traceback

  from beeai_framework.adapters.ollama import OllamaChatModel
  from beeai_framework.backend import ChatModel
  from beeai_framework.emitter import Emitter
  from beeai_framework.errors import FrameworkError


  async def main() -> None:
      emitter = Emitter.root().child(namespace=["app"])
      model = OllamaChatModel()

      # Match events by a concrete name (strictly typed)
      emitter.on("update", lambda data, event: print(data, ": on update"))

      # Match all events emitted directly on the instance (not nested)
      emitter.on("*", lambda data, event: print(data, ": match all instance"))

      # Match all events (included nested)
      cleanup = Emitter.root().on("*.*", lambda data, event: print(data, ": match all nested"))

      # Match events by providing a filter function
      model.emitter.on(
          lambda event: isinstance(event.creator, ChatModel), lambda data, event: print(data, ": match ChatModel")
      )

      # Match events by regex
      emitter.on(re.compile(r"watsonx"), lambda data, event: print(data, ": match regex"))

      await emitter.emit("update", "update")
      await Emitter.root().emit("root", "root")
      await model.emitter.emit("model", "model")

      cleanup()  # You can remove a listener from an emitter by calling the cleanup function it returns


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/emitter/matchers.ts --> */}

  ```ts TypeScript [expandable]
  import { Callback, Emitter } from "beeai-framework/emitter/emitter";
  import { ChatModel } from "beeai-framework/backend/chat";

  interface Events {
    update: Callback<{ data: string }>;
  }

  const emitter = new Emitter<Events>({
    namespace: ["app"],
  });

  // Match events by a concrete name (strictly typed)
  emitter.on("update", async (data, event) => {});

  // Match all events emitted directly on the instance (not nested)
  emitter.match("*", async (data, event) => {});

  // Match all events (included nested)
  emitter.match("*.*", async (data, event) => {});

  // Match events by providing a filter function
  emitter.match(
    (event) => event.creator instanceof ChatModel,
    async (data, event) => {},
  );

  // Match events by regex
  emitter.match(/watsonx/, async (data, event) => {});

  ```
</CodeGroup>

### Event piping

Event piping enables:

* Transferring events between emitters
* Transforming events in transit
* Creating complex event workflows

<CodeGroup>
  {/* <!-- embedme python/examples/emitter/piping.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.emitter import Emitter
  from beeai_framework.errors import FrameworkError


  async def main() -> None:
      first: Emitter = Emitter(namespace=["app"])

      first.on(
          "*.*",
          lambda data, event: print(
              f"'first' has retrieved the following event '{event.path}', isDirect: {event.source == first}"
          ),
      )

      second: Emitter = Emitter(namespace=["app", "llm"])

      second.on(
          "*.*",
          lambda data, event: print(
              f"'second' has retrieved the following event '{event.path}', isDirect: {event.source == second}"
          ),
      )

      # Propagate all events from the 'second' emitter to the 'first' emitter
      unpipe = second.pipe(first)

      await first.emit("a", {})
      await second.emit("b", {})

      print("Unpipe")
      unpipe()

      await first.emit("c", {})
      await second.emit("d", {})


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/emitter/piping.ts --> */}

  ```ts TypeScript [expandable]
  import { Emitter, EventMeta } from "beeai-framework/emitter/emitter";

  const first = new Emitter({
    namespace: ["app"],
  });

  first.match("*.*", (data: unknown, event: EventMeta) => {
    console.log(
      `'first' has retrieved the following event ${event.path}, isDirect: ${event.source === first}`,
    );
  });

  const second = new Emitter({
    namespace: ["app", "llm"],
  });
  second.match("*.*", (data: unknown, event: EventMeta) => {
    console.log(
      `'second' has retrieved the following event '${event.path}', isDirect: ${event.source === second}`,
    );
  });

  // Propagate all events from the 'second' emitter to the 'first' emitter
  const unpipe = second.pipe(first);

  await first.emit("a", {});
  await second.emit("b", {});

  console.log("Unpipe");
  unpipe();

  await first.emit("c", {});
  await second.emit("d", {});

  ```
</CodeGroup>

***

## Framework usage

In the following section we will take a look how to consume events from core modules in the the framework.

<Tip>
  The fastest way to see what is going on under the hood is by doing `instance.run(...).middleware(GlobalTrajectoryMiddleware)`.
</Tip>

### Agent usage

Integrate emitters with agents to:

* Track agent decision-making
* Log agent interactions
* Debug agent behaviors

<CodeGroup>
  {/* <!-- embedme python/examples/emitter/agent_matchers.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.adapters.ollama import OllamaChatModel
  from beeai_framework.agents.react import ReActAgent
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import UnconstrainedMemory


  async def main() -> None:
      agent = ReActAgent(
          llm=OllamaChatModel("llama3.1"),
          memory=UnconstrainedMemory(),
          tools=[],
      )

      # Matching events on the instance level
      agent.emitter.on("*.*", lambda data, event: None)

      # Matching events on the execution (run) level
      await agent.run("Hello agent!").observe(
          lambda emitter: emitter.on("*.*", lambda data, event: print(f"RUN LOG: received event '{event.path}'"))
      )


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/emitter/agentMatchers.ts --> */}

  ```ts TypeScript [expandable]
  import { ReActAgent } from "beeai-framework/agents/react/agent";
  import { UnconstrainedMemory } from "beeai-framework/memory/unconstrainedMemory";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  const agent = new ReActAgent({
    llm: new OllamaChatModel("llama3.1"),
    memory: new UnconstrainedMemory(),
    tools: [],
  });

  // Matching events on the instance level
  agent.emitter.match("*.*", (data, event) => {});

  await agent
    .run({
      prompt: "Hello agent!",
    })
    .observe((emitter) => {
      // Matching events on the execution (run) level
      emitter.match("*.*", (data, event) => {
        console.info(`RUN LOG: received event '${event.path}'`);
      });
    });

  ```
</CodeGroup>

<Note>
  The observe method is also supported on [Tools](modules/tools.mdx) and [Backend](modules/backend.mdx).
</Note>

<Tip>
  See the [events documentation](modules/events.mdx) for more information on standard emitter events.
</Tip>

***

### Advanced usage

Advanced techniques include:

* Custom event handlers
* Complex event filtering
* Performance optimization

<CodeGroup>
  {/* <!-- embedme python/examples/emitter/advanced.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback
  from typing import Any

  from beeai_framework.emitter import Emitter, EventMeta
  from beeai_framework.errors import FrameworkError


  async def main() -> None:
      # Create an emitter
      emitter = Emitter.root().child(
          namespace=["bee", "demo"],
          creator={},  # typically a class
          context={},  # custom data (propagates to the event's context property)
          group_id=None,  # optional id for grouping common events (propagates to the event's groupId property)
      )

      @emitter.on()
      async def on_start(data: dict[str, Any], event: EventMeta) -> None:
          print(f"Received '{event.name}' event with id '{data['id']}'")

      # Listen for "update" event
      cleanup = emitter.on(
          "update", lambda data, event: print(f"Received '{event.name}' with id '{data['id']}' and data '{data['data']}'")
      )
      cleanup()  # deregister a listener

      # Listen for "success" event
      @emitter.on("success")
      async def custom_name(data: dict[str, Any], event: EventMeta) -> None:
          print(f"Received '{event.name}' event with the following data", data)

      await emitter.emit("start", {"id": 123})
      await emitter.emit("update", {"id": 123, "data": "Hello Bee!"})
      await emitter.emit("success", {"id": 123, "result": "Hello world!"})

      emitter.off("success", custom_name)  # deregister a listener


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/emitter/advanced.ts --> */}

  ```ts TypeScript [expandable]
  import { Emitter, EventMeta, Callback } from "beeai-framework/emitter/emitter";

  // Define events in advanced
  interface Events {
    start: Callback<{ id: number }>;
    update: Callback<{ id: number; data: string }>;
  }

  // Create emitter with a type support
  const emitter = Emitter.root.child<Events>({
    namespace: ["bee", "demo"],
    creator: {}, // typically a class
    context: {}, // custom data (propagates to the event's context property)
    groupId: undefined, // optional id for grouping common events (propagates to the event's groupId property)
    trace: undefined, // data related to identity what emitted what and which context (internally used by framework's components)
  });

  // Listen for "start" event
  emitter.on("start", async (data, event: EventMeta) => {
    console.log(`Received ${event.name} event with id "${data.id}"`);
  });

  // Listen for "update" event
  emitter.on("update", async (data, event: EventMeta) => {
    console.log(`Received ${event.name}' with id "${data.id}" and data ${data.data}`);
  });

  await emitter.emit("start", { id: 123 });
  await emitter.emit("update", { id: 123, data: "Hello Bee!" });

  ```
</CodeGroup>

***

## Examples

<CardGroup cols={2}>
  <Card title="Python" icon="python" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/emitter">
    Explore reference emitter implementations in Python
  </Card>

  <Card title="TypeScript" icon="js" href="https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples/emitter">
    Explore reference emitter implementations in TypeScript
  </Card>
</CardGroup>


# Errors
Source: https://framework.beeai.dev/modules/errors



## Overview

Error handling is a critical part of any Python application, especially when dealing with asynchronous operations, various error types, and error propagation across multiple layers. In the BeeAI Framework, we provide a robust and consistent error-handling structure that ensures reliability and ease of debugging.

<Note>
  Supported in Python and TypeScript.
</Note>

***

## The FrameworkError class

Within the BeeAI Framework, regular Python Exceptions are used to handle common issues such as `ValueError`, `TypeError`. However, to provide a more comprehensive error handling experience, we have introduced `FrameworkError`, which is a subclass of Exception. Where additional context is needed, we can use `FrameworkError` to provide additional information about the nature of the error. This may wrap the original exception following the standard Python approach.

Benefits of using `FrameworkError`:

* **Additional properties**: Exceptions may include additional properties to provide a more detailed view of the error.
* **Preserved error chains**: Retains the full history of errors, giving developers full context for debugging.
* **Context**: Each error can contain a dictionary of context, allowing you to store additional values to help the user identify and debug the error.
* **Utility functions:** Includes methods for formatting error stack traces and explanations, making them suitable for use with LLMs and other external tools.
* **Native support:** Built on native Python Exceptions functionality, avoiding the need for additional dependencies while leveraging familiar mechanisms.

This structure ensures that users can trace the complete error history while clearly identifying any errors originating from the BeeAI Framework.

<CodeGroup>
  {/* <!-- embedme python/examples/errors/base.py --> */}

  ```py Python [expandable]
  from beeai_framework.errors import FrameworkError

  # Create the main FrameworkError instance
  error = FrameworkError("Function 'getUser' has failed.", is_fatal=True, is_retryable=False)
  inner_error = FrameworkError("Cannot retrieve data from the API.")
  innermost_error = ValueError("User with Given ID Does not exist!")

  # Chain the errors together using __cause__
  inner_error.__cause__ = innermost_error
  error.__cause__ = inner_error

  # Set the context dictionary for the top level error
  # Add any additional context here. This will help with debugging
  error.context["workflow"] = "activity_planner"
  error.context["provider"] = "ollama"
  error.context["chat_model"] = "granite3.2:8b"

  # Print some properties of the error
  print("\n-- Error properties:")
  print(f"Message: {error.message}")  # Main error message
  # Is the error fatal/retryable?
  print(f"Meta: fatal:{FrameworkError.is_fatal(error)} retryable:{FrameworkError.is_retryable(error)}")
  print(f"Cause: {error.get_cause()}")  # Prints the cause of the error
  print(f"Context: {error.context}")  # Prints the dictionary of the error context

  print("\n-- Explain:")
  print(error.explain())  # Human-readable format without stack traces (ideal for LLMs)
  print("\n-- str():")
  print(str(error))  # Human-readable format (for debug)

  ```

  {/* <!-- embedme typescript/examples/errors/base.ts --> */}

  ```ts TypeScript [expandable]
  import { FrameworkError } from "beeai-framework/errors";

  const error = new FrameworkError(
    "Function 'getUser' has failed.",
    [
      new FrameworkError("Cannot retrieve data from the API.", [
        new Error("User with given ID does not exist!"),
      ]),
    ],
    {
      context: { input: { id: "123" } },
      isFatal: true,
      isRetryable: false,
    },
  );

  console.log("Message", error.message); // Main error message
  console.log("Meta", { fatal: error.isFatal, retryable: error.isRetryable }); // Is the error fatal/retryable?
  console.log("Context", error.context); // Context in which the error occurred
  console.log(error.explain()); // Human-readable format without stack traces (ideal for LLMs)
  console.log(error.dump()); // Full error dump, including sub-errors
  console.log(error.getCause()); // Retrieve the initial cause of the error

  ```
</CodeGroup>

Framework error also has two additional properties which help with agent processing, though ultimately the code that catches the exception will determine the appropriate action.

* **is\_retryable** : hints that the error is retryable.
* **is\_fatal** : hints that the error is fatal.

### Specialized error classes

The BeeAI Framework extends `FrameworkError` to create specialized error classes for different components or scenarios. This ensures that each part of the framework has clear and well-defined error types, improving debugging and error handling.

<Tip>
  Casting an unknown error to a `FrameworkError` can be done by calling the `FrameworkError.ensure` static method ([Python example](https://github.com/i-am-bee/beeai-framework/blob/main/python/examples/errors/cast.py), [TypeScript example](https://github.com/i-am-bee/beeai-framework/blob/main/typescript/examples/errors/cast.ts)).
</Tip>

The definitions for these classes are typically local to the module where they are raised.

| Error Class                | Category         | Description                                                                              |
| :------------------------- | :--------------- | :--------------------------------------------------------------------------------------- |
| `AbortError`               | Aborts           | Raised when an operation has been aborted                                                |
| `ToolError`                | Tools            | Raised when a problem is reported by a tool                                              |
| `ToolInputValidationError` | Tools            | Extends ToolError, raised when input validation fails                                    |
| `AgentError`               | Agents           | Raised when problems occur in agents                                                     |
| `PromptTemplateError`      | Prompt Templates | Raised when problems occur processing prompt templates                                   |
| `LoggerError`              | Loggers          | Raised when errors occur during logging                                                  |
| `SerializerError`          | Serializers      | Raised when problems occur serializing or deserializing objects                          |
| `WorkflowError`            | Workflow         | Raised when a workflow encounters an error                                               |
| `ParserError`              | Parser           | Raised when a parser fails to parse the input data. Includes additional *Reason*         |
| `ResourceError`            | Memory           | Raised when an error occurs with processing agent memory                                 |
| `ResourceFatalError`       | Memory           | Extends ResourceError, raised for particularly severe errors that are likely to be fatal |
| `EmitterError`             | Emitter          | Raised when a problem occurs in the emitter                                              |
| `BackendError`             | Backend          | Raised when a backend encounters an error                                                |
| `ChatModelError`           | Backend          | Extends BackendError, raised when a chat model fails to process input data               |
| `MessageError`             | Backend          | Raised when a message processing fails                                                   |

## Tools example

<CodeGroup>
  {/* <!-- embedme python/examples/errors/tool.py --> */}

  ```py Python [expandable]
  import asyncio

  from beeai_framework.tools import ToolError, tool


  async def main() -> None:
      @tool
      def dummy() -> None:
          """
          A dummy tool.
          """
          raise ToolError("Dummy error.")

      await dummy.run({})


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except ToolError as e:
          print("===CAUSE===")
          print(e.get_cause())
          print("===EXPLAIN===")
          print(e.explain())

  ```

  {/* <!-- embedme typescript/examples/errors/tool.ts --> */}

  ```ts TypeScript [expandable]
  import { DynamicTool, ToolError } from "beeai-framework/tools/base";
  import { FrameworkError } from "beeai-framework/errors";
  import { z } from "zod";

  const tool = new DynamicTool({
    name: "dummy",
    description: "dummy",
    inputSchema: z.object({}),
    handler: async () => {
      throw new Error("Division has failed.");
    },
  });

  try {
    await tool.run({});
  } catch (e) {
    const err = e as FrameworkError;
    console.log(e instanceof ToolError); // true
    console.log("===DUMP===");
    console.log(err.dump());

    console.log("===EXPLAIN===");
    console.log(err.explain());
  }

  ```
</CodeGroup>

***

## Usage

### Basic usage

To use Framework error, add the following import:

```py
from beeai_framework.errors import FrameworkError
```

Add any additional custom errors you need in your code to the import, for example

```py
from beeai_framework.errors import FrameworkError, ChatModelError,ToolError
```

### Creating custom errors

If you wish to create additional errors, you can extend `FrameworkError` or any of the other errors above:

```py
from beeai_framework.errors import FrameworkError

class MyCustomError(FrameworkError):
    def __init__(self, message: str = "My custom error", *, cause: Exception | None = None, context: dict | None = None) -> None:
        super().__init__(message, is_fatal=True, is_retryable=False, cause=cause, context=context)
```

### Wrapping existing errors

You can wrap existing errors in a `FrameworkError`, for example:

```py
inner_err: Exception = ValueError("Value error")
error = FrameworkError.ensure(inner_err)
raise(error)
```

### Using properties and methods

Framework error also has two additional properties which help with agent processing, though ultimately the code that catches the exception will determine the appropriate action.

* **is\_retryable** : hints that the error is retryable.
* **is\_fatal** : hints that the error is fatal.

These can be accessed via:

```py
err = FrameworkError("error")
isfatal: bool = FrameworkError.is_fatal(err)
isretryable: bool = FrameworkError.is_retryable(err)
```

This allows use of some useful functions within the error class.

For example the `explain` static method will return a string that may be more useful for an LLM to interpret:

```py
message: str = FrameworkError.ensure(error).explain()
```

See the source file [errors.py](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/errors.py) for additional methods.

## Examples

<CardGroup cols={2}>
  <Card title="Python" icon="python" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/errors">
    Explore reference error implementations in Python
  </Card>

  <Card title="TypeScript" icon="js" href="https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples/errors">
    Explore reference error implementations in TypeScript
  </Card>
</CardGroup>


# Events
Source: https://framework.beeai.dev/modules/events



## Overview

BeeAI framework uses an event-driven architecture that allows you to observe and respond to various events throughout the execution lifecycle. This document outlines the standard events emitted by different components and their data structures.

All events in the framework follow a consistent pattern:

* Each event has a name (e.g., "start", "success", "error")
* Each event contains a data payload with a defined datatype
* Events can be observed by attaching listeners to the appropriate emitter

<Note>
  Supported in Python and TypeScript.
</Note>

***

## Event types

### ReActAgent events

The following events can be observed calling `ReActAgent.run`.

| Event                         | Data Type                | Description                                                |
| :---------------------------- | :----------------------- | :--------------------------------------------------------- |
| `start`                       | `ReActAgentStartEvent`   | Triggered when the agent begins execution.                 |
| `error`                       | `ReActAgentErrorEvent`   | Triggered when the agent encounters an error.              |
| `retry`                       | `ReActAgentRetryEvent`   | Triggered when the agent is retrying an operation.         |
| `success`                     | `ReActAgentSuccessEvent` | Triggered when the agent successfully completes execution. |
| `update` and `partial_update` | `ReActAgentUpdateEvent`  | Triggered when the agent updates its state.                |
| `tool_start`                  | `ReActAgentToolEvent`    | Triggered when the agent begins using a tool.              |
| `tool_success`                | `ReActAgentToolEvent`    | Triggered when a tool operation completes successfully.    |
| `tool_error`                  | `ReActAgentToolEvent`    | Triggered when a tool operation fails.                     |

[Check out the in-code definition](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/agents/react/events.py) the in-code definition.

### ChatModel events

The following events can be observed when calling `ChatModel.create` or `ChatModel.create_structure`.

| Event       | Data Type                | Description                                                                |
| :---------- | :----------------------- | :------------------------------------------------------------------------- |
| `new_token` | `ChatModelNewTokenEvent` | Triggered when a new token is generated during streaming.                  |
| `success`   | `ChatModelSuccessEvent`  | Triggered when the model generation completes successfully.                |
| `start`     | `ChatModelStartEvent`    | Triggered when model generation begins.                                    |
| `error`     | `ChatModelErrorEvent`    | Triggered when model generation encounters an error.                       |
| `finish`    | `None`                   | Triggered when model generation finishes (regardless of success or error). |

[Check out the in-code definition](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/backend/events.py) the in-code definition.

### Tool events

The following events can be observed when calling `Tool.run`.

| Event     | Data Type          | Description                                                              |
| :-------- | :----------------- | :----------------------------------------------------------------------- |
| `start`   | `ToolStartEvent`   | Triggered when a tool starts executing.                                  |
| `success` | `ToolSuccessEvent` | Triggered when a tool completes execution successfully.                  |
| `error`   | `ToolErrorEvent`   | Triggered when a tool encounters an error.                               |
| `retry`   | `ToolRetryEvent`   | Triggered when a tool operation is being retried.                        |
| `finish`  | `None`             | Triggered when tool execution finishes (regardless of success or error). |

[Check out the in-code definition](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/tools/events.py) the in-code definition.

### Workflow events

The following events can be observed when calling `Workflow.run`.

| Event     | Data Type              | Description                                            |
| :-------- | :--------------------- | :----------------------------------------------------- |
| `start`   | `WorkflowStartEvent`   | Triggered when a workflow step begins execution.       |
| `success` | `WorkflowSuccessEvent` | Triggered when a workflow step completes successfully. |
| `error`   | `WorkflowErrorEvent`   | Triggered when a workflow step encounters an error.    |

[Check out the in-code definition](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/workflows/events.py) the in-code definition.

### ToolCallingAgent events

The following events can be observed calling `ToolCallingAgent.run`.

| Event     | Data Type                      | Description                                                |
| :-------- | :----------------------------- | :--------------------------------------------------------- |
| `start`   | `ToolCallingAgentStartEvent`   | Triggered when the agent begins execution.                 |
| `success` | `ToolCallingAgentSuccessEvent` | Triggered when the agent successfully completes execution. |

[Check out the in-code definition](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/agents/tool_calling/events.py) the in-code definition.

### RequirementAgent events

| Event     | Data Type                      | Description                                                |
| :-------- | :----------------------------- | :--------------------------------------------------------- |
| `start`   | `RequirementAgentStartEvent`   | Triggered when the agent begins execution.                 |
| `success` | `RequirementAgentSuccessEvent` | Triggered when the agent successfully completes execution. |

[Check out the in-code definition](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/agents/experimental/events.py) the in-code definition.

## RunContext events (internal)

Special events that are emitted before the target's handler gets executed.
A run event contains `.run.` in its event's path and has `internal` set to true in event's context object.

| Event     | Data Type             | Description                      |
| :-------- | :-------------------- | :------------------------------- |
| `start`   | `None`                | Triggered when the run starts.   |
| `success` | `<Run return object>` | Triggered when the run succeeds. |
| `error`   | `FrameworkError`      | Triggered when an error occurs.  |
| `finish`  | `None`                | Triggered when the run finishes. |

[Check out the in-code definition](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/context.py#L260-L273) the in-code definition.

### LinePrefixParser events

The following events are caught internally by the line prefix parser.

| Event            | Data Type                | Description                             |
| :--------------- | :----------------------- | :-------------------------------------- |
| `update`         | `LinePrefixParserUpdate` | Triggered when an update occurs.        |
| `partial_update` | `LinePrefixParserUpdate` | Triggered when a partial update occurs. |

[Check out the in-code definition](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/parsers/line_prefix.py) the in-code definition.


# Logger
Source: https://framework.beeai.dev/modules/logger



## Overview

Logger is a core component designed to record and track events, errors, and other important actions during application execution. It provides valuable insights into application behavior, performance, and potential issues, helping developers troubleshoot and monitor systems effectively.

In the BeeAI framework, the `Logger` class is an abstraction built on top of Python's built-in logging module, offering enhanced capabilities specifically designed for AI agent workflows.

<Note>
  Supported in Python and TypeScript.
</Note>

***

## Key features

* Custom log levels: Adds additional log levels like TRACE (below DEBUG) for fine-grained control
* Customized formatting: Different formatting for regular logs vs. event messages
* Agent interaction logging: Special handling for agent-generated events and communication
* Error integration: Seamless integration with the framework's error handling system

***

## Getting started

### Basic usage

To use the logger in your application:

<CodeGroup>
  {/* <!-- embedme python/examples/logger/base.py --> */}

  ```py Python [expandable]
  from beeai_framework.logger import Logger

  # Configure logger with default log level
  logger = Logger("app", level="TRACE")

  # Log at different levels
  logger.trace("Trace!")
  logger.debug("Debug!")
  logger.info("Info!")
  logger.warning("Warning!")
  logger.error("Error!")
  logger.fatal("Fatal!")

  ```

  {/* <!-- embedme typescript/examples/logger/base.ts --> */}

  ```ts TypeScript [expandable]
  import { Logger, LoggerLevel } from "beeai-framework/logger/logger";

  // Configure logger defaults
  Logger.defaults.pretty = true; // Pretty-print logs (default: false, can also be set via ENV: BEE_FRAMEWORK_LOG_PRETTY=true)
  Logger.defaults.level = LoggerLevel.TRACE; // Set log level to trace (default: TRACE, can also be set via ENV: BEE_FRAMEWORK_LOG_LEVEL=trace)
  Logger.defaults.name = undefined; // Optional name for logger (default: undefined)
  Logger.defaults.bindings = {}; // Optional bindings for structured logging (default: empty)

  // Create a child logger for your app
  const logger = Logger.root.child({ name: "app" });

  // Log at different levels
  logger.trace("Trace!");
  logger.debug("Debug!");
  logger.info("Info!");
  logger.warn("Warning!");
  logger.error("Error!");
  logger.fatal("Fatal!");

  ```
</CodeGroup>

## Configuration

The logger's behavior can be customized through environment variables:

* `BEEAI_LOG_LEVEL`: Sets the default log level (defaults to "INFO")

You can also set a specific level when initializing the logger.

### Custom log levels

The logger adds a TRACE level below DEBUG for extremely detailed logging:

```py
# Configure a logger with a specific level
logger = Logger("app", level="TRACE")  # Or use logging constants like logging.DEBUG

# Log with the custom TRACE level
logger.trace("This is a very low-level trace message")
```

***

## Working with the logger

### Formatting

The logger uses a custom formatter that distinguishes between regular log messages and event messages:

* Regular logs: `{timestamp} | {level} | {module}:{function}:{line} - {message}`
* Event messages: `{timestamp} | {level} | {message}`

### Icons and formatting

When logging agent interactions, the logger automatically adds visual icons:

* User messages: üë§
* Agent messages: ü§ñ

This makes logs easier to read and understand when reviewing conversational agent flows.

## Error handling

The logger integrates with BeeAI framework's error handling system through the `LoggerError` class.

***

## Usage with agents

The Logger seamlessly integrates with agents in the framework. Below is an example that demonstrates how logging can be used in conjunction with agents and event emitters.

<CodeGroup>
  {/* <!-- embedme python/examples/logger/agent.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.agents.react import ReActAgent
  from beeai_framework.backend import ChatModel
  from beeai_framework.errors import FrameworkError
  from beeai_framework.logger import Logger
  from beeai_framework.memory import UnconstrainedMemory


  async def main() -> None:
      logger = Logger("app", level="TRACE")

      agent = ReActAgent(llm=ChatModel.from_name("ollama:granite3.3:8b"), tools=[], memory=UnconstrainedMemory())

      output = await agent.run("Hello!").observe(
          lambda emitter: emitter.on(
              "update", lambda data, event: logger.info(f"Event {event.path} triggered by {type(event.creator).__name__}")
          )
      )

      logger.info(f"Agent ü§ñ : {output.last_message.text}")


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/logger/agent.ts --> */}

  ```ts TypeScript [expandable]
  import { ReActAgent } from "beeai-framework/agents/react/agent";
  import { UnconstrainedMemory } from "beeai-framework/memory/unconstrainedMemory";
  import { Logger } from "beeai-framework/logger/logger";
  import { Emitter } from "beeai-framework/emitter/emitter";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  // Set up logging
  Logger.defaults.pretty = true;

  const logger = Logger.root.child({
    level: "trace",
    name: "app",
  });

  // Log events emitted during agent execution
  Emitter.root.match("*.*", (data, event) => {
    const logLevel = event.path.includes(".run.") ? "trace" : "info";
    logger[logLevel](`Event '${event.path}' triggered by '${event.creator.constructor.name}'`);
  });

  // Create and run an agent
  const agent = new ReActAgent({
    llm: new OllamaChatModel("llama3.1"),
    memory: new UnconstrainedMemory(),
    tools: [],
  });

  const response = await agent.run({ prompt: "Hello!" });
  logger.info(response.result.text);

  ```
</CodeGroup>

***

## Examples

<CardGroup cols={2}>
  <Card title="Python" icon="python" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/logger">
    COMING SOON: Explore reference logger implementations in Python
  </Card>

  <Card title="TypeScript" icon="js" href="https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples/logger">
    COMING SOON: Explore reference logger implementations in TypeScript
  </Card>
</CardGroup>


# Memory
Source: https://framework.beeai.dev/modules/memory



## Overview

Memory in the context of an agent refers to the system's capability to store, recall, and utilize information from past interactions. This enables the agent to maintain context over time, improve its responses based on previous exchanges, and provide a more personalized experience.

BeeAI framework provides several memory implementations:

| Type                                            | Description                                             |
| ----------------------------------------------- | ------------------------------------------------------- |
| [**UnconstrainedMemory**](#unconstrainedmemory) | Unlimited storage for all messages                      |
| [**SlidingMemory**](#slidingmemory)             | Keeps only the most recent k entries                    |
| [**TokenMemory**](#tokenmemory)                 | Manages token usage to stay within model context limits |
| [**SummarizeMemory**](#summarizememory)         | Maintains a single summarization of the conversation    |

<Note>
  Supported in Python and TypeScript.
</Note>

***

## Core concepts

### Messages

Messages are the fundamental units stored in memory, representing interactions between users and agents:

* Each message has a role (USER, ASSISTANT, SYSTEM)
* Messages contain text content
* Messages can be added, retrieved, and processed

### Memory types

Different memory strategies are available depending on your requirements:

* **Unconstrained** - Store unlimited messages
* **Sliding Window** - Keep only the most recent N messages
* **Token-based** - Manage a token budget to stay within model context limits
* **Summarization** - Compress previous interactions into summaries

### Integration points

Memory components integrate with other parts of the framework:

* LLMs use memory to maintain conversation context
* Agents access memory to process and respond to interactions
* Workflows can share memory between different processing steps

***

## Basic usage

### Capabilities showcase

<CodeGroup>
  {/* <!-- embedme python/examples/memory/base.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.backend import AssistantMessage, SystemMessage, UserMessage
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import UnconstrainedMemory


  async def main() -> None:
      memory = UnconstrainedMemory()

      # Single Message
      await memory.add(SystemMessage("You are a helpful assistant"))

      # Multiple Messages
      await memory.add_many([UserMessage("What can you do?"), AssistantMessage("Everything!")])

      print(memory.is_empty())  # false
      for message in memory.messages:  # prints the text of all messages
          print(message.text)
      print(memory.as_read_only())  # returns a new read only instance
      memory.reset()  # removes all messages


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/memory/base.ts --> */}

  ```ts TypeScript [expandable]
  import { UnconstrainedMemory } from "beeai-framework/memory/unconstrainedMemory";
  import { AssistantMessage, SystemMessage, UserMessage } from "beeai-framework/backend/message";

  const memory = new UnconstrainedMemory();

  // Single message
  await memory.add(new SystemMessage(`You are a helpful assistant.`));

  // Multiple messages
  await memory.addMany([new UserMessage(`What can you do?`), new AssistantMessage(`Everything!`)]);

  console.info(memory.isEmpty()); // false
  console.info(memory.messages); // prints all saved messages
  console.info(memory.asReadOnly()); // returns a NEW read only instance
  memory.reset(); // removes all messages

  ```
</CodeGroup>

### Usage with LLMs

<CodeGroup>
  {/* <!-- embedme python/examples/memory/llm_memory.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.adapters.ollama import OllamaChatModel
  from beeai_framework.backend import AssistantMessage, SystemMessage, UserMessage
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import UnconstrainedMemory


  async def main() -> None:
      memory = UnconstrainedMemory()
      await memory.add_many(
          [
              SystemMessage("Always respond very concisely."),
              UserMessage("Give me the first 5 prime numbers."),
          ]
      )

      llm = OllamaChatModel("llama3.1")
      response = await llm.create(messages=memory.messages)
      await memory.add(AssistantMessage(response.get_text_content()))

      print("Conversation history")
      for message in memory.messages:
          print(f"{message.role}: {message.text}")


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/memory/llmMemory.ts --> */}

  ```ts TypeScript [expandable]
  import { UnconstrainedMemory } from "beeai-framework/memory/unconstrainedMemory";
  import { Message } from "beeai-framework/backend/message";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  const memory = new UnconstrainedMemory();
  await memory.addMany([
    Message.of({
      role: "system",
      text: `Always respond very concisely.`,
    }),
    Message.of({ role: "user", text: `Give me first 5 prime numbers.` }),
  ]);

  // Generate response
  const llm = new OllamaChatModel("llama3.1");
  const response = await llm.create({ messages: memory.messages });
  await memory.add(Message.of({ role: "assistant", text: response.getTextContent() }));

  console.log(`Conversation history`);
  for (const message of memory) {
    console.log(`${message.role}: ${message.text}`);
  }

  ```
</CodeGroup>

<Tip>
  Memory for non-chat LLMs works exactly the same way.
</Tip>

### Usage with agents

<CodeGroup>
  {/* <!-- embedme python/examples/memory/agent_memory.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.agents.react import ReActAgent
  from beeai_framework.backend import AssistantMessage, ChatModel, UserMessage
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import UnconstrainedMemory

  # Initialize the memory and LLM
  memory = UnconstrainedMemory()


  def create_agent() -> ReActAgent:
      llm = ChatModel.from_name("ollama:granite3.3:8b")

      # Initialize the agent
      agent = ReActAgent(llm=llm, memory=memory, tools=[])

      return agent


  async def main() -> None:
      # Create user message
      user_input = "Hello world!"
      user_message = UserMessage(user_input)

      # Await adding user message to memory
      await memory.add(user_message)
      print("Added user message to memory")

      # Create agent
      agent = create_agent()

      response = await agent.run(
          user_input,
          max_retries_per_step=3,
          total_max_retries=10,
          max_iterations=20,
      )
      print(f"Received response: {response}")

      # Create and store assistant's response
      assistant_message = AssistantMessage(response.last_message.text)

      # Await adding assistant message to memory
      await memory.add(assistant_message)
      print("Added assistant message to memory")

      # Print results
      print(f"\nMessages in memory: {len(agent.memory.messages)}")

      if len(agent.memory.messages) >= 1:
          user_msg = agent.memory.messages[0]
          print(f"User: {user_msg.text}")

      if len(agent.memory.messages) >= 2:
          agent_msg = agent.memory.messages[1]
          print(f"Agent: {agent_msg.text}")
      else:
          print("No agent message found in memory")


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/memory/agentMemory.ts --> */}

  ```ts TypeScript [expandable]
  import { UnconstrainedMemory } from "beeai-framework/memory/unconstrainedMemory";
  import { ReActAgent } from "beeai-framework/agents/react/agent";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  const agent = new ReActAgent({
    memory: new UnconstrainedMemory(),
    llm: new OllamaChatModel("llama3.1"),
    tools: [],
  });
  await agent.run({ prompt: "Hello world!" });

  console.info(agent.memory.messages.length); // 2

  const userMessage = agent.memory.messages[0];
  console.info(`User: ${userMessage.text}`); // User: Hello world!

  const agentMessage = agent.memory.messages[1];
  console.info(`Agent: ${agentMessage.text}`); // Agent: Hello! It's nice to chat with you.

  ```
</CodeGroup>

<Tip>
  If your memory already contains the user message, run the agent with `prompt: null`.
</Tip>

<Note>
  ReAct Agent internally uses `TokenMemory` to store intermediate steps for a given run.
</Note>

***

## Memory types

The framework provides multiple out-of-the-box memory implementations for different use cases.

### UnconstrainedMemory

Unlimited in size, stores all messages without constraints.

<CodeGroup>
  {/* <!-- embedme python/examples/memory/unconstrained_memory.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.backend import UserMessage
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import UnconstrainedMemory


  async def main() -> None:
      # Create memory instance
      memory = UnconstrainedMemory()

      # Add a message
      await memory.add(UserMessage("Hello world!"))

      # Print results
      print(f"Is Empty: {memory.is_empty()}")  # Should print: False
      print(f"Message Count: {len(memory.messages)}")  # Should print: 1

      print("\nMessages:")
      for msg in memory.messages:
          print(f"{msg.role}: {msg.text}")


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/memory/unconstrainedMemory.ts --> */}

  ```ts TypeScript [expandable]
  import { UnconstrainedMemory } from "beeai-framework/memory/unconstrainedMemory";
  import { Message } from "beeai-framework/backend/message";

  const memory = new UnconstrainedMemory();
  await memory.add(
    Message.of({
      role: "user",
      text: `Hello world!`,
    }),
  );

  console.info(memory.isEmpty()); // false
  console.log(memory.messages.length); // 1
  console.log(memory.messages);

  ```
</CodeGroup>

### SlidingMemory

Keeps last `k` entries in the memory. The oldest ones are deleted (unless specified otherwise).

<CodeGroup>
  {/* <!-- embedme python/examples/memory/sliding_memory.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.backend import AssistantMessage, SystemMessage, UserMessage
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import SlidingMemory, SlidingMemoryConfig


  async def main() -> None:
      # Create sliding memory with size 3
      memory = SlidingMemory(
          SlidingMemoryConfig(
              size=3,
              handlers={"removal_selector": lambda messages: messages[0]},  # Remove oldest message
          )
      )

      # Add messages
      await memory.add(SystemMessage("You are a helpful assistant."))

      await memory.add(UserMessage("What is Python?"))

      await memory.add(AssistantMessage("Python is a programming language."))

      # Adding a fourth message should trigger sliding window
      await memory.add(UserMessage("What about JavaScript?"))

      # Print results
      print(f"Messages in memory: {len(memory.messages)}")  # Should print 3
      for msg in memory.messages:
          print(f"{msg.role}: {msg.text}")


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/memory/slidingMemory.ts --> */}

  ```ts TypeScript [expandable]
  import { SlidingMemory } from "beeai-framework/memory/slidingMemory";
  import { Message } from "beeai-framework/backend/message";

  const memory = new SlidingMemory({
    size: 3, // (required) number of messages that can be in the memory at a single moment
    handlers: {
      // optional
      // we select a first non-system message (default behaviour is to select the oldest one)
      removalSelector: (messages) => messages.find((msg) => msg.role !== "system")!,
    },
  });

  await memory.add(Message.of({ role: "system", text: "You are a guide through France." }));
  await memory.add(Message.of({ role: "user", text: "What is the capital?" }));
  await memory.add(Message.of({ role: "assistant", text: "Paris" }));
  await memory.add(Message.of({ role: "user", text: "What language is spoken there?" })); // removes the first user's message
  await memory.add(Message.of({ role: "assistant", text: "French" })); // removes the first assistant's message

  console.info(memory.isEmpty()); // false
  console.log(memory.messages.length); // 3
  console.log(memory.messages);

  ```
</CodeGroup>

### TokenMemory

Ensures that the token sum of all messages is below the given threshold.
If overflow occurs, the oldest message will be removed.

<CodeGroup>
  {/* <!-- embedme python/examples/memory/token_memory.py --> */}

  ```py Python [expandable]
  import asyncio
  import math
  import sys
  import traceback

  from beeai_framework.adapters.ollama import OllamaChatModel
  from beeai_framework.backend import Role, SystemMessage, UserMessage
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import TokenMemory

  # Initialize the LLM
  llm = OllamaChatModel()

  # Initialize TokenMemory with handlers
  memory = TokenMemory(
      llm=llm,
      max_tokens=None,  # Will be inferred from LLM
      capacity_threshold=0.75,
      sync_threshold=0.25,
      handlers={
          "removal_selector": lambda messages: next((msg for msg in messages if msg.role != Role.SYSTEM), messages[0]),
          "estimate": lambda msg: math.ceil((len(msg.role) + len(msg.text)) / 4),
      },
  )


  async def main() -> None:
      # Add system message
      system_message = SystemMessage("You are a helpful assistant.")
      await memory.add(system_message)
      print(f"Added system message (hash: {hash(system_message)})")

      # Add user message
      user_message = UserMessage("Hello world!")
      await memory.add(user_message)
      print(f"Added user message (hash: {hash(user_message)})")

      # Check initial memory state
      print("\nInitial state:")
      print(f"Is Dirty: {memory.is_dirty}")
      print(f"Tokens Used: {memory.tokens_used}")

      # Sync token counts
      await memory.sync()
      print("\nAfter sync:")
      print(f"Is Dirty: {memory.is_dirty}")
      print(f"Tokens Used: {memory.tokens_used}")

      # Print all messages
      print("\nMessages in memory:")
      for msg in memory.messages:
          print(f"{msg.role}: {msg.text} (hash: {hash(msg)})")


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/memory/tokenMemory.ts --> */}

  ```ts TypeScript [expandable]
  import { TokenMemory } from "beeai-framework/memory/tokenMemory";
  import { Message } from "beeai-framework/backend/message";

  const memory = new TokenMemory({
    maxTokens: undefined, // optional (default is 128k),
    capacityThreshold: 0.75, // maxTokens*capacityThreshold = threshold where we start removing old messages
    syncThreshold: 0.25, // maxTokens*syncThreshold = threshold where we start to use a real tokenization endpoint instead of guessing the number of tokens
    handlers: {
      // optional way to define which message should be deleted (default is the oldest one)
      removalSelector: (messages) => messages.find((msg) => msg.role !== "system")!,

      // optional way to estimate the number of tokens in a message before we use the actual tokenize endpoint (number of tokens < maxTokens*syncThreshold)
      estimate: (msg) => Math.ceil((msg.role.length + msg.text.length) / 4),
    },
  });

  await memory.add(Message.of({ role: "system", text: "You are a helpful assistant." }));
  await memory.add(Message.of({ role: "user", text: "Hello world!" }));

  console.info(memory.isDirty); // is the consumed token count estimated or retrieved via the tokenize endpoint?
  console.log(memory.tokensUsed); // number of used tokens
  console.log(memory.stats()); // prints statistics
  await memory.sync(); // calculates real token usage for all messages marked as "dirty"

  ```
</CodeGroup>

### SummarizeMemory

Only a single summarization of the conversation is preserved. Summarization is updated with every new message.

<CodeGroup>
  {/* <!-- embedme python/examples/memory/summarize_memory.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.backend import AssistantMessage, ChatModel, SystemMessage, UserMessage
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import SummarizeMemory


  async def main() -> None:
      # Initialize the LLM with parameters
      llm = ChatModel.from_name(
          "ollama:granite3.3:8b",
          # ChatModelParameters(temperature=0),
      )

      # Create summarize memory instance
      memory = SummarizeMemory(llm)

      # Add messages
      await memory.add_many(
          [
              SystemMessage("You are a guide through France."),
              UserMessage("What is the capital?"),
              AssistantMessage("Paris"),
              UserMessage("What language is spoken there?"),
          ]
      )

      # Print results
      print(f"Is Empty: {memory.is_empty()}")
      print(f"Message Count: {len(memory.messages)}")

      if memory.messages:
          print(f"Summary: {memory.messages[0].get_texts()[0].text}")


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/memory/summarizeMemory.ts --> */}

  ```ts TypeScript [expandable]
  import { Message } from "beeai-framework/backend/message";
  import { SummarizeMemory } from "beeai-framework/memory/summarizeMemory";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  const memory = new SummarizeMemory({
    llm: new OllamaChatModel("llama3.1"),
  });

  await memory.addMany([
    Message.of({ role: "system", text: "You are a guide through France." }),
    Message.of({ role: "user", text: "What is the capital?" }),
    Message.of({ role: "assistant", text: "Paris" }),
    Message.of({ role: "user", text: "What language is spoken there?" }),
  ]);

  console.info(memory.isEmpty()); // false
  console.log(memory.messages.length); // 1
  console.log(memory.messages[0].text); // The capital city of France is Paris, ...

  ```
</CodeGroup>

***

## Creating custom memory

To create your memory implementation, you must implement the `BaseMemory` class.

<CodeGroup>
  {/* <!-- embedme python/examples/memory/custom.py --> */}

  ```py Python [expandable]
  from typing import Any

  from beeai_framework.backend import AnyMessage
  from beeai_framework.memory import BaseMemory


  class MyMemory(BaseMemory):
      @property
      def messages(self) -> list[AnyMessage]:
          raise NotImplementedError("Method not yet implemented.")

      async def add(self, message: AnyMessage, index: int | None = None) -> None:
          raise NotImplementedError("Method not yet implemented.")

      async def delete(self, message: AnyMessage) -> bool:
          raise NotImplementedError("Method not yet implemented.")

      def reset(self) -> None:
          raise NotImplementedError("Method not yet implemented.")

      def create_snapshot(self) -> Any:
          raise NotImplementedError("Method not yet implemented.")

      def load_snapshot(self, state: Any) -> None:
          raise NotImplementedError("Method not yet implemented.")

  ```

  {/* <!-- embedme typescript/examples/memory/custom.ts --> */}

  ```ts TypeScript [expandable]
  import { BaseMemory } from "beeai-framework/memory/base";
  import { Message } from "beeai-framework/backend/message";
  import { NotImplementedError } from "beeai-framework/errors";

  export class MyMemory extends BaseMemory {
    get messages(): readonly Message[] {
      throw new NotImplementedError("Method not implemented.");
    }

    add(message: Message, index?: number): Promise<void> {
      throw new NotImplementedError("Method not implemented.");
    }

    delete(message: Message): Promise<boolean> {
      throw new NotImplementedError("Method not implemented.");
    }

    reset(): void {
      throw new NotImplementedError("Method not implemented.");
    }

    createSnapshot(): unknown {
      throw new NotImplementedError("Method not implemented.");
    }

    loadSnapshot(state: ReturnType<typeof this.createSnapshot>): void {
      throw new NotImplementedError("Method not implemented.");
    }
  }

  ```
</CodeGroup>

<Tip>
  The simplest implementation is `UnconstrainedMemory`.
</Tip>

***

## Examples

<CardGroup cols={2}>
  <Card title="Python" icon="python" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/memory">
    Explore reference memory implementations in Python
  </Card>

  <Card title="TypeScript" icon="js" href="https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples/memory">
    Explore reference memory implementations in TypeScript
  </Card>
</CardGroup>


# Observability
Source: https://framework.beeai.dev/modules/observability

Monitor and debug your BeeAI Framework applications with OpenInference instrumentation

## Overview

The BeeAI Framework provides comprehensive observability through OpenInference instrumentation, enabling you to trace and monitor your AI applications with industry-standard telemetry. This allows you to debug issues, optimize performance, and understand how your agents, tools, and workflows are performing in production.

<Note>
  Currently supported in Python only.
</Note>

## Quickstart

### 1. Install the package

This package provides the OpenInference instrumentor specifically designed for the BeeAI Framework.

```bash
pip install openinference-instrumentation-beeai
```

### 2. Set up observability

Configure OpenTelemetry to create and export spans. This example sets up an OTLP HTTP exporter, a tracer provider, and the BeeAI instrumentor, with the endpoint pointing to a local Arize Phoenix instance.

```py
from openinference.instrumentation.beeai import BeeAIInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk import trace as trace_sdk
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace.export import SimpleSpanProcessor


def setup_observability() -> None:
    resource = Resource(attributes={})
    tracer_provider = trace_sdk.TracerProvider(resource=resource)
    tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))
    trace_api.set_tracer_provider(tracer_provider)

    BeeAIInstrumentor().instrument()
```

<Tip>
  To override the default traces endpoint ([http://localhost:4318/v1/traces](http://localhost:4318/v1/traces)), set the `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT` environment variable.
</Tip>

<Tip>
  For Arize Phoenix, set `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT` to `http://localhost:6006/v1/traces`.
</Tip>

### 3. Enable instrumentation

Call the setup function **before** running any BeeAI Framework code:

```py
setup_observability()
```

The setup function must be called before importing and using any BeeAI Framework components to ensure all operations are properly instrumented.

## What Gets Instrumented

When instrumentation is enabled, BeeAI emits spans and attributes for core runtime operations.

### Agents

* Agent execution start/stop times
* Input prompts and output responses
* Tool usage within agent workflows
* Memory operations and state changes

### Tools

* Tool invocation details
* Input parameters and return values
* Execution time and success/failure status
* Error details when tools fail

### Chat Models

* Model inference requests (including streaming)
* Token usage statistics
* Model parameters (temperature, max tokens, etc.)
* Response timing and latency

### Embedding Models

* Text embedding requests
* Input text and embedding dimensions
* Processing time and batch sizes

### Workflows

* Workflow step execution
* State transitions and data flow
* Step dependencies and execution order

## Observability Backends

### Arize Phoenix

Open-source observability for LLM applications.

<Info>
  **Documentation:** [Arize Phoenix](https://github.com/Arize-ai/phoenix)
</Info>

### LangFuse

Production-ready LLMOps platform with advanced analytics.

<Info>
  **Documentation:** [LangFuse OpenTelemetry Integration](https://langfuse.com/integrations/native/opentelemetry#opentelemetry-native-langfuse-sdk-v3)
</Info>

### LangSmith

Comprehensive LLM development platform by LangChain.

<Info>
  **Documentation:** [LangSmith OpenTelemetry Guide](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
</Info>

### Other Platforms

Any backend supporting OpenTelemetry/OpenInference standards.


# RAG
Source: https://framework.beeai.dev/modules/rag

Build intelligent agents that combine retrieval with generation for enhanced AI capabilities

## Overview

Retrieval-Augmented Generation (RAG) is a powerful paradigm that enhances large language models by providing them with relevant information from external knowledge sources. This approach has become essential for enterprise AI applications that need to work with specific, up-to-date, or domain-specific information that wasn't part of the model's training data.

RAG addresses key limitations of traditional LLMs:

* **Knowledge cutoffs** - Access the most current information
* **Domain expertise** - Integrate specialized knowledge bases
* **Factual accuracy** - Reduce hallucinations with grounded responses
* **Scalability** - Work with vast document collections efficiently

Enterprises rely on RAG for applications like customer support, document analysis, knowledge management, and intelligent search systems.

<Note>
  Location within the framework: [beeai\_framework/rag](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/rag).
</Note>

<Tip>
  RAG is most effective when document chunking and retrieval strategies are tailored to your specific problem domain. It's recommended to experiment with different configurations such as chunk sizes, overlap settings, and retrieval parameters. Future releases of BeeAI will provide enhanced capabilities to streamline this optimization process.
</Tip>

## Philosophy

BeeAI Framework's approach to RAG emphasizes **integration over invention**. Rather than building RAG components from scratch, we provide seamless adapters for proven, production-ready solutions from leading platforms like LangChain and Llama-Index.

This philosophy offers several advantages:

* **Leverage existing expertise** - Use battle-tested implementations
* **Faster time-to-market** - Focus on your application logic, not infrastructure
* **Community support** - Benefit from extensive documentation and community
* **Flexibility** - Switch between providers as needs evolve

## Installation

To use RAG components, install the framework with the RAG extras:

```bash
pip install "beeai-framework[rag]"
```

## RAG Components

The following table outlines the key RAG components available in the BeeAI Framework:

| Component                                                                                                                             | Description                                                                                                           | Compatibility | Future Compatibility |
| ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- | ------------- | -------------------- |
| [**Document Loaders**](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/backend/document_loader.py)       | Responsible for loading content from different formats and sources such as PDFs, web pages, and structured text files | LangChain     | BeeAI                |
| [**Text Splitters**](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/backend/text_splitter.py)           | Splits long documents into workable chunks using various strategies, e.g. fixed length or preserving context          | LangChain     | BeeAI                |
| [**Document**](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/backend/types.py)                         | The basic data structure to house text content, metadata, and relevant scores for retrieval operations                | BeeAI         | -                    |
| [**Vector Store**](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/backend/vector_store.py)              | Used to store document embeddings and retrieve them based on semantic similarity using embedding distance             | LangChain     | BeeAI, Llama-Index   |
| [**Document Processors**](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/backend/document_processor.py) | Used to process and refine documents during the retrieval-generation lifecycle including reranking and filtering      | Llama-Index   | -                    |

## Dynamic Module Loading

BeeAI Framework provides a dynamic module loading system that allows you to instantiate RAG components using string identifiers. This approach enables configuration-driven architectures and easy provider switching.

The `from_name` method uses the format `provider:ClassName` where:

* `provider` identifies the integration module (e.g., "beeai", "langchain")
* `ClassName` specifies the exact class to instantiate

<Tip>
  Dynamic loading enables you to switch between different vector store implementations without changing your application code - just update the configuration string.
</Tip>

### BeeAI Vector Store

{/* <!-- embedme python/examples/backend/module_loading.py --> */}

```py Python
import asyncio
import sys
import traceback

from beeai_framework.adapters.beeai.backend.vector_store import TemporalVectorStore
from beeai_framework.adapters.langchain.mappers.documents import lc_document_to_document
from beeai_framework.backend.embedding import EmbeddingModel
from beeai_framework.backend.vector_store import VectorStore
from beeai_framework.errors import FrameworkError

# LC dependencies - to be swapped with BAI dependencies
try:
    from langchain_community.document_loaders import UnstructuredMarkdownLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ModuleNotFoundError as e:
    raise ModuleNotFoundError(
        "Optional modules are not found.\nRun 'pip install \"beeai-framework[rag]\"' to install."
    ) from e


async def main() -> None:
    embedding_model = EmbeddingModel.from_name("watsonx:ibm/slate-125m-english-rtrvr-v2", truncate_input_tokens=500)

    # Document loading
    loader = UnstructuredMarkdownLoader(file_path="docs/modules/agents.mdx")
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=1000)
    all_splits = text_splitter.split_documents(docs)
    documents = [lc_document_to_document(document) for document in all_splits]
    print(f"Loaded {len(documents)} documents")

    vector_store: TemporalVectorStore = VectorStore.from_name(
        name="beeai:TemporalVectorStore", embedding_model=embedding_model
    )  # type: ignore[assignment]
    _ = await vector_store.add_documents(documents=documents)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except FrameworkError as e:
        traceback.print_exc()
        sys.exit(e.explain())

```

<Tip>
  Native BeeAI modules can be loaded directly by importing and instantiating the module, e.g. `from beeai_framework.adapters.beeai.backend.vector_store import TemporalVectorStore`.
</Tip>

### Supported Provider's Vector Store

<CodeGroup>
  ```py Python
  # LangChain integration
  vector_store = VectorStore.from_name(
      name="langchain:InMemoryVectorStore",
      embedding_model=embedding_model
  )
  ```

  <Tip>
    You can customize dynamically loaded components by passing additional parameters directly to the `from_name` method. These parameters will be forwarded to the component's constructor, allowing you to configure settings like batch sizes, connection pools, or other provider-specific options without changing your code structure.
  </Tip>

  <Tip>
    The same dynamic loading pattern works for document loaders. For example, you can load documents using `DocumentLoader.from_name("langchain:UnstructuredMarkdownLoader", file_path="docs/modules/agents.mdx")` to get your documents ready for the vector store.
  </Tip>

  ## RAG Agent

  The RAG Agent implements a sophisticated retrieval-augmented generation pipeline that combines the power of semantic search with large language models. The agent follows a three-stage process and supports advanced configuration options including custom reranking, flexible retrieval parameters, comprehensive error handling, and query flexibility using various object types.

  ### 1. Retrieval

  The agent searches the vector store using semantic similarity to find the most relevant documents for the user's query. You can configure the number of documents retrieved and similarity thresholds to optimize for your specific use case.

  ### 2. Reranking (Optional)

  Retrieved documents can be reranked using advanced LLM-based models to improve relevance and quality of the context provided to the generation stage. This step significantly enhances response accuracy for complex queries.

  ### 3. Generation

  The LLM generates a response using the retrieved documents as context, ensuring grounded and accurate answers. Built-in error handling ensures informative error messages are stored in memory when issues occur.

  ### Basic Usage

  <Note>
    Document loading and population of the vector store is the developers's responsibility and out of scope for the agent.
  </Note>

  {/* <!-- embedme python/examples/agents/rag_agent.py --> */}

  ```py Python [expandable]
  import asyncio
  import logging
  import os
  import sys
  import traceback

  from dotenv import load_dotenv

  from beeai_framework.adapters.beeai.backend.vector_store import TemporalVectorStore
  from beeai_framework.adapters.langchain.backend.vector_store import LangChainVectorStore
  from beeai_framework.agents.experimental.rag import RAGAgent
  from beeai_framework.backend.chat import ChatModel
  from beeai_framework.backend.document_loader import DocumentLoader
  from beeai_framework.backend.document_processor import DocumentProcessor
  from beeai_framework.backend.embedding import EmbeddingModel
  from beeai_framework.backend.text_splitter import TextSplitter
  from beeai_framework.backend.vector_store import VectorStore
  from beeai_framework.errors import FrameworkError
  from beeai_framework.logger import Logger
  from beeai_framework.memory import UnconstrainedMemory

  load_dotenv()  # load environment variables
  logger = Logger("rag-agent", level=logging.DEBUG)


  POPULATE_VECTOR_DB = True
  VECTOR_DB_PATH_4_DUMP = ""  # Set this path for persistency
  INPUT_DOCUMENTS_LOCATION = "docs/integrations"


  async def populate_documents() -> VectorStore | None:
      embedding_model = EmbeddingModel.from_name("watsonx:ibm/slate-125m-english-rtrvr-v2", truncate_input_tokens=500)

      # Load existing vector store if available
      if VECTOR_DB_PATH_4_DUMP and os.path.exists(VECTOR_DB_PATH_4_DUMP):
          print(f"Loading vector store from: {VECTOR_DB_PATH_4_DUMP}")
          preloaded_vector_store: VectorStore = TemporalVectorStore.load(
              path=VECTOR_DB_PATH_4_DUMP, embedding_model=embedding_model
          )
          return preloaded_vector_store

      # Create new vector store if population is enabled
      if POPULATE_VECTOR_DB:
          loader = DocumentLoader.from_name(
              name="langchain:UnstructuredMarkdownLoader", file_path="docs/modules/agents.mdx"
          )
          try:
              documents = await loader.load()
          except Exception:
              return None

          # Use abstracted text splitter
          text_splitter = TextSplitter.from_name(
              name="langchain:RecursiveCharacterTextSplitter", chunk_size=2000, chunk_overlap=1000
          )
          documents = await text_splitter.split_documents(documents)
          print(f"Loaded {len(documents)} documents")

          print("Rebuilding vector store")
          # Adapter example
          vector_store: TemporalVectorStore = VectorStore.from_name(
              name="beeai:TemporalVectorStore", embedding_model=embedding_model
          )  # type: ignore[assignment]
          # Native examples
          # vector_store: TemporalVectorStore = TemporalVectorStore(embedding_model=embedding_model)
          # vector_store = InMemoryVectorStore(embedding_model)
          _ = await vector_store.add_documents(documents=documents)
          if VECTOR_DB_PATH_4_DUMP and isinstance(vector_store, LangChainVectorStore):
              print(f"Dumping vector store to: {VECTOR_DB_PATH_4_DUMP}")
              vector_store.vector_store.dump(VECTOR_DB_PATH_4_DUMP)
          return vector_store

      # Neither existing DB found nor population enabled
      return None


  async def main() -> None:
      vector_store = await populate_documents()
      if vector_store is None:
          raise FileNotFoundError(
              f"Vector database not found at {VECTOR_DB_PATH_4_DUMP}. "
              "Either set POPULATE_VECTOR_DB=True to create a new one, or ensure the database file exists."
          )

      llm = ChatModel.from_name("ollama:llama3.2")
      reranker = DocumentProcessor.from_name("beeai:LLMDocumentReranker", llm=llm)

      agent = RAGAgent(llm=llm, memory=UnconstrainedMemory(), vector_store=vector_store, reranker=reranker)

      response = await agent.run("What agents are available in BeeAI?")
      print(response.last_message.text)


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```
</CodeGroup>

<Tip>
  For production deployments, consider implementing document caching and index optimization to improve response times.
</Tip>

### RAG as Tools

Vector store population (loading and chunking documents) is typically handled offline in production applications, making Vector Store the prominent RAG building block utilized as a tool.

`VectorStoreSearchTool` enables any agent to perform semantic search against a pre-populated vector store. This provides flexibility for agents that need retrieval capabilities alongside other functionalities.

<Tip>
  The VectorStoreSearchTool can be dynamically instantiated using `VectorStoreSearchTool.from_vector_store_name("beeai:TemporalVectorStore", embedding_model=embedding_model)`, see RAG with RequirementAgent example for the full code.
</Tip>

## Examples

<CardGroup cols={1}>
  <Card title="Python RAG Agent" icon="python" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/agents/rag_agent.py">
    Complete RAG agent implementation with document loading and processing
  </Card>
</CardGroup>

<CardGroup cols={1}>
  <Card title="RAG with RequirementAgent" icon="search" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/agents/experimental/requirement/rag.py">
    Example showing how to use VectorStoreSearchTool with RequirementAgent for RAG capabilities
  </Card>
</CardGroup>


# Serialization
Source: https://framework.beeai.dev/modules/serialization



## Overview

Serialization is a process of converting complex data structures or objects (e.g., agents, memories, or tools) into a format that can be easily stored, transmitted, and reconstructed later. Think of it as creating a blueprint of your object that can be used to rebuild it exactly as it was.

BeeAI framework provides robust serialization capabilities through its built-in `Serializer` class that enables:

* üíæ Persistence: Store agent state, memory, tools, and other components
* üîÑ Transmission: Send complex objects across network boundaries or processes
* üì¶ Snapshots: Create point-in-time captures of component state
* üîß Reconstruction: Rebuild objects from their serialized representation

<CodeGroup>
  {/* <!-- comingsoon python/examples/serialization/base.py --> */}

  ```py Python [expandable]
  Example coming soon
  ```

  {/* <!-- embedme typescript/examples/serialization/base.ts --> */}

  ```ts TypeScript [expandable]
  import { Serializer } from "beeai-framework/serializer/serializer";

  const original = new Date("2024-01-01T00:00:00.000Z");
  const serialized = await Serializer.serialize(original);
  const deserialized = await Serializer.deserialize(serialized);

  console.info(deserialized instanceof Date); // true
  console.info(original.toISOString() === deserialized.toISOString()); // true

  ```
</CodeGroup>

***

## Core Concepts

### `Serializable` Class

Most framework components implement the `Serializable` class with these key methods:

| Method                   | Purpose                                              |
| ------------------------ | ---------------------------------------------------- |
| `createSnapshot()`       | Captures the current state                           |
| `loadSnapshot(snapshot)` | Applies a snapshot to the current instance           |
| `fromSnapshot(snapshot)` | Creates a new instance from a snapshot (static)      |
| `fromSerialized(data)`   | Creates a new instance from serialized data (static) |

### Serialization Process

The serialization process involves:

1. Converting complex objects into a format that preserves their structure and data
2. Including type information to enable proper reconstruction
3. Managing references to maintain object identity across serialization boundaries
4. Handling special cases like circular references and custom types

***

## Basic Usage

### Serializing framework components

Most BeeAI components can be serialized out of the box. Here's an example using memory:

<CodeGroup>
  {/* <!-- comingsoon python/examples/serialization/memory.py --> */}

  ```py Python [expandable]
  Example coming soon
  ```

  {/* <!-- embedme typescript/examples/serialization/memory.ts --> */}

  ```ts TypeScript [expandable]
  import { TokenMemory } from "beeai-framework/memory/tokenMemory";
  import { AssistantMessage, UserMessage } from "beeai-framework/backend/message";

  const memory = new TokenMemory();
  await memory.add(new UserMessage("What is your name?"));

  const serialized = await memory.serialize();
  const deserialized = await TokenMemory.fromSerialized(serialized);

  await deserialized.add(new AssistantMessage("Bee"));

  ```
</CodeGroup>

<Tip>
  Most framework components are `Serializable`.
</Tip>

## Advanced Features

### Custom Serialization

If you want to serialize a class that the `Serializer` does not know, you may register it using one of the following options.

**1. Register External Classes**

You can register external classes with the serializer:

<CodeGroup>
  {/* <!-- comingsoon python/examples/serialization/custom_external.py --> */}

  ```py Python [expandable]
  Example coming soon
  ```

  {/* <!-- embedme typescript/examples/serialization/customExternal.ts --> */}

  ```ts TypeScript [expandable]
  import { Serializer } from "beeai-framework/serializer/serializer";

  class MyClass {
    constructor(public readonly name: string) {}
  }

  Serializer.register(MyClass, {
    // Defines how to transform a class to a plain object (snapshot)
    toPlain: (instance) => ({ name: instance.name }),
    // Defines how to transform a plain object (snapshot) a class instance
    fromPlain: (snapshot) => new MyClass(snapshot.name),

    // optional handlers to support lazy initiation (handling circular dependencies)
    createEmpty: () => new MyClass(""),
    updateInstance: (instance, update) => {
      Object.assign(instance, update);
    },
  });

  const instance = new MyClass("Bee");
  const serialized = await Serializer.serialize(instance);
  const deserialized = await Serializer.deserialize<MyClass>(serialized);

  console.info(instance);
  console.info(deserialized);

  ```
</CodeGroup>

**2. Implement the `Serializable` Interface**

For deeper integration, extend the Serializable class:

<CodeGroup>
  {/* <!-- comingsoon python/examples/serialization/custom_internal.py --> */}

  ```py Python [expandable]
  Example coming soon
  ```

  {/* <!-- embedme typescript/examples/serialization/customInternal.ts --> */}

  ```ts TypeScript [expandable]
  import { Serializable } from "beeai-framework/internals/serializable";

  class MyClass extends Serializable {
    constructor(public readonly name: string) {
      super();
    }

    static {
      // register class to the global serializer register
      this.register();
    }

    createSnapshot(): unknown {
      return {
        name: this.name,
      };
    }

    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>) {
      Object.assign(this, snapshot);
    }
  }

  const instance = new MyClass("Bee");
  const serialized = await instance.serialize();
  const deserialized = await MyClass.fromSerialized(serialized);

  console.info(instance);
  console.info(deserialized);

  ```
</CodeGroup>

<Note>
  Failure to register a class that the `Serializer` does not know will result in the `SerializerError` error. BeeAI framework avoids importing all potential classes automatically to prevent increased application size and unnecessary dependencies.
</Note>

## Context matters

<CodeGroup>
  {/* <!-- comingsoon python/examples/serialization/context.py --> */}

  ```py Python [expandable]
  Example coming soon
  ```

  {/* <!-- embedme typescript/examples/serialization/context.ts --> */}

  ```ts TypeScript [expandable]
  import { UnconstrainedMemory } from "beeai-framework/memory/unconstrainedMemory";
  import { UserMessage } from "beeai-framework/backend/message";

  // String containing serialized `UnconstrainedMemory` instance with one message in it.
  const serialized = `{"__version":"0.0.0","__root":{"__serializer":true,"__class":"Object","__ref":"18","__value":{"target":"UnconstrainedMemory","snapshot":{"__serializer":true,"__class":"Object","__ref":"17","__value":{"messages":{"__serializer":true,"__class":"Array","__ref":"1","__value":[{"__serializer":true,"__class":"SystemMessage","__ref":"2","__value":{"content":{"__serializer":true,"__class":"Array","__ref":"3","__value":[{"__serializer":true,"__class":"Object","__ref":"4","__value":{"type":"text","text":"You are a helpful assistant."}}]},"meta":{"__serializer":true,"__class":"Object","__ref":"5","__value":{"createdAt":{"__serializer":true,"__class":"Date","__ref":"6","__value":"2025-02-06T14:51:01.459Z"}}},"role":"system"}},{"__serializer":true,"__class":"UserMessage","__ref":"7","__value":{"content":{"__serializer":true,"__class":"Array","__ref":"8","__value":[{"__serializer":true,"__class":"Object","__ref":"9","__value":{"type":"text","text":"Hello!"}}]},"meta":{"__serializer":true,"__class":"Object","__ref":"10","__value":{"createdAt":{"__serializer":true,"__class":"Date","__ref":"11","__value":"2025-02-06T14:51:01.459Z"}}},"role":"user"}},{"__serializer":true,"__class":"AssistantMessage","__ref":"12","__value":{"content":{"__serializer":true,"__class":"Array","__ref":"13","__value":[{"__serializer":true,"__class":"Object","__ref":"14","__value":{"type":"text","text":"Hello, how can I help you?"}}]},"meta":{"__serializer":true,"__class":"Object","__ref":"15","__value":{"createdAt":{"__serializer":true,"__class":"Date","__ref":"16","__value":"2025-02-06T14:51:01.459Z"}}},"role":"assistant"}}]}}}}}}`;

  // If `Message` was not imported the serialization would fail because the `Message` had no chance to register itself.
  const memory = await UnconstrainedMemory.fromSerialized(serialized, {
    // this part can be omitted if all classes used in the serialized string are imported (and have `static` register block) or at least one initiated
    extraClasses: [UserMessage],
  });
  console.info(memory.messages);

  ```
</CodeGroup>

***

## Examples

<CardGroup cols={2}>
  <Card title="TypeScript" icon="js" href="https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples/serialization">
    Explore reference serialization implementations in TypeScript
  </Card>

  <Card title="Python" icon="python">
    COMING SOON: Explore reference serialization implementations in Python
  </Card>
</CardGroup>


# Serve
Source: https://framework.beeai.dev/modules/serve



The `Serve` module enables developers to expose components built with the BeeAI Framework through a server to external clients.
Out of the box, we provide implementations for protocols such as A2A and MCP, allowing you to quickly serve existing functionalities.
You can also create your own custom adapter if needed.

<Note>
  Location within the framework: [beeai\_framework/serve](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/serve).
</Note>

## Supported Providers

The following table lists the currently supported providers:

| Name                                    | Dependency                                      | Location                         |
| :-------------------------------------- | :---------------------------------------------- | :------------------------------- |
| [A2A](https://a2a-protocol.org/)        | `beeai_framework.adapters.a2a.serve`            | `beeai-platform[a2a]`            |
| [BeeAI Platform](https://beeai.dev/)    | `beeai_framework.adapters.beeai_platform.serve` | `beeai-platform[beeai-platform]` |
| [MCP](https://modelcontextprotocol.io/) | `beeai_framework.adapters.mcp.serve`            | `beeai-platform[mcp]`            |

<Note>
  For more details, see the [Integrations page](/integrations).
</Note>

## Usage

{/* <!-- embedme python/examples/serve/a2a_server.py --> */}

```python
from beeai_framework.adapters.a2a import A2AServer, A2AServerConfig
from beeai_framework.agents.experimental import RequirementAgent
from beeai_framework.backend import ChatModel
from beeai_framework.memory import UnconstrainedMemory
from beeai_framework.serve.utils import LRUMemoryManager
from beeai_framework.tools.search.duckduckgo import DuckDuckGoSearchTool
from beeai_framework.tools.weather import OpenMeteoTool


def main() -> None:
    llm = ChatModel.from_name("ollama:granite3.3:8b")
    agent = RequirementAgent(
        llm=llm,
        tools=[DuckDuckGoSearchTool(), OpenMeteoTool()],
        memory=UnconstrainedMemory(),
    )

    # Register the agent with the A2A server and run the HTTP server
    # For the ToolCallingAgent, we don't need to specify ACPAgent factory method
    # because it is already registered in the A2AServer
    # we use LRU memory manager to keep limited amount of sessions in the memory
    A2AServer(
        config=A2AServerConfig(port=9999, protocol="jsonrpc"), memory_manager=LRUMemoryManager(maxsize=100)
    ).register(agent).serve()


if __name__ == "__main__":
    main()

```

## Extending Functionality

By default, each provider supports registration of a limited set of modules (agents, tools, templates, etc.).
You can extend this functionality by registering a custom factory using `Server.register_factory` method.


# Templates
Source: https://framework.beeai.dev/modules/templates



## Overview

Templates are predefined structures used to create consistent outputs. In the context of AI applications, prompt templates provide structured guidance for language models to generate targeted responses. They include placeholders that can be filled with specific information at runtime.

The Framework implements this functionality through the `PromptTemplate` class, which uses Mustache-style syntax (via the `chevron` library) for variable substitution. The implementation adds type safety and validation using Pydantic or Zod schemas.

At its core, the `PromptTemplate` class:

* Validates input data against a Pydantic model schema
* Handles template variable substitution
* Supports dynamic content generation through callable functions
* Provides default values for optional fields
* Enables template customization through forking

<Tip>
  Prompt Templates are fundamental building blocks in the framework and are extensively used in agent implementations.
</Tip>

<Note>
  Supported in Python and TypeScript.
</Note>

## Basic usage

### Simple template

Create templates with basic variable substitution and type validation.

<CodeGroup>
  {/* <!-- embedme python/examples/templates/basic_template.py --> */}

  ```py Python [expandable]
  import sys
  import traceback

  from pydantic import BaseModel

  from beeai_framework.errors import FrameworkError
  from beeai_framework.template import PromptTemplate


  def main() -> None:
      class UserMessage(BaseModel):
          label: str
          input: str

      template: PromptTemplate[UserMessage] = PromptTemplate(
          schema=UserMessage,
          template="""{{label}}: {{input}}""",
      )

      prompt = template.render(label="Query", input="What interesting things happened on this day in history?")

      print(prompt)


  if __name__ == "__main__":
      try:
          main()
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/templates/primitives.ts --> */}

  ```ts TypeScript [expandable]
  import { PromptTemplate } from "beeai-framework/template";
  import { z } from "zod";

  const greetTemplate = new PromptTemplate({
    template: `Hello {{name}}`,
    schema: z.object({
      name: z.string(),
    }),
  });

  const output = greetTemplate.render({
    name: "Alex",
  });
  console.log(output); // Hello Alex!

  ```
</CodeGroup>

This example creates a simple template that formats a user message with a label and input text. The Pydantic model or Zod schema ensures type safety for the template variables.

### Template functions

Add dynamic content to templates using custom functions.

<CodeGroup>
  {/* <!-- embedme python/examples/templates/functions.py --> */}

  ```py Python [expandable]
  import sys
  import traceback
  from datetime import UTC, datetime
  from typing import Any

  from pydantic import BaseModel

  from beeai_framework.errors import FrameworkError
  from beeai_framework.template import PromptTemplate


  def main() -> None:
      class AuthorMessage(BaseModel):
          text: str
          author: str | None = None
          created_at: str | None = None

      def format_meta(data: dict[str, Any]) -> str:
          if data.get("author") is None and data.get("created_at") is None:
              return ""

          author = data.get("author") or "anonymous"
          created_at = data.get("created_at") or datetime.now(UTC).strftime("%A, %B %d, %Y at %I:%M:%S %p")

          return f"\nThis message was created at {created_at} by {author}."

      template: PromptTemplate[AuthorMessage] = PromptTemplate(
          schema=AuthorMessage,
          functions={
              "format_meta": lambda data: format_meta(data),
          },
          template="""Message: {{text}}{{format_meta}}""",
      )

      # Message: Hello from 2024!
      # This message was created at 2024-01-01T00:00:00+00:00 by John.
      message = template.render(
          text="Hello from 2024!", author="John", created_at=datetime(2024, 1, 1, tzinfo=UTC).isoformat()
      )
      print(message)

      # Message: Hello from the present!
      message = template.render(text="Hello from the present!")
      print(message)


  if __name__ == "__main__":
      try:
          main()
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/templates/functions.ts --> */}

  ```ts TypeScript [expandable]
  import { PromptTemplate } from "beeai-framework/template";
  import { z } from "zod";

  const messageTemplate = new PromptTemplate({
    schema: z
      .object({
        text: z.string(),
        author: z.string().optional(),
        createdAt: z.string().datetime().optional(),
      })
      .passthrough(),
    functions: {
      formatMeta: function () {
        if (!this.author && !this.createdAt) {
          return "";
        }

        const author = this.author || "anonymous";
        const createdAt = this.createdAt || new Date().toISOString();

        return `\nThis message was created at ${createdAt} by ${author}.`;
      },
    },
    template: `Message: {{text}}{{formatMeta}}`,
  });

  // Message: Hello from 2024!
  // This message was created at 2024-01-01T00:00:00.000Z by John.
  console.log(
    messageTemplate.render({
      text: "Hello from 2024!",
      author: "John",
      createdAt: new Date("2024-01-01").toISOString(),
    }),
  );

  // Message: Hello from the present!
  console.log(
    messageTemplate.render({
      text: "Hello from the present!",
    }),
  );

  ```
</CodeGroup>

This example demonstrates how to add custom functions to templates:

* The `format_meta` function returns the date and author in a readable string
* Functions can be called directly from the template using Mustache-style syntax

### Working with objects

Handle complex nested data structures in templates with proper type validation.

<CodeGroup>
  {/* <!-- embedme python/examples/templates/objects.py --> */}

  ```py Python [expandable]
  import sys
  import traceback

  from pydantic import BaseModel

  from beeai_framework.errors import FrameworkError
  from beeai_framework.template import PromptTemplate


  def main() -> None:
      class Response(BaseModel):
          duration: int

      class ExpectedDuration(BaseModel):
          expected: int
          responses: list[Response]

      template: PromptTemplate[ExpectedDuration] = PromptTemplate(
          schema=ExpectedDuration,
          template="""Expected Duration: {{expected}}ms; Retrieved: {{#responses}}{{duration}}ms {{/responses}}""",
          defaults={"expected": 5},
      )

      # Expected Duration: 5ms; Retrieved: 3ms 5ms 6ms
      output = template.render(responses=[Response(duration=3), Response(duration=5), Response(duration=6)])
      print(output)


  if __name__ == "__main__":
      try:
          main()
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/templates/objects.ts --> */}

  ```ts TypeScript [expandable]
  import { PromptTemplate } from "beeai-framework/template";
  import { z } from "zod";

  const template = new PromptTemplate({
    template: `Expected Duration: {{expected}}ms; Retrieved: {{#responses}}{{duration}}ms {{/responses}}`,
    schema: z.object({
      expected: z.number(),
      responses: z.array(z.object({ duration: z.number() })),
    }),
    defaults: {
      expected: 5,
    },
  });

  const output = template.render({
    expected: undefined, // default value will be used
    responses: [{ duration: 3 }, { duration: 5 }, { duration: 6 }],
  });
  console.log(output); // Expected Duration: 5ms; Retrieved: 3ms 5ms 6ms

  ```
</CodeGroup>

This example shows how to work with nested objects in templates. The Mustache syntax allows for iterating through the responses array and accessing properties of each object.

### Working with arrays

Process collections of data within templates for dynamic list generation.

<CodeGroup>
  {/* <!-- embedme python/examples/templates/arrays.py --> */}

  ```py Python [expandable]
  import sys
  import traceback

  from pydantic import BaseModel, Field

  from beeai_framework.errors import FrameworkError
  from beeai_framework.template import PromptTemplate


  def main() -> None:
      class ColorsObject(BaseModel):
          colors: list[str] = Field(..., min_length=1)

      template: PromptTemplate[ColorsObject] = PromptTemplate(
          schema=ColorsObject,
          template="""Colors: {{#colors}}{{.}}, {{/colors}}""",
      )

      # Colors: Green, Yellow,
      output = template.render(colors=["Green", "Yellow"])
      print(output)


  if __name__ == "__main__":
      try:
          main()
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/templates/arrays.ts --> */}

  ```ts TypeScript [expandable]
  import { PromptTemplate } from "beeai-framework/template";
  import { z } from "zod";

  const template = new PromptTemplate({
    schema: z.object({
      colors: z.array(z.string()).min(1),
    }),
    template: `Colors: {{#trim}}{{#colors}}{{.}},{{/colors}}{{/trim}}`,
  });

  const output = template.render({
    colors: ["Green", "Yellow"],
  });
  console.log(output); // Colors: Green,Yellow

  ```
</CodeGroup>

This example demonstrates how to iterate over arrays in templates using Mustache's section syntax.

*Source: [python/examples/templates/arrays.py](https://github.com/i-am-bee/beeai-framework/blob/main/python/examples/templates/arrays.py)*

### Template forking

The fork() method allows you to create new templates based on existing ones, with customizations.

Template forking is useful for:

* Creating variations of templates while maintaining core functionality
* Adding new fields or functionality to existing templates
* Specializing generic templates for specific use cases

{/* <!-- embedme python/examples/templates/forking.py --> */}

```py Python [expandable]
import sys
import traceback
from typing import Any

from pydantic import BaseModel

from beeai_framework.errors import FrameworkError
from beeai_framework.template import PromptTemplate, PromptTemplateInput


def main() -> None:
    class OriginalSchema(BaseModel):
        name: str
        objective: str

    original: PromptTemplate[OriginalSchema] = PromptTemplate(
        schema=OriginalSchema,
        template="""You are a helpful assistant called {{name}}. Your objective is to {{objective}}.""",
    )

    def customizer(temp_input: PromptTemplateInput[Any]) -> PromptTemplateInput[Any]:
        new_temp = temp_input.model_copy()
        new_temp.template = f"""{temp_input.template} Your answers must be concise."""
        new_temp.defaults["name"] = "Bee"
        return new_temp

    modified = original.fork(customizer=customizer)

    # You are a helpful assistant called Bee. Your objective is to fulfill the user needs. Your answers must be concise.
    prompt = modified.render(objective="fulfill the user needs")
    print(prompt)


if __name__ == "__main__":
    try:
        main()
    except FrameworkError as e:
        traceback.print_exc()
        sys.exit(e.explain())

```

This example shows how to create a new template based on an existing one.

*Source: [python/examples/templates/forking.py](https://github.com/i-am-bee/beeai-framework/blob/main/python/examples/templates/forking.py)*

### Default values

Provide default values for template variables that can be overridden at runtime.

***

## Using templates with agents

The framework's agents use specialized templates to structure their behavior. You can customize these templates to alter how agents operate:

{/* <!-- embedme python/examples/templates/system_prompt.py --> */}

```py Python [expandable]
import sys
import traceback

from beeai_framework.agents.react.runners.default.prompts import (
    SystemPromptTemplate,
    ToolDefinition,
)
from beeai_framework.errors import FrameworkError
from beeai_framework.tools.weather import OpenMeteoTool
from beeai_framework.utils.strings import to_json


def main() -> None:
    tool = OpenMeteoTool()

    tool_def = ToolDefinition(
        name=tool.name,
        description=tool.description,
        input_schema=to_json(tool.input_schema.model_json_schema()),
    )

    # Render the granite system prompt
    prompt = SystemPromptTemplate.render(
        instructions="You are a helpful AI assistant!", tools=[tool_def], tools_length=1
    )

    print(prompt)


if __name__ == "__main__":
    try:
        main()
    except FrameworkError as e:
        traceback.print_exc()
        sys.exit(e.explain())

```

This example demonstrates how to create a system prompt for an agent with tool definitions, which enables the agent to use external tools like weather data retrieval.

*Source: [python/examples/templates/system\_prompt.py](https://github.com/i-am-bee/beeai-framework/blob/main/python/examples/templates/system_prompt.py)*

***

## Examples

<CardGroup cols={2}>
  <Card title="Python" icon="python" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/templates">
    Explore reference template implementations in Python
  </Card>

  <Card title="TypeScript" icon="js" href="https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples/templates">
    Explore reference template implementations in TypeScript
  </Card>
</CardGroup>


# Tools
Source: https://framework.beeai.dev/modules/tools



## Overview

Tools extend agent capabilities beyond text processing, enabling interaction with external systems and data sources. They act as specialized modules that extend the agent's abilities, allowing it to interact with external systems, access information, and execute actions in response to user queries.

<Note>
  Supported in Python and TypeScript.
</Note>

## Built-in tools

Ready-to-use tools that provide immediate functionality for common agent tasks:

| Tool                    | Description                                                                                        |
| :---------------------- | :------------------------------------------------------------------------------------------------- |
| `DuckDuckGoTool`        | Search for data on DuckDuckGo                                                                      |
| `OpenMeteoTool`         | Retrieve weather information for specific locations and dates                                      |
| `WikipediaTool`         | Search for data on Wikipedia                                                                       |
| `VectorStoreSearchTool` | Search for relevant documents in a vector store using semantic similarity                          |
| `MCPTool`               | Discover and use tools exposed by arbitrary [MCP Server](https://modelcontextprotocol.io/examples) |
| `PythonTool`            | Run arbitrary Python code in a sandboxed environment                                               |
| `SandboxTool`           | Run custom Python functions in a sandboxed environment                                             |
| `ThinkTool`             | Gives an agent a place to think.                                                                   |
| `HandoffTool`           | Delegates a task to an expert agent                                                                |

‚ûï [Request additional built-in tools](https://github.com/i-am-bee/beeai-framework/discussions)

<Tip>
  Would you like to use a tool from LangChain? See the LangChain tool example in [Python](https://github.com/i-am-bee/beeai-framework/blob/main/python/examples/tools/langchain_tool.py).
  or [TypeScript](https://github.com/i-am-bee/beeai-framework/blob/main/typescript/examples/tools/langchain.ts).
</Tip>

## Usage

### Basic usage

The simplest way to use a tool is to instantiate it directly and call its `run()` method with appropriate input:

<CodeGroup>
  {/* <!-- embedme python/examples/tools/base.py --> */}

  ```py Python [expandable]
  import asyncio
  import datetime
  import sys

  from beeai_framework.errors import FrameworkError
  from beeai_framework.middleware.trajectory import GlobalTrajectoryMiddleware
  from beeai_framework.tools.weather import OpenMeteoTool, OpenMeteoToolInput


  async def main() -> None:
      tool = OpenMeteoTool()
      result = await tool.run(
          input=OpenMeteoToolInput(
              location_name="New York", start_date=datetime.date.today(), end_date=datetime.date.today()
          )
      ).middleware(GlobalTrajectoryMiddleware())
      print(result.get_text_content())


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/tools/base.ts --> */}

  ```ts TypeScript [expandable]
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";

  const tool = new OpenMeteoTool();

  const today = new Date().toISOString().split("T")[0];
  const result = await tool.run({
    location: { name: "New York" },
    start_date: today,
    end_date: today,
  });
  console.log(result.getTextContent());

  ```
</CodeGroup>

### Advanced usage

Tools often support additional configuration options to customize their behavior:

<CodeGroup>
  {/* <!-- embedme python/examples/tools/advanced.py --> */}

  ```py Python [expandable]
  import asyncio
  import datetime
  import sys
  import traceback

  from beeai_framework.errors import FrameworkError
  from beeai_framework.tools.weather import OpenMeteoTool, OpenMeteoToolInput


  async def main() -> None:
      tool = OpenMeteoTool()
      result = await tool.run(
          input=OpenMeteoToolInput(
              location_name="New York",
              start_date=datetime.date.today(),
              end_date=datetime.date.today(),
              temperature_unit="celsius",
          )
      )
      print(result.get_text_content())


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/tools/advanced.ts --> */}

  ```ts TypeScript [expandable]
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";
  import { UnconstrainedCache } from "beeai-framework/cache/unconstrainedCache";

  const tool = new OpenMeteoTool({
    cache: new UnconstrainedCache(),
    retryOptions: {
      maxRetries: 3,
    },
  });
  console.log(tool.name); // OpenMeteo
  console.log(tool.description); // Retrieve current, past, or future weather forecasts for a location.
  console.log(tool.inputSchema()); // (zod/json schema)

  await tool.cache.clear();

  const today = new Date().toISOString().split("T")[0];
  const result = await tool.run({
    location: { name: "New York" },
    start_date: today,
    end_date: today,
    temperature_unit: "celsius",
  });
  console.log(result.isEmpty()); // false
  console.log(result.result); // prints raw data
  console.log(result.getTextContent()); // prints data as text

  ```
</CodeGroup>

### Using tools with agents

The true power of tools emerges when integrating them with agents. Tools extend the agent's capabilities, allowing it to perform actions beyond text generation:

<CodeGroup>
  {/* <!-- embedme python/examples/tools/agent.py --> */}

  ```py Python [expandable]
  from beeai_framework.adapters.ollama import OllamaChatModel
  from beeai_framework.agents.react import ReActAgent
  from beeai_framework.memory import UnconstrainedMemory
  from beeai_framework.tools.weather import OpenMeteoTool

  agent = ReActAgent(llm=OllamaChatModel("llama3.1"), tools=[OpenMeteoTool()], memory=UnconstrainedMemory())

  ```

  {/* <!-- embedme typescript/examples/tools/agent.ts --> */}

  ```ts TypeScript [expandable]
  import { ArXivTool } from "beeai-framework/tools/arxiv";
  import { ReActAgent } from "beeai-framework/agents/react/agent";
  import { UnconstrainedMemory } from "beeai-framework/memory/unconstrainedMemory";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  const agent = new ReActAgent({
    llm: new OllamaChatModel("llama3.1"),
    memory: new UnconstrainedMemory(),
    tools: [new ArXivTool()],
  });

  ```
</CodeGroup>

### Using the tool decorator in Python

For simpler tools, you can use the `tool` decorator to quickly create a tool from a function:

{/* <!-- embedme python/examples/tools/decorator.py --> */}

```py Python [expandable]
import asyncio
import json
import sys
import traceback
from urllib.parse import quote

import requests

from beeai_framework.agents.react import ReActAgent
from beeai_framework.backend import ChatModel
from beeai_framework.errors import FrameworkError
from beeai_framework.logger import Logger
from beeai_framework.memory import UnconstrainedMemory
from beeai_framework.tools import StringToolOutput, tool

logger = Logger(__name__)


# defining a tool using the `tool` decorator
@tool
def basic_calculator(expression: str) -> StringToolOutput:
    """
    A calculator tool that performs mathematical operations.

    Args:
        expression: The mathematical expression to evaluate (e.g., "2 + 3 * 4").

    Returns:
        The result of the mathematical expression
    """
    try:
        encoded_expression = quote(expression)
        math_url = f"https://newton.vercel.app/api/v2/simplify/{encoded_expression}"

        response = requests.get(
            math_url,
            headers={"Accept": "application/json"},
        )
        response.raise_for_status()

        return StringToolOutput(json.dumps(response.json()))
    except Exception as e:
        raise RuntimeError(f"Error evaluating expression: {e!s}") from Exception


async def main() -> None:
    # using the tool in an agent

    chat_model = ChatModel.from_name("ollama:granite3.3:8b")

    agent = ReActAgent(llm=chat_model, tools=[basic_calculator], memory=UnconstrainedMemory())

    result = await agent.run("What is the square root of 36?", total_max_retries=10)

    print(result.last_message.text)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except FrameworkError as e:
        traceback.print_exc()
        sys.exit(e.explain())

```

## Built-in tool examples

### DuckDuckGo search tool

Use the DuckDuckGo search tool to retrieve real-time search results from across the internet, including news, current events, or content from specific websites or domains.

<CodeGroup>
  {/* <!-- embedme python/examples/tools/duckduckgo.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.agents.react import ReActAgent
  from beeai_framework.backend import ChatModel
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import UnconstrainedMemory
  from beeai_framework.tools.search.duckduckgo import DuckDuckGoSearchTool


  async def main() -> None:
      chat_model = ChatModel.from_name("ollama:granite3.3:8b")
      agent = ReActAgent(llm=chat_model, tools=[DuckDuckGoSearchTool()], memory=UnconstrainedMemory())

      result = await agent.run("How tall is the mount Everest?")

      print(result.last_message.text)


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/tools/custom/extending.ts --> */}

  ```ts TypeScript [expandable]
  import { z } from "zod";
  import {
    DuckDuckGoSearchTool,
    DuckDuckGoSearchToolSearchType as SafeSearchType,
  } from "beeai-framework/tools/search/duckDuckGoSearch";

  const searchTool = new DuckDuckGoSearchTool();

  const customSearchTool = searchTool.extend(
    z.object({
      query: z.string(),
      safeSearch: z.boolean().default(true),
    }),
    (input, options) => {
      if (!options.search) {
        options.search = {};
      }
      options.search.safeSearch = input.safeSearch ? SafeSearchType.STRICT : SafeSearchType.OFF;

      return { query: input.query };
    },
  );

  const response = await customSearchTool.run(
    {
      query: "News in the world!",
      safeSearch: true,
    },
    {
      signal: AbortSignal.timeout(10_000),
    },
  );
  console.info(response);

  ```
</CodeGroup>

### OpenMeteo weather tool

Use the OpenMeteo tool to retrieve real-time weather forecasts including detailed information on temperature, wind speed, and precipitation. Access forecasts predicting weather up to 16 days in the future and archived forecasts for weather up to 30 days in the past. Ideal for obtaining up-to-date weather predictions and recent historical weather trends.

<CodeGroup>
  {/* <!-- embedme python/examples/tools/openmeteo.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.agents.react import ReActAgent
  from beeai_framework.backend import ChatModel
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import UnconstrainedMemory
  from beeai_framework.tools.weather import OpenMeteoTool


  async def main() -> None:
      llm = ChatModel.from_name("ollama:granite3.3:8b")
      agent = ReActAgent(llm=llm, tools=[OpenMeteoTool()], memory=UnconstrainedMemory())

      result = await agent.run("What's the current weather in London?")

      print(result.last_message.text)


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/tools/base.ts --> */}

  ```ts TypeScript [expandable]
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";

  const tool = new OpenMeteoTool();

  const today = new Date().toISOString().split("T")[0];
  const result = await tool.run({
    location: { name: "New York" },
    start_date: today,
    end_date: today,
  });
  console.log(result.getTextContent());

  ```
</CodeGroup>

### Wikipedia tool

Use the Wikipedia tool to retrieve detailed information from Wikipedia.org on a wide range of topics, including famous individuals, locations, organizations, and historical events. Ideal for obtaining comprehensive overviews or specific details on well-documented subjects. May not be suitable for lesser-known or more recent topics. The information is subject to community edits which can be inaccurate.

<CodeGroup>
  {/* <!-- embedme python/examples/tools/wikipedia.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.errors import FrameworkError
  from beeai_framework.tools.search.wikipedia import (
      WikipediaTool,
      WikipediaToolInput,
  )


  async def main() -> None:
      wikipedia_client = WikipediaTool({"full_text": True})
      tool_input = WikipediaToolInput(query="bee")
      result = await wikipedia_client.run(tool_input)
      print(result.get_text_content())


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/tools/custom/piping.ts --> */}

  ```ts TypeScript [expandable]
  import { WikipediaTool } from "beeai-framework/tools/search/wikipedia";
  import { SimilarityTool } from "beeai-framework/tools/similarity";
  import { splitString } from "beeai-framework/internals/helpers/string";
  import { z } from "zod";

  const wikipedia = new WikipediaTool();
  const similarity = new SimilarityTool({
    maxResults: 5,
    provider: async (input) =>
      input.documents.map((document) => ({
        score: document.text
          .toLowerCase()
          .split(" ")
          .reduce((acc, word) => acc + (input.query.toLowerCase().includes(word) ? 1 : 0), 0),
      })),
  });

  const wikipediaWithSimilarity = wikipedia
    .extend(
      z.object({
        page: z.string().describe("Wikipedia page"),
        query: z.string().describe("Search query"),
      }),
      (newInput) => ({ query: newInput.page }),
    )
    .pipe(similarity, (input, output) => ({
      query: input.query,
      documents: output.results.flatMap((document) =>
        Array.from(splitString(document.fields.markdown ?? "", { size: 1000, overlap: 50 })).map(
          (chunk) => ({
            text: chunk,
            source: document,
          }),
        ),
      ),
    }));

  const response = await wikipediaWithSimilarity.run({
    page: "JavaScript",
    query: "engine",
  });
  console.info(response);

  ```
</CodeGroup>

### Vector store search tool

Use the VectorStoreSearchTool to perform semantic search against pre-populated vector stores. This tool enables agents to retrieve relevant documents from knowledge bases using semantic similarity, making it ideal for RAG (Retrieval-Augmented Generation) applications and knowledge-based question answering.

<Note>
  This tool requires a pre-populated vector store. Vector store population (loading and chunking documents) is typically handled offline in production applications.
</Note>

{/* <!-- embedme python/examples/agents/experimental/requirement/rag.py --> */}

```py Python [expandable]
import asyncio
import os

from beeai_framework.agents.experimental import RequirementAgent
from beeai_framework.backend import ChatModel
from beeai_framework.backend.document_loader import DocumentLoader
from beeai_framework.backend.embedding import EmbeddingModel
from beeai_framework.backend.text_splitter import TextSplitter
from beeai_framework.backend.vector_store import VectorStore
from beeai_framework.memory import UnconstrainedMemory
from beeai_framework.middleware.trajectory import GlobalTrajectoryMiddleware
from beeai_framework.tools import Tool
from beeai_framework.tools.search.retrieval import VectorStoreSearchTool

POPULATE_VECTOR_DB = True
VECTOR_DB_PATH_4_DUMP = ""  # Set this path for persistency


async def setup_vector_store() -> VectorStore | None:
    """
    Setup vector store with BeeAI framework documentation.
    """
    embedding_model = EmbeddingModel.from_name("watsonx:ibm/slate-125m-english-rtrvr-v2", truncate_input_tokens=500)

    # Load existing vector store if available
    if VECTOR_DB_PATH_4_DUMP and os.path.exists(VECTOR_DB_PATH_4_DUMP):
        print(f"Loading vector store from: {VECTOR_DB_PATH_4_DUMP}")
        from beeai_framework.adapters.beeai.backend.vector_store import TemporalVectorStore

        preloaded_vector_store: VectorStore = TemporalVectorStore.load(
            path=VECTOR_DB_PATH_4_DUMP, embedding_model=embedding_model
        )
        return preloaded_vector_store

    # Create new vector store if population is enabled
    # NOTE: Vector store population is typically done offline in production applications
    if POPULATE_VECTOR_DB:
        # Load documentation about BeeAI agents - this serves as our knowledge base
        # for answering questions about the different types of agents available
        loader = DocumentLoader.from_name(
            name="langchain:UnstructuredMarkdownLoader", file_path="docs/modules/agents.mdx"
        )
        try:
            documents = await loader.load()
        except Exception as e:
            print(f"Failed to load documents: {e}")
            return None

        # Split documents into chunks
        text_splitter = TextSplitter.from_name(
            name="langchain:RecursiveCharacterTextSplitter", chunk_size=1000, chunk_overlap=200
        )
        documents = await text_splitter.split_documents(documents)
        print(f"Loaded {len(documents)} document chunks")

        # Create vector store and add documents
        vector_store = VectorStore.from_name(name="beeai:TemporalVectorStore", embedding_model=embedding_model)
        await vector_store.add_documents(documents=documents)
        print("Vector store populated with documents")

        return vector_store

    return None


async def main() -> None:
    """
    Example demonstrating RequirementAgent using VectorStoreSearchTool.

    The agent will use the vector store search tool to find relevant information
    about BeeAI framework agents and provide comprehensive answers.

    Note: In typical applications, you would use a pre-populated vector store
    rather than populating it at runtime. This example includes population
    logic for demonstration purposes only.
    """
    # Setup vector store with BeeAI documentation
    vector_store = await setup_vector_store()
    if vector_store is None:
        raise FileNotFoundError(
            "Failed to instantiate Vector Store. "
            "Either set POPULATE_VECTOR_DB=True to create a new one, or ensure the database file exists."
        )

    # Create the vector store search tool
    search_tool = VectorStoreSearchTool(vector_store=vector_store)

    # Alternative: Create search tool using dynamic loading
    # embedding_model = EmbeddingModel.from_name("watsonx:ibm/slate-125m-english-rtrvr-v2", truncate_input_tokens=500)
    # search_tool = VectorStoreSearchTool.from_name(
    #     name="beeai:TemporalVectorStore",
    #     embedding_model=embedding_model
    # )

    # Create RequirementAgent with the vector store search tool
    llm = ChatModel.from_name("ollama:llama3.1:8b")
    agent = RequirementAgent(
        llm=llm,
        memory=UnconstrainedMemory(),
        instructions=(
            "You are a helpful assistant that answers questions about the BeeAI framework. "
            "Use the vector store search tool to find relevant information from the documentation "
            "before providing your answer. Always search for information first, then provide a "
            "comprehensive response based on what you found."
        ),
        tools=[search_tool],
        # Log all tool calls to the console for easier debugging
        middlewares=[GlobalTrajectoryMiddleware(included=[Tool])],
    )

    query = "What types of agents are available in BeeAI?"
    response = await agent.run(query)
    print(f"query: {query}\nResponse: {response.last_message.text}")


if __name__ == "__main__":
    asyncio.run(main())

```

### MCP tool

Leverage the Model Context Protocol (MCP) to define, initialize, and utilize tools on compatible MCP servers. These servers expose executable functionalities, enabling AI models to perform tasks such as computations, API calls, or system operations.

<Tip>
  Check out the [MCP Slack integration tutorial](/guides/mcp-slackbot)
</Tip>

<CodeGroup>
  {/* <!-- embedme python/examples/tools/mcp_tool_creation.py --> */}

  ```py Python [expandable]
  import asyncio
  import os

  from dotenv import load_dotenv
  from mcp import StdioServerParameters
  from mcp.client.stdio import stdio_client

  from beeai_framework.tools.mcp import MCPTool

  load_dotenv()

  # Create server parameters for stdio connection
  server_params = StdioServerParameters(
      command="npx",
      args=["-y", "@modelcontextprotocol/server-slack"],
      env={
          "SLACK_BOT_TOKEN": os.environ["SLACK_BOT_TOKEN"],
          "SLACK_TEAM_ID": os.environ["SLACK_TEAM_ID"],
          "PATH": os.getenv("PATH", default=""),
      },
  )


  async def slack_post_message_tool() -> MCPTool:
      slack_tools = await MCPTool.from_client(stdio_client(server_params))
      return next(filter(lambda tool: tool.name == "slack_post_message", slack_tools))


  async def main() -> None:
      tool = await slack_post_message_tool()
      print(tool.name, tool.description)


  if __name__ == "__main__":
      asyncio.run(main())

  ```

  {/* <!-- embedme typescript/examples/tools/mcp.ts --> */}

  ```ts TypeScript [expandable]
  import { Client } from "@modelcontextprotocol/sdk/client/index.js";
  import { MCPTool } from "beeai-framework/tools/mcp";
  import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";
  import { ReActAgent } from "beeai-framework/agents/react/agent";
  import { UnconstrainedMemory } from "beeai-framework/memory/unconstrainedMemory";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  // Create MCP Client
  const client = new Client(
    {
      name: "test-client",
      version: "1.0.0",
    },
    {
      capabilities: {},
    },
  );

  // Connect the client to any MCP server with tools capablity
  await client.connect(
    new StdioClientTransport({
      command: "npx",
      args: ["-y", "@modelcontextprotocol/server-everything"],
    }),
  );

  try {
    // Server usually supports several tools, use the factory for automatic discovery
    const tools = await MCPTool.fromClient(client);
    const agent = new ReActAgent({
      llm: new OllamaChatModel("llama3.1"),
      memory: new UnconstrainedMemory(),
      tools,
    });
    // @modelcontextprotocol/server-everything contains "add" tool
    await agent.run({ prompt: "Find out how much is 4 + 7" }).observe((emitter) => {
      emitter.on("update", async ({ data, update, meta }) => {
        console.log(`Agent (${update.key}) ü§ñ : `, update.value);
      });
    });
  } finally {
    // Close the MCP connection
    await client.close();
  }

  ```
</CodeGroup>

### Python tool

The Python tool allows AI agents to execute Python code within a secure, sandboxed environment. This tool enables access to files that are either provided by the user or created during execution.

This enables agents to:

* Perform calculations and data analysis
* Create and modify files
* Process and transform user data
* Generate visualizations and reports
* And more

<Note>
  This tool requires [beeai-code-interpreter](https://github.com/i-am-bee/bee-code-interpreter) to use.
  Get started quickly with the BeeAI Framework starter template for [Python](https://github.com/i-am-bee/beeai-framework-py-starter)
  or [TypeScript](https://github.com/i-am-bee/beeai-framework-py-starter).
</Note>

Key components:

* `LocalPythonStorage` ‚Äì Handles where Python code is stored and run.
  * `local_working_dir` ‚Äì A temporary folder where the code is saved before running.
  * `interpreter_working_dir` ‚Äì The folder where the code actually runs, set by the `CODE_INTERPRETER_TMPDIR` setting.
* `PythonTool` ‚Äì Connects to an external Python interpreter to run code.
  * `code_interpreter_url` ‚Äì The web address where the code gets executed (default: `http://127.0.0.1:50081`).
  * `storage` -- Controls where the code is stored. By default, it saves files locally using `LocalPythonStorage`. You can set up a different storage option, like cloud storage, if needed.

<CodeGroup>
  {/* <!-- embedme python/examples/tools/python_tool.py --> */}

  ```py Python [expandable]
  import asyncio
  import os
  import sys
  import tempfile
  import traceback

  from dotenv import load_dotenv

  from beeai_framework.adapters.ollama import OllamaChatModel
  from beeai_framework.agents.react import ReActAgent
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import UnconstrainedMemory
  from beeai_framework.tools.code import LocalPythonStorage, PythonTool

  # Load environment variables
  load_dotenv()


  async def main() -> None:
      llm = OllamaChatModel("llama3.1")
      storage = LocalPythonStorage(
          local_working_dir=tempfile.mkdtemp("code_interpreter_source"),
          # CODE_INTERPRETER_TMPDIR should point to where code interpreter stores it's files
          interpreter_working_dir=os.getenv("CODE_INTERPRETER_TMPDIR", "./tmp/code_interpreter_target"),
      )
      python_tool = PythonTool(
          code_interpreter_url=os.getenv("CODE_INTERPRETER_URL", "http://127.0.0.1:50081"),
          storage=storage,
      )
      agent = ReActAgent(llm=llm, tools=[python_tool], memory=UnconstrainedMemory())
      result = await agent.run("Calculate 5036 * 12856 and save the result to answer.txt").on(
          "update", lambda data, event: print(f"Agent ü§ñ ({data.update.key}) : ", data.update.parsed_value)
      )
      print(result.last_message.text)

      result = await agent.run("Read the content of answer.txt?").on(
          "update", lambda data, event: print(f"Agent ü§ñ ({data.update.key}) : ", data.update.parsed_value)
      )
      print(result.last_message.text)


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/tools/custom/python.ts --> */}

  ```ts TypeScript [expandable]
  import "dotenv/config";
  import { CustomTool } from "beeai-framework/tools/custom";

  const customTool = await CustomTool.fromSourceCode(
    {
      // Ensure the env exists
      url: process.env.CODE_INTERPRETER_URL!,
      env: { API_URL: "https://riddles-api.vercel.app/random" },
    },
    `import requests
  import os
  from typing import Optional, Union, Dict

  def get_riddle() -> Optional[Dict[str, str]]:
    """
    Fetches a random riddle from the Riddles API.

    This function retrieves a random riddle and its answer. It does not accept any input parameters.

    Returns:
        Optional[Dict[str, str]]: A dictionary containing:
            - 'riddle' (str): The riddle question.
            - 'answer' (str): The answer to the riddle.
        Returns None if the request fails.
    """
    url = os.environ.get('API_URL')
    
    try:
        response = requests.get(url)
        response.raise_for_status() 
        return response.json() 
    except Exception as e:
        return None`,
  );

  ```
</CodeGroup>

### Sandbox tool

The Sandbox tool provides a way to define and run custom Python functions in a secure, sandboxed environment. It's ideal when you need to encapsulate specific functionality that can be called by the agent.

<Note>
  This tool requires [beeai-code-interpreter](https://github.com/i-am-bee/bee-code-interpreter) to use.
  Get started quickly with [beeai-framework-py-starter](https://github.com/i-am-bee/beeai-framework-py-starter).
</Note>

<CodeGroup>
  {/* <!-- embedme python/examples/tools/custom/sandbox.py --> */}

  ```py Python [expandable]
  import asyncio
  import os
  import sys
  import traceback

  from dotenv import load_dotenv

  from beeai_framework.errors import FrameworkError
  from beeai_framework.tools.code import SandboxTool

  load_dotenv()


  async def main() -> None:
      sandbox_tool = await SandboxTool.from_source_code(
          url=os.getenv("CODE_INTERPRETER_URL", "http://127.0.0.1:50081"),
          env={"API_URL": "https://riddles-api.vercel.app/random"},
          source_code="""
  import requests
  import os
  from typing import Optional, Union, Dict

  def get_riddle() -> Optional[Dict[str, str]]:
      '''
      Fetches a random riddle from the Riddles API.

      This function retrieves a random riddle and its answer. It does not accept any input parameters.

      Returns:
          Optional[Dict[str, str]]: A dictionary containing:
              - 'riddle' (str): The riddle question.
              - 'answer' (str): The answer to the riddle.
          Returns None if the request fails.
      '''
      url = os.environ.get('API_URL')

      try:
          response = requests.get(url)
          response.raise_for_status()
          return response.json()
      except Exception as e:
          return None
  """,
      )

      result = await sandbox_tool.run({})

      print(result.get_text_content())


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/tools/custom/python.ts --> */}

  ```ts TypeScript [expandable]
  import "dotenv/config";
  import { CustomTool } from "beeai-framework/tools/custom";

  const customTool = await CustomTool.fromSourceCode(
    {
      // Ensure the env exists
      url: process.env.CODE_INTERPRETER_URL!,
      env: { API_URL: "https://riddles-api.vercel.app/random" },
    },
    `import requests
  import os
  from typing import Optional, Union, Dict

  def get_riddle() -> Optional[Dict[str, str]]:
    """
    Fetches a random riddle from the Riddles API.

    This function retrieves a random riddle and its answer. It does not accept any input parameters.

    Returns:
        Optional[Dict[str, str]]: A dictionary containing:
            - 'riddle' (str): The riddle question.
            - 'answer' (str): The answer to the riddle.
        Returns None if the request fails.
    """
    url = os.environ.get('API_URL')
    
    try:
        response = requests.get(url)
        response.raise_for_status() 
        return response.json() 
    except Exception as e:
        return None`,
  );

  ```
</CodeGroup>

<Tip>
  Environmental variables can be overridden (or defined) in the following ways:

  1. During the creation of a `SandboxTool`, either via the constructor or the factory function (`SandboxTool.from_source_code`).
  2. By passing them directly as part of the options when invoking: `my_tool.run(..., env={ "MY_ENV": "MY_VALUE" })`.
</Tip>

<Note>
  Only `PythonTool` can access files.
</Note>

***

## Creating custom tools

Custom tools allow you to build your own specialized tools to extend agent capabilities.

To create a new tool, implement the base `Tool` class. The framework provides flexible options for tool creation, from simple to complex implementations.

<Note>
  Initiate the `Tool` by passing your own handler (function) with the `name`, `description` and `input schema`.
</Note>

### Basic custom tool

Here's an example of a simple custom tool that provides riddles:

<CodeGroup>
  {/* <!-- embedme python/examples/tools/custom/base.py --> */}

  ```py Python [expandable]
  import asyncio
  import random
  import sys
  from typing import Any

  from pydantic import BaseModel, Field

  from beeai_framework.context import RunContext
  from beeai_framework.emitter import Emitter
  from beeai_framework.errors import FrameworkError
  from beeai_framework.tools import StringToolOutput, Tool, ToolRunOptions


  class RiddleToolInput(BaseModel):
      riddle_number: int = Field(description="Index of riddle to retrieve.")


  class RiddleTool(Tool[RiddleToolInput, ToolRunOptions, StringToolOutput]):
      name = "Riddle"
      description = "It selects a riddle to test your knowledge."
      input_schema = RiddleToolInput

      data = (
          "What has hands but can't clap?",
          "What has a face and two hands but no arms or legs?",
          "What gets wetter the more it dries?",
          "What has to be broken before you can use it?",
          "What has a head, a tail, but no body?",
          "The more you take, the more you leave behind. What am I?",
          "What goes up but never comes down?",
      )

      def __init__(self, options: dict[str, Any] | None = None) -> None:
          super().__init__(options)

      def _create_emitter(self) -> Emitter:
          return Emitter.root().child(
              namespace=["tool", "example", "riddle"],
              creator=self,
          )

      async def _run(
          self, input: RiddleToolInput, options: ToolRunOptions | None, context: RunContext
      ) -> StringToolOutput:
          index = input.riddle_number % (len(self.data))
          riddle = self.data[index]
          return StringToolOutput(result=riddle)


  async def main() -> None:
      tool = RiddleTool()
      tool_input = RiddleToolInput(riddle_number=random.randint(0, len(RiddleTool.data)))
      result = await tool.run(tool_input)
      print(result)


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/tools/custom/python.ts --> */}

  ```ts TypeScript [expandable]
  import "dotenv/config";
  import { CustomTool } from "beeai-framework/tools/custom";

  const customTool = await CustomTool.fromSourceCode(
    {
      // Ensure the env exists
      url: process.env.CODE_INTERPRETER_URL!,
      env: { API_URL: "https://riddles-api.vercel.app/random" },
    },
    `import requests
  import os
  from typing import Optional, Union, Dict

  def get_riddle() -> Optional[Dict[str, str]]:
    """
    Fetches a random riddle from the Riddles API.

    This function retrieves a random riddle and its answer. It does not accept any input parameters.

    Returns:
        Optional[Dict[str, str]]: A dictionary containing:
            - 'riddle' (str): The riddle question.
            - 'answer' (str): The answer to the riddle.
        Returns None if the request fails.
    """
    url = os.environ.get('API_URL')
    
    try:
        response = requests.get(url)
        response.raise_for_status() 
        return response.json() 
    except Exception as e:
        return None`,
  );

  ```
</CodeGroup>

<Tip>
  The input schema (`inputSchema`) processing can be asynchronous when needed for more complex validation or preprocessing.
</Tip>

<Tip>
  For structured data responses, use `JSONToolOutput` or implement your own custom output type.
</Tip>

### Advanced custom tool

For more complex scenarios, you can implement tools with robust input validation, error handling, and structured outputs:

<CodeGroup>
  {/* <!-- embedme python/examples/tools/custom/openlibrary.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  from typing import Any

  import httpx
  from pydantic import BaseModel, Field

  from beeai_framework.context import RunContext
  from beeai_framework.emitter import Emitter
  from beeai_framework.errors import FrameworkError
  from beeai_framework.tools import JSONToolOutput, Tool, ToolError, ToolInputValidationError, ToolRunOptions


  class OpenLibraryToolInput(BaseModel):
      title: str | None = Field(description="Title of book to retrieve.", default=None)
      olid: str | None = Field(description="Open Library number of book to retrieve.", default=None)
      subjects: str | None = Field(description="Subject of a book to retrieve.", default=None)


  class OpenLibraryToolResult(BaseModel):
      preview_url: str
      info_url: str
      bib_key: str


  class OpenLibraryToolOutput(JSONToolOutput[OpenLibraryToolResult]):
      pass


  class OpenLibraryTool(Tool[OpenLibraryToolInput, ToolRunOptions, OpenLibraryToolOutput]):
      name = "OpenLibrary"
      description = """Provides access to a library of books with information about book titles,
          authors, contributors, publication dates, publisher and isbn."""
      input_schema = OpenLibraryToolInput

      def __init__(self, options: dict[str, Any] | None = None) -> None:
          super().__init__(options)

      def _create_emitter(self) -> Emitter:
          return Emitter.root().child(
              namespace=["tool", "example", "openlibrary"],
              creator=self,
          )

      async def _run(
          self, tool_input: OpenLibraryToolInput, options: ToolRunOptions | None, context: RunContext
      ) -> OpenLibraryToolOutput:
          key = ""
          value = ""
          input_vars = vars(tool_input)
          for val in input_vars:
              if input_vars[val] is not None:
                  key = val
                  value = input_vars[val]
                  break
          else:
              raise ToolInputValidationError("All input values in OpenLibraryToolInput were empty.") from None

          async with httpx.AsyncClient() as client:
              response = await client.get(
                  f"https://openlibrary.org/api/books?bibkeys={key}:{value}&jsmcd=data&format=json",
                  headers={"Content-Type": "application/json", "Accept": "application/json"},
              )
              response.raise_for_status()

              result = response.json().get(f"{key}:{value}")
              if not result:
                  raise ToolError(f"No book found with {key}={value}.")

              return OpenLibraryToolOutput(OpenLibraryToolResult.model_validate(result))


  async def main() -> None:
      tool = OpenLibraryTool()
      tool_input = OpenLibraryToolInput(title="It")
      result = await tool.run(tool_input)
      print(result)


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/tools/custom/openLibrary.ts --> */}

  ```ts TypeScript [expandable]
  import {
    BaseToolOptions,
    BaseToolRunOptions,
    Tool,
    ToolInput,
    JSONToolOutput,
    ToolError,
    ToolEmitter,
  } from "beeai-framework/tools/base";
  import { z } from "zod";
  import { createURLParams } from "beeai-framework/internals/fetcher";
  import { GetRunContext } from "beeai-framework/context";
  import { Callback, Emitter } from "beeai-framework/emitter/emitter";

  type ToolOptions = BaseToolOptions & { maxResults?: number };
  type ToolRunOptions = BaseToolRunOptions;

  export interface OpenLibraryResponse {
    numFound: number;
    start: number;
    numFoundExact: boolean;
    q: string;
    offset: number;
    docs: Record<string, any>[];
  }

  export class OpenLibraryToolOutput extends JSONToolOutput<OpenLibraryResponse> {
    isEmpty(): boolean {
      return !this.result || this.result.numFound === 0 || this.result.docs.length === 0;
    }
  }

  export class OpenLibraryTool extends Tool<OpenLibraryToolOutput, ToolOptions, ToolRunOptions> {
    name = "OpenLibrary";
    description =
      "Provides access to a library of books with information about book titles, authors, contributors, publication dates, publisher and isbn.";

    inputSchema() {
      return z
        .object({
          title: z.string(),
          author: z.string(),
          isbn: z.string(),
          subject: z.string(),
          place: z.string(),
          person: z.string(),
          publisher: z.string(),
        })
        .partial();
    }

    public readonly emitter: ToolEmitter<
      ToolInput<this>,
      OpenLibraryToolOutput,
      {
        beforeFetch: Callback<{ request: { url: string; options: RequestInit } }>;
        afterFetch: Callback<{ data: OpenLibraryResponse }>;
      }
    > = Emitter.root.child({
      namespace: ["tool", "search", "openLibrary"],
      creator: this,
    });

    static {
      this.register();
    }

    protected async _run(
      input: ToolInput<this>,
      _options: Partial<ToolRunOptions>,
      run: GetRunContext<this>,
    ) {
      const request = {
        url: `https://openlibrary.org?${createURLParams({
          searchon: input,
        })}`,
        options: { signal: run.signal } as RequestInit,
      };

      await run.emitter.emit("beforeFetch", { request });
      const response = await fetch(request.url, request.options);

      if (!response.ok) {
        throw new ToolError(
          "Request to Open Library API has failed!",
          [new Error(await response.text())],
          {
            context: { input },
          },
        );
      }

      const json: OpenLibraryResponse = await response.json();
      if (this.options.maxResults) {
        json.docs.length = this.options.maxResults;
      }

      await run.emitter.emit("afterFetch", { data: json });
      return new OpenLibraryToolOutput(json);
    }
  }

  ```
</CodeGroup>

### Implementation guidelines

When creating custom tools, follow these key requirements:

**1. Implement the `Tool` class**

To create a custom tool, you need to extend the base `Tool` class and implement several required components. The output must be an implementation of the `ToolOutput` interface, such as `StringToolOutput` for text responses or `JSONToolOutput` for structured data.

**2. Create a descriptive name**

Your tool needs a clear, descriptive name that follows naming conventions:

```py
name = "MyNewTool"
```

The name must only contain characters a-z, A-Z, 0-9, or one of - or \_.

**3. Write an effective description**

The description is crucial as it determines when the agent uses your tool:

```py
description = "Takes X action when given Y input resulting in Z output"
```

You should experiment with different natural language descriptions to ensure the tool is used in the correct circumstances. You can also include usage tips and guidance for the agent in the description, but its advisable to keep the description succinct in order to reduce the probability of conflicting with other tools, or adversely affecting agent behavior.

**4. Define a clear input schema**

Create a Pydantic model that defines the expected inputs with helpful descriptions:

```py
class OpenMeteoToolInput(BaseModel):
    location_name: str = Field(description="The name of the location to retrieve weather information.")
    country: str | None = Field(description="Country name.", default=None)
    start_date: str | None = Field(
        description="Start date for the weather forecast in the format YYYY-MM-DD (UTC)", default=None
    )
    end_date: str | None = Field(
        description="End date for the weather forecast in the format YYYY-MM-DD (UTC)", default=None
    )
    temperature_unit: Literal["celsius", "fahrenheit"] = Field(
        description="The unit to express temperature", default="celsius"
    )
```

*Source: [/python/beeai\_framework/tools/weather/openmeteo.py](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/tools/weather/openmeteo.py)*

The input schema is a required field used to define the format of the input to your tool. The agent will formalise the natural language input(s) it has received and structure them into the fields described in the tool's input. The input schema will be created based on the `MyNewToolInput` class. Keep your tool input schema simple and provide schema descriptions to help the agent to interpret fields.

**5. Implement the `_run()` method**

This method contains the core functionality of your tool, processing the input and returning the appropriate output.

```py
def _run(self, input: OpenMeteoToolInput, options: Any = None) -> None:
    params = urlencode(self.get_params(input), doseq=True)
    logger.debug(f"Using OpenMeteo URL: https://api.open-meteo.com/v1/forecast?{params}")
    response = requests.get(
        f"https://api.open-meteo.com/v1/forecast?{params}",
        headers={"Content-Type": "application/json", "Accept": "application/json"},
    )
    response.raise_for_status()
    return StringToolOutput(json.dumps(response.json()))
```

*Source: [/python/beeai\_framework/tools/weather/openmeteo.py](https://github.com/i-am-bee/beeai-framework/blob/main/python/beeai_framework/tools/weather/openmeteo.py)*

***

## Best practices

### 1. Data minimization

If your tool is providing data to the agent, try to ensure that the data is relevant and free of extraneous metatdata. Preprocessing data to improve relevance and minimize unnecessary data conserves agent memory, improving overall performance.

### 2. Provide hints

If your tool encounters an error that is fixable, you can return a hint to the agent; the agent will try to reuse the tool in the context of the hint. This can improve the agent's ability
to recover from errors.

### 3. Security and stability

When building tools, consider that the tool is being invoked by a somewhat unpredictable third party (the agent). You should ensure that sufficient guardrails are in place to prevent
adverse outcomes.

***

## Examples

<CardGroup cols={2}>
  <Card title="Python" icon="python" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/tools">
    Explore reference tool implementations in Python
  </Card>

  <Card title="TypeScript" icon="js" href="https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples/tools">
    Explore reference tool implementations in TypeScript
  </Card>
</CardGroup>


# Workflows
Source: https://framework.beeai.dev/modules/workflows



## Overview

Workflows provide a flexible and extensible component for managing and executing structured sequences of tasks. They are particularly useful for:

* üîÑ **Dynamic Execution:** Steps can direct the flow based on state or results
* ‚úÖ **Validation:** Define schemas for data consistency and type safety
* üß© **Modularity:** Steps can be standalone or invoke nested workflows
* üëÅÔ∏è **Observability:** Emit events during execution to track progress or handle errors

<Note>
  Supported in Python and TypeScript.
</Note>

***

## Core Concepts

### State

State is the central data structure in a workflow. It's a Pydantic model that:

* Holds the data passed between steps
* Provides type validation and safety
* Persists throughout the workflow execution

### Steps

Steps are the building blocks of a workflow. Each step is a function that:

* Takes the current state as input
* Can modify the state
* Returns the name of the next step to execute or a special reserved value

### Transitions

Transitions determine the flow of execution between steps. Each step returns either:

* The name of the next step to execute
* `Workflow.NEXT` - proceed to the next step in order
* `Workflow.SELF` - repeat the current step
* `Workflow.END` - end the workflow execution

***

## Basic Usage

### Simple Workflow

The example below demonstrates a minimal workflow that processes steps in sequence. This pattern is useful for straightforward, linear processes where each step builds on the previous one.

<CodeGroup>
  {/* <!-- embedme python/examples/workflows/simple.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from pydantic import BaseModel

  from beeai_framework.errors import FrameworkError
  from beeai_framework.workflows import Workflow


  async def main() -> None:
      # State
      class State(BaseModel):
          input: str

      workflow = Workflow(State)
      workflow.add_step("first", lambda state: print("Running first step!"))
      workflow.add_step("second", lambda state: print("Running second step!"))
      workflow.add_step("third", lambda state: print("Running third step!"))

      await workflow.run(State(input="Hello"))


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/workflows/simple.ts --> */}

  ```ts Typescript [expandable]
  import { Workflow } from "beeai-framework/workflows/workflow";
  import { z } from "zod";

  const schema = z.object({
    hops: z.number().default(0),
  });

  const workflow = new Workflow({ schema })
    .addStep("a", async (state) => {
      state.hops += 1;
    })
    .addStep("b", () => (Math.random() > 0.5 ? Workflow.PREV : Workflow.END));

  const response = await workflow.run({ hops: 0 }).observe((emitter) => {
    emitter.on("start", (data) => console.log(`-> start ${data.step}`));
    emitter.on("error", (data) => console.log(`-> error ${data.step}`));
    emitter.on("success", (data) => console.log(`-> finish ${data.step}`));
  });

  console.log(`Hops: ${response.result.hops}`);
  console.log(`-> steps`, response.steps.map((step) => step.name).join(","));

  ```
</CodeGroup>

***

## Advanced Features

### Multi-Step and Nested Workflows

This advanced example showcases a workflow that implements multiplication through repeated addition‚Äîdemonstrating control flow, state manipulation, nesting, and conditional logic.

Workflow nesting allows complex behaviors to be encapsulated as reusable components, enabling hierarchical composition of workflows. This promotes modularity, reusability, and better organization of complex agent logic.

<CodeGroup>
  {/* <!-- embedme python/examples/workflows/nesting.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback
  from typing import Literal, TypeAlias

  from pydantic import BaseModel

  from beeai_framework.errors import FrameworkError
  from beeai_framework.workflows import Workflow, WorkflowReservedStepName

  WorkflowStep: TypeAlias = Literal["pre_process", "add_loop", "post_process"]


  async def main() -> None:
      # State
      class State(BaseModel):
          x: int
          y: int
          abs_repetitions: int | None = None
          result: int | None = None

      def pre_process(state: State) -> WorkflowStep:
          print("pre_process")
          state.abs_repetitions = abs(state.y)
          return "add_loop"

      def add_loop(state: State) -> WorkflowStep | WorkflowReservedStepName:
          if state.abs_repetitions and state.abs_repetitions > 0:
              result = (state.result if state.result is not None else 0) + state.x
              abs_repetitions = (state.abs_repetitions if state.abs_repetitions is not None else 0) - 1
              print(f"add_loop: intermediate result {result}")
              state.abs_repetitions = abs_repetitions
              state.result = result
              return Workflow.SELF
          else:
              return "post_process"

      def post_process(state: State) -> WorkflowReservedStepName:
          print("post_process")
          if state.y < 0:
              result = -(state.result if state.result is not None else 0)
              state.result = result
          return Workflow.END

      multiplication_workflow = Workflow[State, WorkflowStep](name="MultiplicationWorkflow", schema=State)
      multiplication_workflow.add_step("pre_process", pre_process)
      multiplication_workflow.add_step("add_loop", add_loop)
      multiplication_workflow.add_step("post_process", post_process)

      response = await multiplication_workflow.run(State(x=8, y=5))
      print(f"result: {response.state.result}")

      response = await multiplication_workflow.run(State(x=8, y=-5))
      print(f"result: {response.state.result}")


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/workflows/nesting.ts --> */}

  ```ts Typescript [expandable]
  import { Workflow } from "beeai-framework/workflows/workflow";
  import { z } from "zod";

  const schema = z.object({
    threshold: z.number().min(0).max(1),
    counter: z.number().default(0),
  });

  const addFlow = new Workflow({ schema }).addStep("run", async (state) => {
    state.counter += 1;
    return Math.random() > 0.5 ? Workflow.SELF : Workflow.END;
  });

  const subtractFlow = new Workflow({
    schema,
  }).addStep("run", async (state) => {
    state.counter -= 1;
    return Math.random() > 0.5 ? Workflow.SELF : Workflow.END;
  });

  const workflow = new Workflow({
    schema,
  })
    .addStep("start", (state) =>
      Math.random() > state.threshold ? "delegateAdd" : "delegateSubtract",
    )
    .addStep("delegateAdd", addFlow.asStep({ next: Workflow.END }))
    .addStep("delegateSubtract", subtractFlow.asStep({ next: Workflow.END }));

  const response = await workflow.run({ threshold: 0.5 }).observe((emitter) => {
    emitter.on("start", (data, event) =>
      console.log(`-> step ${data.step}`, event.trace?.parentRunId ? "(nested flow)" : ""),
    );
  });
  console.info(`Counter:`, response.result);

  ```
</CodeGroup>

This workflow demonstrates several powerful concepts:

* Implementing loops by returning `Workflow.SELF`
* Conditional transitions between steps
* Progressive state modification to accumulate results
* Sign handling through state transformation
* Type-safe step transitions using Literal types

***

### Multi-Agent Workflows

The multi-agent workflow pattern enables the orchestration of specialized agents that collaborate to solve complex problems. Each agent focuses on a specific domain or capability, with results combined by a coordinator agent.

<CodeGroup>
  {/* <!-- embedme python/examples/workflows/multi_agents.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from beeai_framework.backend import ChatModel
  from beeai_framework.emitter import EmitterOptions
  from beeai_framework.errors import FrameworkError
  from beeai_framework.tools.search.wikipedia import WikipediaTool
  from beeai_framework.tools.weather import OpenMeteoTool
  from beeai_framework.workflows.agent import AgentWorkflow, AgentWorkflowInput
  from examples.helpers.io import ConsoleReader


  async def main() -> None:
      llm = ChatModel.from_name("ollama:llama3.1")
      workflow = AgentWorkflow(name="Smart assistant")

      workflow.add_agent(
          name="Researcher",
          role="A diligent researcher.",
          instructions="You look up and provide information about a specific topic.",
          tools=[WikipediaTool()],
          llm=llm,
      )

      workflow.add_agent(
          name="WeatherForecaster",
          role="A weather reporter.",
          instructions="You provide detailed weather reports.",
          tools=[OpenMeteoTool()],
          llm=llm,
      )

      workflow.add_agent(
          name="DataSynthesizer",
          role="A meticulous and creative data synthesizer",
          instructions="You can combine disparate information into a final coherent summary.",
          llm=llm,
      )

      reader = ConsoleReader()

      reader.write("Assistant ü§ñ : ", "What location do you want to learn about?")
      for prompt in reader:
          await (
              workflow.run(
                  inputs=[
                      AgentWorkflowInput(prompt="Provide a short history of the location.", context=prompt),
                      AgentWorkflowInput(
                          prompt="Provide a comprehensive weather summary for the location today.",
                          expected_output="Essential weather details such as chance of rain, temperature and wind. Only report information that is available.",
                      ),
                      AgentWorkflowInput(
                          prompt="Summarize the historical and weather data for the location.",
                          expected_output="A paragraph that describes the history of the location, followed by the current weather conditions.",
                      ),
                  ]
              )
              .on(
                  # Event Matcher -> match agent's 'success' events
                  lambda event: isinstance(event.creator, ChatModel) and event.name == "success",
                  # log data to the console
                  lambda data, event: reader.write(
                      "->Got response from the LLM",
                      "  \n->".join([str(message.content[0].model_dump()) for message in data.value.messages]),
                  ),
                  EmitterOptions(match_nested=True),
              )
              .on(
                  "success",
                  lambda data, event: reader.write(
                      f"->Step '{data.step}' has been completed with the following outcome."
                      f"\n\n{data.state.final_answer}\n\n",
                      data.model_dump(exclude={"data"}),
                  ),
              )
          )
          reader.write("Assistant ü§ñ : ", "What location do you want to learn about?")


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- embedme typescript/examples/workflows/multiAgents.ts --> */}

  ```ts Typescript [expandable]
  import "dotenv/config";
  import { createConsoleReader } from "examples/helpers/io.js";
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";
  import { WikipediaTool } from "beeai-framework/tools/search/wikipedia";
  import { AgentWorkflow } from "beeai-framework/workflows/agent";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";

  const workflow = new AgentWorkflow("Smart assistant");
  const llm = new OllamaChatModel("llama3.1");

  workflow.addAgent({
    name: "Researcher",
    role: "A diligent researcher",
    instructions: "You look up and provide information about a specific topic.",
    tools: [new WikipediaTool()],
    llm,
  });
  workflow.addAgent({
    name: "WeatherForecaster",
    role: "A weather reporter",
    instructions: "You provide detailed weather reports.",
    tools: [new OpenMeteoTool()],
    llm,
  });
  workflow.addAgent({
    name: "DataSynthesizer",
    role: "A meticulous and creative data synthesizer",
    instructions: "You can combine disparate information into a final coherent summary.",
    llm,
  });

  const reader = createConsoleReader();
  reader.write("Assistant ü§ñ : ", "What location do you want to learn about?");
  for await (const { prompt } of reader) {
    const { result } = await workflow
      .run([
        { prompt: "Provide a short history of the location.", context: prompt },
        {
          prompt: "Provide a comprehensive weather summary for the location today.",
          expectedOutput:
            "Essential weather details such as chance of rain, temperature and wind. Only report information that is available.",
        },
        {
          prompt: "Summarize the historical and weather data for the location.",
          expectedOutput:
            "A paragraph that describes the history of the location, followed by the current weather conditions.",
        },
      ])
      .observe((emitter) => {
        emitter.on("success", (data) => {
          reader.write(
            `Step '${data.step}' has been completed with the following outcome:\n`,
            data.state?.finalAnswer ?? "-",
          );
        });
      });

    reader.write(`Assistant ü§ñ`, result.finalAnswer);
    reader.write("Assistant ü§ñ : ", "What location do you want to learn about?");
  }

  ```
</CodeGroup>

This pattern demonstrates:

* Role specialization through focused agent configuration
* Efficient tool distribution to relevant specialists
* Parallel processing of different aspects of a query
* Synthesis of multiple expert perspectives into a cohesive response

<Tip>
  See the [events documentation](/modules/events) for more information on standard emitter events.
</Tip>

### Memory in Workflows

Integrating memory into workflows allows agents to maintain context across interactions, enabling conversational interfaces and stateful processing. This example demonstrates a simple conversational echo workflow with persistent memory.

<CodeGroup>
  {/* <!-- embedme python/examples/workflows/memory.py --> */}

  ```py Python [expandable]
  import asyncio
  import sys
  import traceback

  from pydantic import BaseModel, InstanceOf

  from beeai_framework.backend import AssistantMessage, UserMessage
  from beeai_framework.errors import FrameworkError
  from beeai_framework.memory import UnconstrainedMemory
  from beeai_framework.workflows import Workflow
  from examples.helpers.io import ConsoleReader


  async def main() -> None:
      # State with memory
      class State(BaseModel):
          memory: InstanceOf[UnconstrainedMemory]
          output: str = ""

      async def echo(state: State) -> str:
          # Get the last message in memory
          last_message = state.memory.messages[-1]
          state.output = last_message.text[::-1]
          return Workflow.END

      reader = ConsoleReader()

      memory = UnconstrainedMemory()
      workflow = Workflow(State)
      workflow.add_step("echo", echo)

      for prompt in reader:
          # Add user message to memory
          await memory.add(UserMessage(content=prompt))
          # Run workflow with memory
          response = await workflow.run(State(memory=memory))
          # Add assistant response to memory
          await memory.add(AssistantMessage(content=response.state.output))

          reader.write("Assistant ü§ñ : ", response.state.output)


  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())

  ```

  {/* <!-- todo typescript/examples/workflows/memory.ts --> */}

  ```ts Typescript [expandable]
  Example coming soon
  ```
</CodeGroup>

This pattern demonstrates:

* Integration of memory as a first-class citizen in workflow state
* Conversation loops that preserve context across interactions
* Bidirectional memory updating (reading recent messages, storing responses)
* Clean separation between the persistent memory and workflow-specific state

***

## Examples

<CardGroup cols={2}>
  <Card title="Python" icon="python" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/workflows">
    Explore reference workflow implementations in Python
  </Card>

  <Card title="TypeScript" icon="js" href="https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples/workflows">
    Explore reference workflow implementations in TypeScript
  </Card>
</CardGroup>


